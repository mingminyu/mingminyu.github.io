# 第5章：模型建立方法讨论

建立模型可运用的方法非常多，如判别分析（Discriminant Analysis）​、线性回归（Linear Regression）​、逻辑回归（Logistic Regression）及分类树（Classification Trees）等统计方法；或是类神经网络（Neural Networks）​、基因算法（Genetic Algorithms）及专家系统（Expert Systems）等非统计方法。

在实务运用上，选择线性回归或逻辑回归来建构评分模式，在模式实行上成本较低也较快速，是模型研发人员最常选用的方式。

回归分析始于19世纪末，学者高登（Gorden）发表了一项关于用父母身体特征去预测子女身体特征的研究成果。他在文献中首度使用“Regression”来表示父母身高对子女身高的影响效应，因此，利用一个或多个变量以预测另一变量的方法被称为“回归分析”​。简而言之，就是用一个解释变量（Independent Variable，或称“自变量”​）去描述或预测反应变量 （Dependent Variable，或称“因变量”​）所建立的回归模式，也称为“简单回归”​（Simple Regression）​；若用多个变量去预测一个反应变量，则所建立的回归模式称为“复回归”​（Multiple Regression）​。至于“逻辑回归”​（Logistic Regression）则是为了利用多个解释变量去预测一个二元事件（如是/否、好/坏等）而建立的回归模型。

接下来就针对业界常用的线性回归或逻辑回归做进一步的介绍。

## 5.1 线性回归（Linear Regression）

### 5.1.1 模型设定

线性回归是研究单一因变量与一个或一个以上自变量之间的关系。线性回归适用于连续属性的模型配对，当只有一个自变量时，其回归模型为

$$
E(Y \mid X = x) = \beta_0 + \beta_1 x
$$

若自变量为 P 个，则可表示为

$$
Y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \varepsilon_i, \quad i = 1, \cdots, n
$$

式中，Yi为因变量；x为模型变量（自变量）​；β0为截距项；βi为参数；εi为残差项。

多项自变量与单一因变量的关系一般又被称为多元线性回归，一般线性回归需满足以下基本假设：

1. 自变量与应变量之间呈线性关系。
2. 残差项的期望值为 0。
3. 残差项的变异数是常数。
4. 观测值互相独立。
5. 残差项需服从常态分配。

### 5.1.2 变量选择

在回归分析与逻辑回归中，选取自变量时，最常使用的方式是逐步回归（Stepwise MultipleRegression）​，其又分为3种：同时分析法（Simultaneous Multiple Regression）​、逐步分析法（Stepwise Multiple Regression）及阶层分析法（Hierarchical Multiple Regression）​。其中逐步分析法最常使用，包括如下3种。

1. 顺向选择法（Forward Selection）： 先从所有变量中，选择对模型贡献最大者进入回归方程式，然后，逐一选择第二个、第三个，甚至多个预测力较高的变量进入模型中，而进入的标准为是否具有最小F概率值，通常设值为0.50，若自变量的F值小于此值，将被选取进入。
2. 反向淘汰法（Backward Elimination）：此方法与顺向选择法相反，首先，选入所有的变量；其次，逐一删除对模型的贡献最小者；最后，选其他变量进入模型中。剔除的标准为是否具有最大F概率值，通常设值为0.10，若自变量的F值大于此值，将被选取剔除。
3. 逐步分析法（Stepwise Analysis）：逐步分析法是上述两项技术的综合。首先，模型中不包含任何预测变项，然后，采用顺向选择法，将对模型贡献最大的自变量挑选进入回归模式中。而在每一步骤中，已被纳入模型的自变量则必须再经过反向淘汰法的考验，以决定该变项是被淘汰还是被留下。逐步回归法F概率值的进入标准通常为0.15，剔除标准也为0.15。

### 5.1.3 模型表现

在运用线性回归分析做推论时，常用T检验、F检验及R2等来检验所产生的重要统计量是否具有统计上的显著水平，以此判断此回归式是否具有意义。

1. T检验：检验回归系数是否具有统计上的显著意义，T检验统计量用来检验每一个系数是否为0，若经由检验发现某个系数并不显著，也就是与0无显著差异，则需观察是否是资料本身的问题（如样本数不足）​，或是非线性关系等，若此变量对模型表现无贡献即可考虑将该变量剔除。
2. F检验：回归分析中，F值是用来检验自变量集合与因变量之间是否具有显著关系。在此以F统计量来检验整个回归式是否具有意义，也就是检验回归式的所有系数是否均为0，若均为0，则所检验的回归式无法描述因变量的变化，有再予修正的必要。具体而言，F检验系假设 H0:β1=β2=…=βn=0，H1:βi≠0，i=1,…,n。

$$
F = \frac{\mathrm{MSR}}{\mathrm{MSE}} = \frac{\mathrm{SSR}/p}{\mathrm{SSE}/(n - p - 1)}
$$

式中，$\mathrm{SSR} = \sum_{i=1}^n(Y_i - \hat{Y_i})^2$ 被称为“回归平方和”​；$\mathrm{SSE} = \sum_{i=1}^n(Y_i - \bar{Y})^2$ 被称为“误差平方和”​；n为样本数，p为自变量的数目；

$\hat Y_i$ 与 $Y_i$ 分别为第i笔因变量的估计值与实际观测值，$\bar Y$ 为因变量的平均值。

如以0.05作为显著水平，若F值＞F （1-0.05; p;n-p-1）​，则表示拒绝虚无假设H0，代表此回归式在统计上是具有意义的。

3. 决定系数R2及Adjusted R2：决定系数是用以说明所估计出来的回归式能够解释实际状况的程度，通常以判定系数R2判断因变量与整体自变量的关系是否密切，也就是回归模式的解释能力是否充足。

$$
\mathbb{R}^2 = \frac{\text{SSR}}{\text{SST}} = 1- \frac{\text{SSE}}{\text{SST}}
$$

式中，$ \text{SSR} = \sum_{i=1}^n(Y_i - \bar{Y})^2 $，$ \text{SST} = \sum_{i=1}^n(Y_i - \bar{Y})^2 $， $ \text{SSE} = \sum_{i=1}^n(Y_i - \hat{Y_i})^2 $， $ \text{SSR} $ 为估计值，Yi为观测值。


因0≤SSE≤SST，故0≤R2≤1。在回归分析中，若不考虑自由度，则自变量个数越多，R2会越接近1。若R2越接近1。表示回归模型的解释能力越佳，然而增加的自变量或许是无关的变量，此时将会降低回归式的意义，因此，有学者提出调整自由度后的 $\bar{R}^2$ ，用以弥补上述缺点。

$$
\bar{R}^2 = 1-\frac{\text{SSE} / (T/p)}{\text{SST} / (T-1)}
$$


## 5.2 逻辑回归（Logistic Regression）

### 5.1.1 模型设定

逻辑回归（Logistic Regression）与传统的回归分析性质相似，不过它是用来处理类别性数据问题的。

逻辑回归模型的因变量为二选一的属性变量，其出现的变量值为好与坏（包括违约/非违约事件、失败/成功等情况）的二选一可能事件。此方法具有易懂、非黑箱作业及能与概率结合等优点，因此，为开发评分卡最常使用的方法。

假设因变量为Y，Y值为0或1，自变量为X = （X1，X2，…，Xk）​。

令 π（x）=E（Y｜X）=1×P（Y=1｜X）+0×P（Y=0｜X）=P（Y=1｜X）​，则可将其逻辑回归模型表示如下：

式中，

π=π（x）为当X=（x1, x2,…, xk）时，Y=1的概率，而逻辑回归模型将此概率的logit转换设定为线性形态。将逻辑回归模型表示为成功概率π（x）的形态，即为

### 5.1.2 参数估计

逻辑回归模型的参数是用最大概似法估计。每个观察值yi皆为0或1，因此，yi～Bernoulli（π）,i=1,…,n（此为白努利分配，n为样本数）​，其概似函数表示如下：

![](5_01.png)

在模型参数估计部分，将概似函数最大化以估计参数。β'=（β1, β2,…, βk）​，因此，将上式分别对各参数微分且令其为零。然而，因微分后所得出的方程式为非线性方程式，因此，在运算上需利用反复迭代的算法来进行参数估计。

### 5.1.3 自变量处理

在模型投入变量部分，不建议采用原始变量，因原始变量容易导致模型稳定性不佳。一般常见方法为采用每一变量分组后的WOE值，或每一变量分组的虚拟变量（Dummy Variable）​。

> WOE值

使用前阶段中每个变量分组后的WOE值来取代原本的变量值作为回归模型训练的投入，既可避免变量值中出现极端值（Outliers）的情形，又可减少模型过度配适（Overfitting）的现象（见表5-1）​。

![表5-1　利用WOE值取代原变量值](5_02.png)

WOE的计算方式如下：


式中，i为特征变量分箱的组别；Distr Good为各分组中好件占全体好件的比例；Distr Bad为各分组中坏件占全体坏件的比例。以表5-1为例，特征变量年纪（Age）的分箱群中，23～26岁的分组的WOE值为


由公式可看出，WOE值代表各分箱内样本的好件对坏件的比率，换而言之，WOE值表述该特征变量中各个属性（Attribute）的风险程度大小。WOE值越高，代表该属性的风险程度越小；WOE值越低，代表该属性的风险程度越大（见表5-2）​。

![表5-2　WOE计算范例](5_03.png)

> 虚拟变量

另外，也可用每一个变量分组设置虚拟变量（Dummy Variable）​。就逻辑回归来说，离散或名目尺度的变量，例如，性别、学历和婚姻状态等并不适宜。以区间尺度为例，每个数字代表不同层级，此时的数字没有任何显著意义，在这种情况下，就可使用虚拟变量的方法。举例来说，以表5-3“教育程度”为例，此解释变量分为5个群组，含小学、初中、高中职、大专及研究生以上，其虚拟变量设计如表5-3所示。

一般来说，如果名目尺度的变量被分为n个群组，则需要设计n－1个不同形式的虚拟变量，其中设定为基准的分组，也就是数值全为0的判定，通常会采用Bad%接近全体者的分组。

![表5-3　利用虚拟变量值取代原变量值](5_04.png)


### 5.1.4 模型表现

## 5.3 两阶段式建立方法

一般在建立评分卡时，逻辑回归的因变量是0或1，且无法使用连续型变量，因此，可考虑利用线性回归，将前阶段逻辑回归分析产生的残差值作为第二阶段的因变量。

采取二阶段回归，可选择将预测力较强的变量纳入后阶段线性回归模型，如此一来，评分模型比较不易被预测力高的变量独断而产生影响及偏颇。例如，在建立信用卡评分卡时，为避免模型过分依赖联征变量，可在第一阶段将逻辑回归纳入行内变量进行分析，联征变量则在第二阶段加入。

两阶段式回归建立步骤如下：

1. 模型建立：为使模型有较佳的精确性及稳定性，通常会将模型样本分为“开发－保留”两组数据集（Development & Hold-Out Sample）​，分别占整体样本的“30%～70%”​，利用70%的开发样本执行逐步回归。
2. 第一阶段回归——逻辑回归模型：若有变量其区间给分与其对应的好坏对比值（GB Index）出现矛盾，或与实际认知相斥，则需重新设计该变量区间或选择排除该变量，再重新执行回归动作。
3. 相关分析：检查选入变量的相关性，将所有变量进行相关系数分析，相关系数高于0.85的变量取VOI高者，以避免模型存在共线性的问题。
4. 重复执行步骤2～步骤3的动作以寻找最佳模型。
5. 第二阶段回归：步骤4可得到第一阶段逻辑回归计算后的残差值，以此作为第二阶段线性回归的因变量。相似的回归分析过程也在第二阶段执行，重复执行步骤2～步骤5以确保所挑选变量组合符合统计与实务经验，最后得出最终评分模型。
6. 模型检验：利用30%的保留样本（Hold-out）与时间外样本（Outof-time）分别进行效度检验，以确保模型的精确性及稳定性，若无法达到指定标准，则重复步骤1～步骤5。通常是以Gini值与KS值来作为指定标准，一般而言，Gini值达40%、K-S值达30%，表示模型对好坏案件鉴别力强（Gini值与K-S值的说明及模型验证详见第八章）​。
7. 将最终评分模型所得的变量系数乘以1 000即可得到评分卡的分数。

## 5.4 初始模型讨论

针对评分卡建立流程，初始模型讨论主要针对前阶段各流程做细致介绍，并着重将初始模型纳入变量的讨论，此部分需各单位专业人员将实务经验与模型做结合，故该评分卡产品的上游至下游相关单位皆应派员参与讨论，如业务单位、营销单位、征审单位及政策单位等。

初始模型讨论的议程重点如下：

1. 简述评分卡目的与流程架构。
2. 样本区间说明。
3. 资料简介。
4. 评分卡的好坏定义介绍。
5. 评分卡开发过程简介。...

## 5.5 范例
