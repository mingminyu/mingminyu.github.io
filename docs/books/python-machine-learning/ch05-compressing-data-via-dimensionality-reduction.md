# ç¬¬5ç« ï¼šé€šè¿‡é™ç»´å‹ç¼©æ•°æ®

> GitHub Notebook åœ°å€: http://nbviewer.ipython.org/github/rasbt/python-machine-learning-book/blob/master/code/ch05/ch05.ipynb
> 
åœ¨ç¬¬4ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é€šè¿‡ä¸åŒçš„ç‰¹å¾é€‰æ‹©æŠ€æœ¯å¯¹æ•°æ®é›†è¿›è¡Œé™ç»´çš„æ–¹æ³•ã€‚å¦ä¸€ç§å¸¸ç”¨äºé™ç»´çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•å°±æ˜¯ç‰¹å¾æŠ½å–ã€‚åœ¨æœ¬ç« ä¸­ï¼Œè¯»è€…å°†å­¦ä¹ ä¸‰ç§å¯ä»¥å¸®åŠ©æˆ‘ä»¬å½’çº³æ€»ç»“æ•°æ®é›†å†…æ‰€è•´å«ä¿¡æ¯çš„æŠ€æœ¯ï¼Œå®ƒä»¬éƒ½å¯ä»¥å°†åŸå§‹æ•°æ®é›†å˜æ¢åˆ°ä¸€ä¸ªç»´åº¦æ›´ä½çš„æ–°çš„ç‰¹å¾å­ç©ºé—´ã€‚æ•°æ®å‹ç¼©ä¹Ÿæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦å†…å®¹ï¼Œéšç€ç°ä»£æŠ€æœ¯çš„å‘å±•ï¼Œå°†ä¼šäº§ç”Ÿè¶Šæ¥è¶Šå¤šçš„æ•°æ®ï¼Œæ•°æ®å‹ç¼©æŠ€æœ¯å¯ä»¥å¸®åŠ©æˆ‘ä»¬é˜Ÿæ•°æ®è¿›è¡Œå­˜å‚¨å’Œåˆ†æã€‚æœ¬ç« å°†æ¶µç›–å¦‚ä¸‹ä¸»é¢˜:

- æ— ç›‘ç£æ•°æ®å‹ç¼©â€”â€”ä¸»æˆåˆ†åˆ†æ(Principal Component Analysis, PCA)
- åŸºäºç±»åˆ«å¯åˆ†æœ€å¤§åŒ–çš„ç›‘ç£é™ç»´æŠ€æœ¯â€”â€”çº¿æ€§åˆ¤åˆ«åˆ†æ(Linear Discriminant Analysis, LDA)
- é€šè¿‡æ ¸ä¸»æˆåˆ†åˆ†æ(kernel principal component analysis) è¿›è¡Œéçº¿æ€§é™ç»´


# 5.1 æ— ç›‘ç£æ•°æ®é™ç»´æŠ€æœ¯â€”â€”ä¸»æˆåˆ†åˆ†æ

ä¸ç‰¹å¾é€‰æ‹©ç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç‰¹å¾æŠ½å–æ¥å‡å°‘æ•°æ®é›†ä¸­ç‰¹å¾çš„æ•°é‡ã€‚ä¸è¿‡ï¼Œå½“ä½¿ç”¨åºåˆ—åå‘é€‰æ‹©ç­‰ç‰¹å¾é€‰æ‹©ç®—æ³•æ—¶ï¼Œèƒ½å¤Ÿä¿æŒæ•°æ®çš„åŸå§‹ç‰¹å¾ï¼Œè€Œç‰¹å¾æŠ½å–ç®—æ³•åˆ™ä¼šå°†æ•°æ®è½¬æ¢æˆ–è€…æ˜ å°„åˆ°ä¸€ä¸ªæ–°çš„ç‰¹å¾ç©ºé—´ã€‚åŸºäºé™ç»´åœ¨æ•°æ®é¢„å¤„ç†é¢†åŸŸçš„å«ä¹‰ï¼Œç‰¹å¾æŠ½å–å¯ä»¥ç†è§£ä¸º: åœ¨å°½å¯èƒ½å¤šåœ°ä¿æŒç›¸å…³ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¯¹æ•°æ®è¿›è¡Œå‹ç¼©çš„ä¸€ç§æ–¹æ³•ã€‚ç‰¹å¾æŠ½å–é€šå¸¸ç”¨äºæé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ ·ä¹Ÿå¯ä»¥å¸®åŠ©æˆ‘ä»¬é™ä½â€œç»´åº¦ç¾éš¾â€â€”â€”å°¤å…¶å½“æ¨¡å‹ä¸é€‚äºæ­£åˆ™åŒ–å¤„ç†æ—¶ã€‚

ä¸»æˆåˆ†åˆ†æ(principal component analysis, PCA) æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºä¸åŒé¢†åŸŸçš„æ— ç›‘ç£çº¿æ€§æ•°æ®è½¬æ¢æŠ€æœ¯ï¼Œå…¶çªå‡ºä½œç”¨æ˜¯é™ç»´ã€‚PCA çš„å…¶ä»–å¸¸ç”¨é¢†åŸŸåŒ…æ‹¬: è‚¡ç¥¨äº¤æ˜“å¸‚åœºçš„æ¢ç´¢æ€§åˆ†æå’Œä¿¡å·å»å™ªï¼Œä»¥åŠç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸçš„åŸºå› ç»„å’ŒåŸºå› è¡¨è¾¾æ°´å¹³æ•°æ®åˆ†æç­‰ã€‚PCA å¯ä»¥å¸®åŠ©æˆ‘ä»¬åŸºäºç‰¹å¾ä¹‹é—´çš„å…³ç³»è¯†åˆ«å‡ºæ•°æ®å†…åœ¨çš„æ¨¡å¼ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒPCA çš„ç›®æ ‡æ˜¯åœ¨é«˜ç»´æ•°æ®ä¸­æ‰¾åˆ°æœ€å¤§æ–¹å·®çš„æ–¹å‘ï¼Œå¹¶å°†æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªç»´åº¦ä¸å¤§äºåŸå§‹æ•°æ®çš„æ–°çš„å­ç©ºé—´ä¸Šã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»¥æ–°ç‰¹å¾çš„åæ ‡æ˜¯ç›¸äº’æ­£äº¤ä¸ºçº¦æŸæ¡ä»¶ï¼Œæ–°çš„å­ç©ºé—´ä¸Šæ­£äº¤çš„åæ ‡è½´(ä¸»æˆåˆ†)å¯è¢«è§£é‡Šä¸ºæ–¹å·®æœ€å¤§çš„æ–¹å‘ã€‚åœ¨æ­¤ï¼Œ$x_1$ å’Œ $x_2$ ä¸ºåŸå§‹ç‰¹å¾çš„åæ ‡è½´ï¼Œè€Œ PC1 å’Œ PC2 å³ä¸ºä¸»æˆåˆ†ã€‚

![](https://imgconvert.csdnimg.cn/aHR0cDovL2dpdGh1Yi5jb20vbWluZ21pbnl1L2ltYWdlcy9yYXcvbWFzdGVyL3B5dGhvbi1tYWNoaW5lLWxlYXJuaW5nL2NoMDUvNS0xLmpwZw)

å¦‚æœä½¿ç”¨ PCA é™ç»´ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ª $d \times k$çš„è½¬æ¢çŸ©é˜µ $W$ã€‚è¿™æ ·å°±å¯ä»¥å°†ä¸€ä¸ªæ ·æœ¬å‘é‡ $x$ æ˜ å°„åˆ°ä¸€ä¸ªæ–°çš„ $k$ ç»´ç‰¹å¾å­ç©ºé—´ä¸Šå»ï¼Œæ­¤ç©ºé—´çš„ç»´åº¦å°äºåŸå§‹çš„ $d$ ç»´ç‰¹å¾ç©ºé—´:

$$
\begin{align}
x &= [x_1, x_2, ..., x_d], \quad x \in R^d \\
&  \downarrow x^W, \quad W \in R^{d \times k} \\
z &= [z_1, z_2, ..., z_k], \quad z \in R^k
\end{align}
$$

å®Œæˆä»åŸå§‹ $d$ ç»´æ•°æ®åˆ°æ–°çš„ $k$ ç»´å­ç©ºé—´(ä¸€èˆ¬æƒ…å†µä¸‹ $k << d$) çš„è½¬æ¢åï¼Œç¬¬ä¸€ä¸»æˆåˆ†çš„æ–¹å·®åº”è¯¥æ˜¯æœ€å¤§çš„ï¼Œç”±äºå„å¤§ä¸»æˆåˆ†ä¹‹é—´æ˜¯ä¸ç›¸å…³çš„(æ­£äº¤çš„)ï¼Œåç»­å„ä¸»æˆåˆ†ä¹Ÿå…·å¤‡å°½å¯èƒ½å¤§æ–¹å·®ã€‚éœ€æ³¨æ„çš„æ˜¯ï¼Œä¸»æˆåˆ†çš„æ–¹å‘å¯¹æ•°æ®å€¼çš„èŒƒå›´é«˜åº¦æ•æ„Ÿï¼Œå¦‚æœç‰¹å¾çš„å€¼ä½¿ç”¨ä¸åŒçš„åº¦é‡æ ‡å‡†ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œä»¥è®©å„ç‰¹å¾å…·æœ‰ç›¸åŒçš„é‡è¦æ€§ã€‚

åœ¨è¯¦ç»†è®¨è®ºä½¿ç”¨ PCA ç®—æ³•é™ç»´ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤æ¥æ¦‚æ‹¬ä»¥ä¸‹ç®—æ³•çš„æµç¨‹:

1. å¯¹åŸå§‹ $d$ ç»´æ•°æ®é›†åšæ ‡å‡†åŒ–å¤„ç†ã€‚
2. æ„é€ æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µã€‚
3. è®¡ç®—åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç›¸åº”çš„ç‰¹å¾å‘é‡ã€‚
4. é€‰æ‹©ä¸å‰ $k$ ä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå…¶ä¸­ $k$ ä¸ºæ–°ç‰¹å¾ç©ºé—´çš„ç»´åº¦($k \leq d$)ã€‚
5. é€šè¿‡å‰ $k$ ä¸ªç‰¹å¾å‘é‡æ„é€ æ˜ å°„çŸ©é˜µ $W$ã€‚
6. é€šè¿‡æ˜ å°„çŸ©é˜µ $W$ å°† d ç»´çš„è¾“å…¥æ•°æ®é›† $X$ è½¬æ¢åˆ°æ–°çš„ $k$ ç»´ç‰¹å¾å­ç©ºé—´ã€‚

## 5.1.1 æ€»ä½“æ–¹å·®ä¸è´¡çŒ®æ–¹å·®

åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ä¸»æˆåˆ†åˆ†æç®—æ³•çš„å‰å››ä¸ªæ­¥éª¤: æ•°æ®æ ‡å‡†åŒ–ã€æ„é€ åæ–¹å·®çŸ©é˜µã€è·å¾—åæ–¹å·®ä¸¾ç€ä½ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œä»¥åŠæŒ‰é™åºæ’åˆ—ç‰¹å¾å€¼å¯¹æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œä»¥åŠæŒ‰é™åºæ’åˆ—ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡:

é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½ç¬¬4ç« å·²ç»ä½¿ç”¨è¿‡çš„è‘¡è„é…’æ•°æ®é›†:

```python
import pandas as pd
df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è‘¡è„é…’æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†â€”â€”åˆ†åˆ«å æ•°æ®é›†çš„ 70% å’Œ 30%ï¼Œå¹¶ä½¿ç”¨å•ä½æ–¹å·®å°†å…¶æ ‡å‡†åŒ–ã€‚

```python
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler

X, y = df_wine.loc[:, 1:].values, df_wind.loc[:, 0].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)
X_test_std = sc.fit_transform(X_test)
```

é€šè¿‡ä¸Šè¿°ä»£ç å®Œæˆæ•°æ®é¢„å¤„ç†åï¼Œæˆ‘ä»¬è¿›å…¥ç¬¬äºŒæ­¥: æ„é€ åæ–¹å·®çŸ©é˜µã€‚æ­¤ $d \times d$ ç»´åæ–¹å·®çŸ©é˜µæ˜¯æ²¿ä¸»å¯¹è§’çº¿å¯¹ç§°çš„ï¼Œå…¶ä¸­ $d$ ä¸ºæ•°æ®é›†çš„ç»´åº¦ï¼Œæ­¤è°¨éµæˆå¯¹åœ°å­˜å‚¨äº†ä¸åŒç‰¹å¾ä¹‹é—´çš„åæ–¹å·®ã€‚ä¾‹å¦‚ï¼Œå¯¹ç¾¤ä½“è¿›è¡Œæè¿°çš„ä¸¤ä¸ªç‰¹å¾ $x_j$ å’Œ $x_k$ å¯é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—å®ƒä»¬ä¹‹é—´çš„åæ–¹å·®:

$$\sigma_{jk} = \frac {1}{n} \sum_{i=1}^n (x_j^{(i)} - \mu_j)(x_k^{(i)} - \mu_k)$$

åœ¨æ­¤ï¼Œ$\mu_j$ å’Œ $\mu_k$ åˆ†åˆ«ä¸ºç‰¹å¾ $j$ å’Œ $k$ çš„å‡å€¼ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†åšäº†æ ‡å‡†åŒ–å¤„ç†ï¼Œæ ·æœ¬çš„å‡å€¼å°†ä¸ºé›¶ã€‚ä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„åæ–¹å·®å¦‚æœä¸ºæ­£ï¼Œè¯´æ˜å®ƒä»¬ä¼šåŒæ—¶å¢åŠ å¢å‡ï¼Œè€Œä¸€ä¸ªè´Ÿçš„åæ–¹å·®å€¼åˆ™è¡¨ç¤ºä¸¤ä¸ªç‰¹å¾ä¼šæœç›¸åçš„æ–¹å‘å˜åŠ¨ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåŒ…å«ä¸‰ä¸ªç‰¹å¾çš„åæ–¹å·®çŸ©é˜µå¯è®°ä¸º(æ³¨æ„: æ­¤å¤„ $\Sigma$ ä»£è¡¨å¸Œè…Šå­—æ¯ simgaï¼Œåœ¨æ­¤è¯·å‹¿æ±‚å’Œç¬¦å·æ··ä¸ºä¸€è°ˆ):

$$
\Sigma = \begin{bmatrix} 
\sigma_1^2 & \sigma_{12} & \sigma_{13} \\
\sigma_{21} & \sigma_2^2 & \sigma_{23} \\
\sigma_{31} & \sigma_{32} & \sigma_3^2
\end{bmatrix}
$$

åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ä»£è¡¨ä¸»æˆåˆ†(æœ€å¤§æ–¹å·®æ–¹å‘)ï¼Œè€Œå¯¹åº”çš„ç‰¹å¾å€¼å¤§å°å°±å†³å®šäº†ç‰¹å¾å‘é‡çš„é‡è¦æ€§ã€‚å°±è‘¡è„é…’æ•°æ®é›†çœ‹æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ° $13 \times 13$ ç»´åæ–¹å·®çŸ©é˜µçš„13ä¸ªç‰¹å¾å‘é‡åŠå…¶å¯¹åº”çš„ç‰¹å¾å€¼ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æ¥è®¡ç®—åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å¯¹ã€‚é€šè¿‡çº¿æ€§ä»£æ•°æˆ–å¾®ç§¯åˆ†çš„ç›¸å…³çŸ¥è¯†æˆ‘ä»¬çŸ¥é“ï¼Œç‰¹å¾å€¼ V éœ€æ»¡è¶³å¦‚ä¸‹æ¡ä»¶:

$$
\Delta \nu = \lambda \nu
$$
æ­¤å¤„çš„ç‰¹å¾å€¼ $\lambda$ æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚ç”±äºæ‰‹åŠ¨è®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ä»æŸç§ç¨‹åºä¸Šæ¥è¯´ï¼Œæ˜¯ä¸€é¡¹ç¹çä¸”å¤æ‚çš„å·¥ä½œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ NumPy ä¸­çš„ linalg.eig å‡½æ•°æ¥è®¡ç®—è‘¡è„é…’æ•°æ®é›†åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å¯¹:

```python
>>> import numpy as np
>>> cov_mat = np.cov(X_train_std.T)
>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
>>> print('\nEigenvalues \n%s' % eigen_vals)
Eigenvalues 
[4.8923083  2.46635032 1.42809973 1.01233462 0.84906459 0.60181514
 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212
 0.2399553 ]
```

åº”ç”¨ numpy.cov å‡½æ•°ï¼Œæˆ‘ä»¬è®¡ç®—å¾—åˆ°äº†ç»æ ‡å‡†åŒ–å¤„ç†çš„è®­ç»ƒæ•°æ®é›†çš„åæ–¹å·®çŸ©é˜µã€‚ä½¿ç”¨ linalg.eig å‡½æ•°ï¼Œé€šè¿‡ç‰¹å¾åˆ†è§£ï¼Œå¾—åˆ°ä¸€ä¸ªåŒ…å« 13 ä¸ªç‰¹å¾å€¼çš„å‘é‡(eigen_vals)ï¼ŒåŠå…¶å¯¹åº”çš„ç‰¹å¾å€¼ï¼Œç‰¹å¾å‘é‡ä»¥åˆ—çš„æ–¹å¼å­˜å‚¨ä¸€ä¸ª $13 \times 13$ ç»´çš„çŸ©é˜µä¸­(eigen_vecs)ã€‚

å› ä¸ºè¦å°†æ•°æ®å‹ç¼©åˆ°ä¸€ä¸ªæ–°çš„ç‰¹å¾å­ç©ºé—´ä¸Šæ¥å®ç°æ•°æ®é™ç»´ï¼Œæ‰€ä»¥æˆ‘ä»¬åªé€‰æ‹©é‚£äº›åŒ…å«æœ€å¤šä¿¡æ¯(æ–¹å·®æœ€å¤§)çš„ç‰¹å¾å‘é‡(ä¸»æˆåˆ†)ç»„æˆå­é›†ã€‚ç”±äºç‰¹å¾å€¼çš„å¤§å°å†³å®šäº†ç‰¹å¾å‘é‡çš„é‡è¦æ€§ï¼Œå› æ­¤éœ€è¦å°†ç‰¹å¾å€¼æŒ‰é™åºæ’åˆ—ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯æ’åºåœ¨å‰ $k$ ä¸ªçš„ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚åœ¨æ•´ç†åŒ…å«ä¿¡æ¯æœ€å¤§çš„å‰ $k$ ä¸ªç‰¹å¾å‘é‡å‰ï¼Œæˆ‘ä»¬å…ˆå›æ‰§ç‰¹å¾å€¼çš„æ–¹å·®è´¡çŒ®ç‡(variable explained ratios) å›¾åƒã€‚

ç‰¹å¾å€¼ $\lambda_j$ çš„æ–¹å·®è´¡çŒ®ç‡æ˜¯æŒ‡ï¼Œç‰¹å¾å€¼ $\lambda_j$ ä¸æ‰€æœ‰ç‰¹å¾å€¼å’Œçš„æ¯”å€¼:

$$
\frac {\lambda_j} {\sum_{j=1}^d \lambda_j}
$$
ä½¿ç”¨ NumPy çš„ cumsum å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºç´¯è®¡æ–¹å·®ï¼Œå…¶å›¾åƒå¯é€šè¿‡ matplotlib çš„ step å‡½æ•°ç»˜åˆ¶:

```python
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp)
import matplotlib.pyplot as plt
plt.bar(range(1, 14), var_exp, alpha=0.5, align='center', 
        label='individual explained variabce')
plt.step(range(1, 14), cum_var_exp, where='mid',
    label='cumulative explained variable')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.show()
```
ç”±ä¸‹å›¾å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ä¸€ä¸»æˆåˆ†å æ–¹å·®æ€»å’Œçš„ 40% å·¦å³ï¼›æ­¤å¤–ï¼Œè¿˜å¯ä»¥çœ‹å‡ºå‰ä¸¤ä¸ªä¸»æˆåˆ†å æ€»ä½“æ–¹å·®çš„è¿‘ 60%:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232237240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

è™½ç„¶æ–¹å·®è´¡çŒ®ç‡å›¾åƒå¯ä»¥è®©æˆ‘ä»¬è”æƒ³åˆ°ç¬¬4ç« ä¸­é€šè¿‡éšæœºæ£®æ—è®¡ç®—å‡ºçš„å…³äºç‰¹å¾çš„é‡è¦ç¨‹åº¦ï¼Œä½†æˆ‘ä»¬åº”è¯¥æ³¨æ„: PCA æ˜¯ä¸€ç§æ— ç›‘ç£æ–¹æ³•ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å¿½ç•¥ç±»æ ‡ä¿¡æ¯ã€‚ç›¸å¯¹è€Œè¨€ï¼Œéšæœºæ£®æ—é€šè¿‡ç±»æ ‡ä¿¡æ¯æ¥è®¡ç®—åŠèŠ‚ç‚¹ä¸çº¯åº¦ï¼Œè€Œæ–¹å·®åº¦é‡çš„æ˜¯ç‰¹å¾å€¼åœ¨è½´çº¿ä¸Šçš„åˆ†å¸ƒã€‚

## 5.1.2 ç‰¹å¾è½¬æ¢

åœ¨å°†æ–¹å·®çŸ©é˜µåˆ†è§£ä¸ºç‰¹å¾å¯¹åï¼Œæˆ‘ä»¬ç»§ç»­æ‰§è¡Œ PCA æ–¹æ³•çš„æœ€åä¸‰ä¸ªæ­¥éª¤ï¼Œå°†è‘¡è„é…’æ•°æ®é›†ä¸­çš„ä¿¡æ¯è½¬æ¢åˆ°æ–°çš„ä¸»æˆåˆ†è½´ä¸Šã€‚åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ç‰¹å¾å€¼æŒ‰é™åºè¿›è¡Œæ’åˆ—ï¼Œå¹¶é€šè¿‡æŒ‘é€‰å‡ºå¯¹åº”çš„ç‰¹å¾å‘é‡æ„é€ å‡ºæ˜ å°„çŸ©é˜µï¼Œç„¶åä½¿ç”¨æ˜ å°„çŸ©é˜µè½¬æ¢åˆ°ä½ç»´çš„å­ç©ºé—´ä¸Šã€‚

é¦–å…ˆï¼ŒæŒ‰ç‰¹å¾å€¼çš„é™åºæ’åˆ—ç‰¹å¾å¯¹:

```python
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) 
    for i in range(len(eigen_vals))]
eigen_pairs.sort(reverse=True)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€‰å–ä¸¤ä¸ªå¯¹åº”çš„ç‰¹å¾å€¼æœ€å¤§çš„ç‰¹å¾å‘é‡ï¼Œè¿™ä¸¤ä¸ªå€¼ä¹‹å’Œå æ®äº†æ•°æ®é›†æ€»ä½“æ–¹å·®çš„ 60%ã€‚è¯·æ³¨æ„ï¼Œå‡ºäºæ¼”ç¤ºçš„éœ€è¦ï¼Œæˆ‘ä»¬åªé€‰æ‹©äº†ä¸¤ä¸ªç‰¹å¾å‘é‡ï¼Œå› ä¸ºåœ¨æœ¬å°èŠ‚ä¸­æˆ‘ä»¬å°†ä»¥äºŒç»´æ•£ç‚¹å›¾çš„æ–¹å¼å›æ‰§ç›¸å…³å›¾åƒã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç¡®å®šä¸»æˆåˆ†çš„æ•°é‡æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å®é™…æƒ…å†µåœ¨è®¡ç®—æ•ˆç‡ä¸åˆ†ç±»å™¨æ€§èƒ½ä¹‹é—´åšå‡ºæƒè¡¡ã€‚

```python
>>> w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
            eigen_pairs[1][1][:, np.newaxis]))
>>> print('Matrix W: \n', w)
Matrix W: 
 [[ 0.14669811  0.50417079]
 [-0.24224554  0.24216889]
 [-0.02993442  0.28698484]
 [-0.25519002 -0.06468718]
 [ 0.12079772  0.22995385]
 [ 0.38934455  0.09363991]
 [ 0.42326486  0.01088622]
 [-0.30634956  0.01870216]
 [ 0.30572219  0.03040352]
 [-0.09869191  0.54527081]
 [ 0.30032535 -0.27924322]
 [ 0.36821154 -0.174365  ]
 [ 0.29259713  0.36315461]]
```


æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œé€šè¿‡é€‰å–çš„ä¸¤ä¸ªç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ª $13 \times 12$ ç»´çš„æ˜ å°„çŸ©é˜µ $W$ã€‚é€šè¿‡æ˜ å°„çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªæ ·æœ¬ $x$(ä»¥13ç»´çš„è¡Œå‘é‡è¡¨ç¤º) è½¬æ¢åˆ° PCA çš„å­ç©ºé—´ä¸Šå¾—åˆ° $x'$ï¼Œæ ·æœ¬è½¬æ¢ä¸ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ–°ç‰¹å¾çš„äºŒç»´å‘é‡:

$$
x'=xW
$$

```python
>>> X_train_std[0].dot(w)
array([2.59891628, 0.00484089])
```

ç±»ä¼¼åœ°ï¼Œé€šè¿‡è®¡ç®—çŸ©é˜µçš„ç‚¹ç§¯ï¼Œæˆ‘ä»¬å¯ä»¥å°† $124 \times 13$ ç»´çš„è®­ç»ƒæ•°æ®é›†è½¬æ¢åˆ°åŒ…å«ä¸¤ä¸ªä¸»æˆåˆ†çš„å­ç©ºé—´ä¸Š:

$$
X'=XW
$$

```python
>>> X_train_pca = X_train_std.dot(w)
```

æœ€åï¼Œè½¬æ¢åçš„è‘¡è„é…’æ•°æ®é›†å°†ä»¥ $124 \times 2$ ç»´çŸ©é˜µçš„æ–¹å¼å­˜å‚¨ï¼Œæˆ‘ä»¬ä»¥äºŒç»´æ•£ç‚¹å›¾çš„æ–¹å¼æ¥å¯¹å…¶è¿›è¡Œå¯è§†åŒ–å±•ç¤º:

```python
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_pca[y_train == l, 0],
                X_train_pca[y_train == l, 1],
                c=c, label=l, marker=m)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(loc='lower left')
plt.show()
```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232257555.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


ä¸ºäº†æ¼”ç¤ºæ•£ç‚¹å›¾ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç±»æ ‡ä¿¡æ¯ï¼Œä½†è¯·æ³¨æ„ PCA æ˜¯æ— ç›‘ç£æ–¹æ³•ï¼Œæ— éœ€ç±»æ ‡ä¿¡æ¯ã€‚

## 5.1.3 ä½¿ç”¨ scikit-learn è¿›è¡Œä¸»æˆåˆ†åˆ†æ

é€šè¿‡ä¸Šä¸€å°èŠ‚ä¸­è¯¦å°½çš„ä»‹ç»ï¼Œæˆ‘ä»¬äº†è§£äº† PCA å†…éƒ¨çš„å·¥ä½œåŸç†ï¼Œæ¥ä¸‹æ¥è®¨è®ºå¦‚ä½•ä½¿ç”¨ scikit-learn ä¸­æä¾›çš„ PCA ç±»ã€‚PCA ä¹Ÿæ˜¯ scikit-learn ä¸­çš„ä¸€ä¸ªæ•°æ®è½¬æ¢ç±»ï¼Œåœ¨ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å‚æ•°å¯¹è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®è¿›è¡Œè½¬æ¢ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹è¿›è¡Œæ‹Ÿåˆã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn ä¸­çš„ PCA å¯¹è‘¡è„é…’æ•°æ®é›†åšé¢„å¤„ç†ï¼Œç„¶åä½¿ç”¨é€»è¾‘æ–¯ç‰¹å›å½’å¯¹è½¬æ¢åçš„æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œæœ€åç”¨ç¬¬2ç« å®šä¹‰çš„ plot_decision_region å‡½æ•°å¯¹å†³ç­–åŒºåŸŸè¿›è¡Œå¯è§†åŒ–å±•ç¤ºã€‚

```python
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def plot_decision_regions(X, y, classifier, resolution=0.02):
    # setup markder generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors=  ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
            np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
            alpha=0.8, c=cmap(idx), 
            marker=markers[idx], label=cl)

from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
lr = LogisticRegression()
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)
lr.fit(X_train_pca, y_train)
plot_decision_regions(X_train_pca, y_train, classifier=lr)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(loc='lower left')
plt.show()
```

æ‰§è¡Œä¸Šè¿°ä»£ç åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å°†è®­ç»ƒæ•°æ®è½¬æ¢åˆ°ä¸¤ä¸ªä¸»æˆåˆ†è½´åç”Ÿæˆçš„å†³ç­–åŒºåŸŸã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232508970.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


å¦‚æœæ¯”è¾ƒ scikit-learn ä¸­æä¾›çš„ PCA ç±»ä¸æˆ‘ä»¬è‡ªå·±å®ç°çš„ PCA ç±»çš„åˆ†æç»“æœï¼Œå¯ä»¥å‘ç°: ä¸Šå›¾å¯ä»¥çœ‹ä½œæ˜¯æˆ‘ä»¬æ­¤å‰è‡ªå·±å®Œæˆ PCA æ–¹æ³•æ‰€å¾—åˆ°æ²¿ y è½´è¿›è¡Œæ—‹è½¬åçš„ç»“æœã€‚å‡ºç°æ­¤å·®å¼‚çš„åŸå› ï¼Œä¸æ˜¯ä¸¤ç§æ–¹æ³•åœ¨å®ç°ä¸­å‡ºç°äº†ä»€ä¹ˆé”™è¯¯ï¼Œè€Œåœ¨äºç‰¹å¾åˆ†ææ–¹æ³•: ç‰¹å¾å‘é‡å¯ä»¥ä¸ºæ­£æˆ–è€…ä¸ºè´Ÿã€‚è¿™ä¸æ˜¯é‡ç‚¹ï¼Œå› ä¸ºæœ‰éœ€è¦æ—¶æˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®ä¸Šä¹˜ä»¥ -1 æ¥å®ç°å›¾åƒçš„é•œåƒã€‚æ³¨æ„ï¼Œç‰¹å¾å‘é‡é€šå¸¸ä¼šç¼©æ”¾åˆ°å•ä½é•¿åº¦1ã€‚ä¸ºäº†ä¿è¯æ•´ä¸ªåˆ†æè¿‡ç¨‹çš„å®Œæ•´æ€§ï¼Œæˆ‘ä»¬ç»˜åˆ¶ä¸€ä¸‹é€»è¾‘æ–¯ç‰¹å›å½’åœ¨è½¬æ¢åçš„æµ‹è¯•æ•°æ®ä¸Šæ‰€å¾—åˆ°çš„å†³ç­–åŒºåŸŸï¼Œçœ‹å…¶æ˜¯å¦èƒ½å¾ˆå¥½åœ°å°†å„ç±»åˆ†å¼€:

```python
plot_decision_regions(X_test_pca, y_test, classifier=lr)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(loc='lower left')
plt.show()
```

æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œæˆ‘ä»¬ç»˜åˆ¶å‡ºäº†æµ‹è¯•é›†çš„å†³ç­–åŒºåŸŸï¼Œå¯ä»¥çœ‹åˆ°: é€»è¾‘æ–¯ç‰¹å›å½’åœ¨è¿™ä¸ªå°çš„äºŒç»´ç‰¹å¾å­ç©ºé—´ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåªè¯¯åˆ¤äº†ä¸€ä¸ªæ ·æœ¬ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232525298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

å¦‚æœæˆ‘ä»¬é˜Ÿä¸åŒä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®ç‡æ„Ÿå…´è¶£ï¼Œå¯ä»¥å°† PCA ç±»ä¸­çš„ n_components å‚æ•°è®¾ç½®ä¸º Noneã€‚ç”±æ­¤ï¼Œå¯ä»¥ä¿ç•™æ‰€æœ‰çš„ä¸»æˆåˆ†ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ explained_variance_ratio_ å±æ€§å¾—åˆ°ç›¸åº”çš„æ–¹å·®è´¡çŒ®ç‡:

```python
>>> pca = PCA(n_components=None)
>>> X_train_pca = pca.fit_transform(X_train_std)
>>> pca.explained_variance_ratio_
array([0.37329648, 0.18818926, 0.10896791, 0.07724389, 0.06478595,
       0.04592014, 0.03986936, 0.02521914, 0.02258181, 0.01830924,
       0.01635336, 0.01284271, 0.00642076])
```

è¯·æ³¨æ„ï¼Œåœ¨åˆå§‹åŒ– PCA ç±»æ—¶ï¼Œå¦‚æœæˆ‘ä»¬å°† n_components è®¾ç½®ä¸º Noneï¼Œé‚£ä¹ˆå®ƒå°†æŒ‰ç…§æ–¹å·®è´¡çŒ®ç‡é€’å‡é¡ºåºè¿”å›æ‰€æœ‰çš„ä¸»æˆåˆ†ï¼Œè€Œä¸æ˜¯è¿›è¡Œé™ç»´æ“ä½œã€‚

# 5.2 é€šè¿‡çº¿æ€§åˆ¤åˆ«åˆ†æå‹ç¼©æ— ç›‘ç£æ•°æ®

çº¿æ€§åˆ¤åˆ«åˆ†æ(Linear Discriminant Analysisï¼ŒLDA) æ˜¯ä¸€ç§å¯ä½œä¸ºç‰¹å¾æŠ½å–çš„æŠ€æœ¯ï¼Œå®ƒå¯ä»¥æé«˜æ•°æ®åˆ†æè¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ï¼Œå¯¹äºä¸é€‚ç”¨æ­£åˆ™åŒ–çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥é™ä½å› ç»´åº¦ç¾éš¾å¸¦æ¥çš„è¿‡æ‹Ÿåˆã€‚

LDA çš„åŸºæœ¬æ¦‚å¿µä¸ PCA éå¸¸ç›¸ä¼¼ï¼ŒPCA è§†å›¾åœ¨æ•°æ®é›†ä¸­æ‰¾åˆ°æ–¹å·®æœ€å¤§çš„æ­£äº¤çš„ä¸»æˆåˆ†åˆ†é‡çš„è½´ï¼Œè€Œ LDA çš„ç›®æ ‡æ˜¯å‘ç°å¯ä»¥æœ€ä¼˜åŒ–åˆ†ç±»çš„ç‰¹å¾å­ç©ºé—´ã€‚LDA ä¸ PCA éƒ½æ˜¯å¯ç”¨äºé™ä½æ•°æ®é›†ç»´åº¦çš„çº¿æ€§è½¬æ¢æŠ€å·§ã€‚å…¶ä¸­ï¼ŒPCA æ˜¯æ— ç›‘ç£ç®—æ³•ï¼Œè€Œ LDA æ˜¯ç›‘ç£ç®—æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·ç›´è§‚åœ°è®¤ä¸º: ä¸ PCA ç›¸æ¯”ï¼ŒLDA æ˜¯ä¸€ç§æ›´ä¼˜è¶Šçš„ç”¨äºåˆ†ç±»çš„ç‰¹å¾æå–æŠ€æœ¯ã€‚ä½†æ˜¯ A.M.Martinez æå‡º: åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­çš„æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æ¯ä¸ªç±»åˆ«ä¸­åªæœ‰å°‘é‡æ ·æœ¬ï¼Œä½¿ç”¨ PCA ä½œä¸ºé¢„å¤„ç†å·¥å…·çš„åˆ†ç±»ç»“æœæ›´ä½³$^{æ³¨1}$ã€‚

ã€ŒğŸ”– LDA æœ‰æ—¶ä¹Ÿç§°ä½œ Fisher LDAï¼ŒRonald A.Fisher äº 1937å¹´é’ˆå¯¹äºŒç±»åˆ«é—®é¢˜å¯¹ Fisher çº¿æ€§åˆ¤åˆ«(Fisher Linear Discriminant) åšäº†æœ€åˆçš„å½¢å¼åŒ–$^{æ³¨2}$ã€‚1948å¹´ï¼ŒåŸºäºç±»åˆ«æ–¹å·®ç›¸ç­‰å’Œç±»å†…æ ·æœ¬å‘ˆç°æ ‡å‡†æ­£å¤ªåˆ†å¸ƒçš„å‡è®¾ï¼ŒRadhakrishna å°† Fisher LDA æ³›åŒ–åˆ°äº†å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ä¸Šï¼Œå³æˆ‘ä»¬ç°åœ¨æ‰€è¯´çš„ LDA$^{æ³¨3}$ã€‚ã€

> æ³¨1: A.M.Martinez and A.C.Kak.PCA VerSUS LDA. Pattern Analysis and Machine Intelligenceï¼ŒIEEE Transactions onï¼Œ23(2): 228-223ï¼Œ2001.
> æ³¨2: R.A. Fisher. The Use of Multiple Measurements in Taxonomic Problems. Annals of Eugenicsï¼Œ7(2): 179-188ï¼Œ1936.
> æ³¨3: C.R. Rao. The Utilization of Multiple Measurements in Problems of Biological Classification. Journal of the Royal Statistical Society. Series B(Methodological)ï¼Œ10(2): 159-203ï¼Œ1948.

ä¸‹å›¾è§£é‡Šäº†äºŒç±»åˆ«åˆ†ç±»ä¸­ LDA çš„æ¦‚å¿µã€‚ç±»åˆ«1ã€ç±»åˆ«2 ä¸­çš„æ ·æœ¬åˆ†åˆ«ç”¨å‰å·å’ŒåŸç‚¹æ¥è¡¨ç¤º:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232540862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåœ¨ x è½´æ–¹å‘(LD1)ï¼Œé€šè¿‡ç°è±¡åˆ¤å®šï¼Œå¯ä»¥å¾ˆå¥½åœ°å°†æ­£æ€åˆ†å¸ƒçš„ä¸¤ä¸ªç±»åˆ†å¼€ã€‚è™½ç„¶æ²¿ y è½´(LD2) æ–¹å‘çš„çº¿æ€§åˆ¤å®šä¿æŒäº†æ•°æ®é›†çš„è¾ƒå¤§æ–¹å·®ï¼Œä½†æ˜¯æ²¿æ­¤æ–¹å‘æ— æ³•æä¾›å…³äºç±»åˆ«åŒºåˆ†çš„ä»»ä½•ä¿¡æ¯ï¼Œå› æ­¤å®ƒä¸æ˜¯ä¸€ä¸ªå¥½çš„çº¿æ€§åˆ¤å®šã€‚

ä¸€ä¸ªå…³äº LDA çš„å‡è®¾å°±æ˜¯æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å˜‰å®šå„ç±»åˆ«ä¸­æ•°æ®å…·æœ‰ç›¸åŒçš„åæ–¹å·®çŸ©é˜µï¼Œä¸”æ ·æœ¬çš„ç‰¹å¾ä»ç»Ÿè®¡ä¸Šæ¥è®²æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ä¸è¿‡ï¼Œå³ä½¿ä¸€ä¸ªæˆ–å¤šä¸ªå‡è®¾æ²¡æœ‰æ»¡è¶³ï¼ŒLDA ä»æ—§å¯ä»¥å¾ˆå¥½åœ°å®Œæˆé™ç»´å·¥ä½œ$^{æ³¨4}$ã€‚

> æ³¨4: RR.O.Dudaï¼ŒP.E.Hartï¼Œand D.G.Stork.Pattern Classification.2nd.Edition.New York, 2001.

åœ¨è¿›å…¥ä¸‹ä¸€èŠ‚è¯¦ç»†è®¨è®º LDA çš„åŸç†ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥æ€»ç»“ä¸€ä¸ª LDA æ–¹æ³•çš„å…³é”®æ­¥éª¤:

1. å¯¹ $d$ ç»´æ•°æ®é›†è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†(d ä¸ºç‰¹å¾çš„æ•°é‡)
2. å¯¹äºæ¯ä¸€ç±»åˆ«ï¼Œè®¡ç®— $d$ ç»´çš„å‡å€¼å‘é‡ã€‚
3. æ„é€ ç±»é—´çš„æ•£æ­¥çŸ©é˜µ $S_B$ ä»¥åŠç±»å†…çš„æ•£æ­¥çŸ©é˜µ $S_W$ã€‚
4. è®¡ç®—çŸ©é˜µ $S_W^{-1}S_B$ çš„ç‰¹å¾å€¼åŠå¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚
5. é€‰å–å‰ $k$ ä¸ªç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ„é€ ä¸€ä¸ª $d \times k$ ç»´çš„è½¬æ¢çŸ©é˜µ $W$ï¼Œå…¶ä¸­ç‰¹å¾å‘é‡ä»¥åˆ—çš„å½¢å¼æ’åˆ—ã€‚
6. ä½¿ç”¨è½¬æ¢çŸ©é˜µ $W$ å°†æ ·æœ¬æ˜ å°„åˆ°æ–°çš„ç‰¹å¾å­ç©ºé—´ä¸Šã€‚

> ğŸ”– ç‰¹å¾å‘ˆæ­£æ€åˆ†å¸ƒä¸”ç‰¹å¾é—´ç›¸äº’ç‹¬ç«‹æ˜¯æˆ‘ä»¬ä½¿ç”¨ LDA æ—¶æ‰€ä½œçš„å‡è®¾ã€‚åŒæ—¶ï¼ŒLDA ç®—æ³•å‡å®šå„ä¸ªç±»åˆ«çš„åæ–¹å·®çŸ©é˜µæ˜¯ä¸€è‡´çš„ã€‚ç„¶è€Œï¼Œå³ä½¿æˆ‘ä»¬è¿èƒŒäº†ä¸Šè¿°å‡è®¾ï¼ŒLDA ç®—æ³•ä»æ—§èƒ½å¾ˆå¥½åœ°å®Œæˆæ•°æ®é™ç»´åŠåˆ†ç±»ä»»åŠ¡(R.O.Dudaï¼ŒP.E.Hartï¼Œand D.G.Stork.Pattern Classification.2nd.Edition.New York, 2001)ã€‚


## 5.2.1 è®¡ç®—æ•£æ­¥çŸ©é˜µ

æœ¬èŠ‚ä¼Šå§‹ï¼Œåœ¨è®²è§£ PCA æ—¶å°±å¯¹æ™®æ³¡é…’æ•°æ®é›†åšäº†æ ‡å‡†åŒ–å¤„ç†ï¼Œå› æ­¤æˆ‘ä»¬å°†è·³è¿‡ç¬¬ä¸€æ­¥ç›´æ¥è®¡ç®—å‡å€¼å‘é‡ã€‚è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬å°†åˆ†åˆ«æ„å»ºå†…æ•£å¸ƒçŸ©é˜µå’Œç±»é—´æ•£å¸ƒçŸ©é˜µã€‚å‡å€¼å‘é‡ $m_i$ å­˜å‚¨äº†ç±»åˆ« $i$ ä¸­æ ·æœ¬çš„ç‰¹å¾å‡å€¼ $\mu_m$:

$$
m_i = \frac {1}{n} \sum_{x \in D_i}^c x_m
$$
è‘¡è„é…’æ•°æ®é›†çš„ä¸‰ä¸ªç±»åˆ«å¯¹åº”ä¸‰ä¸ªå‡å€¼å‘é‡:

$$
m_i = 
\begin{bmatrix}
\mu_{i, alcohol} \\
\mu_{i, malic acid} \\
\vdots \\
\mu_{i, proline}
\end{bmatrix}
\quad i \in \{1,2,3\}
$$


```python
np.set_printoptions(precision=4)
mean_vecs = []
for label in range(1, 4):
    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))
    print('MV %s: %s\n' % (label, mean_vecs[label - 1]))
    
# è¾“å‡º:
MV 1: [ 0.9259 -0.3091  0.2592 -0.7989  0.3039  0.9608  1.0515 -0.6306  0.5354
  0.2209  0.4855  0.798   1.2017]

MV 2: [-0.8727 -0.3854 -0.4437  0.2481 -0.2409 -0.1059  0.0187 -0.0164  0.1095
 -0.8796  0.4392  0.2776 -0.7016]

MV 3: [ 0.1637  0.8929  0.3249  0.5658 -0.01   -0.9499 -1.228   0.7436 -0.7652
  0.979  -1.1698 -1.3007 -0.3912]
```

é€šè¿‡å‡å€¼å‘é‡ï¼Œæˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹ç±»å†…æ•£å¸ƒçŸ©é˜µ $S_W$:

$$
S_W = \sum_{i=1}^c S_i
$$
è¿™å¯ä»¥é€šè¿‡ç´¯åŠ å„ç±»åˆ« $i$ çš„æ•£æ­¥çŸ©é˜µ $S_i$ æ¥è®¡ç®—:

$$
S_i = \sum_{x \in D_i}^c (x-m_i)(x-m_i)^T
$$

```python
d = 13 # number of features
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scatter = np.zeros((d, d))
    for row in X[y == label]:
        row, mv = row.reshape(d, 1), mv.reshape(d, 1)
        class_scatter += (row - mv).dot((row - mv).T)
    S_W += class_scatter
print('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))

# è¾“å‡º:
Within-class scatter matrix: 13x13
```

æ­¤å‰æˆ‘ä»¬é˜Ÿæ•£å¸ƒçŸ©é˜µè¿›è¡Œè®¡ç®—æ—¶ï¼Œæ›¾å‡è®¾è®­ç»ƒé›†çš„ç±»æ ‡æ˜¯å‡åŒ€åˆ†å¸ƒçš„ã€‚ä½†æ˜¯ï¼Œé€šè¿‡æ‰“å°ç±»æ ‡çš„æ•°é‡ï¼Œå¯ä»¥çœ‹åˆ°åœ¨æ­¤å¹¶æœªéµå¾ªæ­¤å‡è®¾:

```python
>>> print('Class label distribution: %s' % np.bincount(y_train)[1:])
Class label distribution: [40 49 35]
```

å› æ­¤ï¼Œåœ¨æˆ‘ä»¬é€šè¿‡ç´¯åŠ æ–¹å¼è®¡ç®—æ•£å¸ƒçŸ©é˜µ $S_W$ å‰ï¼Œéœ€è¦å¯¹å„ç±»åˆ«çš„æ•£æ­¥çŸ©é˜µ $S_i$ åšç¼©æ”¾å¤„ç†ã€‚å½“æˆ‘ä»¬ç”¨å„ç±»åˆ«å•ç‹¬çš„æ•£å¸ƒçŸ©é˜µé™¤ä»¥æ­¤ç±»åˆ«å†…æ ·æœ¬çš„æ•°é‡ $N_i$ æ—¶ï¼Œå¯ä»¥å‘ç°è®¡ç®—æ•£å¸ƒçŸ©é˜µçš„æ–¹å¼ä¸è®¡ç®—åæ–¹å·®çŸ©é˜µ $\sum_i$ çš„æ–¹å¼æ˜¯ä¸€è‡´çš„ã€‚åæ–¹å·®çŸ©é˜µå¯ä»¥çœ‹ä½œæ˜¯å½’ä¸€åŒ–çš„æ•£å¸ƒçŸ©é˜µ:

$$
\sum_i = \frac {1}{N_i} S_w= \frac {1}{N_i} \sum_{x \in d_i}^c (x-m_i)(x-m_i)^T
$$

```python
d = 13 # number of features
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scatter = np.cov(X_train_std[y_train == label].T)
    S_W += class_scatter

print('Scaled within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))

# è¾“å‡º:
Scaled within-class scatter matrix: 13x13
```

åœ¨å®Œæˆç±»å†…æ•£å¸ƒçŸ©é˜µ(æˆ–åæ–¹å·®çŸ©é˜µ)çš„è®¡ç®—åï¼Œæˆ‘ä»¬è¿›å…¥ä¸‹ä¸€æ­¥éª¤ï¼Œè®¡ç®—ç±»é—´æ•£å¸ƒçŸ©é˜µ $S_B$:

$$
S_B = \sum_{i=1}^c N_i (m_i - m)(m_i-m)^T
$$

å…¶ä¸­ï¼Œ$m$ ä¸ºå…¨å±€å‡å€¼ï¼Œå®ƒåœ¨è®¡ç®—æ—¶ç”¨åˆ°äº†æ‰€æœ‰ç±»åˆ«ä¸­çš„å…¨éƒ¨æ ·æœ¬:

```python
mean_overall = np.mean(X_train_std, axis=0)
d = 13 # number of features
S_B = np.zeros((d, d))
for i, mean_vec in enumerate(mean_vecs):
    n = X[y == i+1, :].shape[0]
    mean_vec = mean_vec.reshape(d, 1)
    mean_overall = mean_overall.reshape(d, 1)
    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)
print('Between-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))

# è¾“å‡º:
Between-class scatter matrix: 13x13
```

## 5.2.2 åœ¨æ–°ç‰¹å¾å­ç©ºé—´ä¸Šé€‰å–çº¿æ€§åˆ¤åˆ«ç®—æ³•

LDA ä½™ä¸‹çš„æ­¥éª¤ä¸ PCA çš„æ­¥éª¤ç›¸ä¼¼ã€‚ä¸è¿‡ï¼Œè¿™é‡Œæˆ‘ä»¬éƒ¨é˜Ÿåæ–¹å·®çŸ©é˜µåšç‰¹å¾åšç‰¹å¾åˆ†è§£ï¼Œè€Œæ˜¯æ±‚è§£çŸ©é˜µ $S_W^{-1}S_B$ å¹¿ä¹‰ç‰¹å¾å€¼:

```python
eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))
```

åœ¨æ±‚å¾—ç‰¹å¾ä¹‹åï¼Œæˆ‘ä»¬æŒ‰ç…§é™åºå¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åº:

```python
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) 
        				for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)
print('Eigenvalues in decreasing order:\n')
for eigen_val in eigen_pairs:
    print(eigen_val[0])
    
# è¾“å‡º:
Eigenvalues in decreasing order:

643.015384346051
225.0869818541626
8.792307609971557e-14
5.250276792004119e-14
5.2491045182795644e-14
5.2491045182795644e-14
5.062616992290714e-14
4.281024234637359e-14
4.281024234637359e-14
1.1103061461361521e-14
5.3026791025565126e-15
5.3026791025565126e-15
4.953358662764158e-15
```

ç†Ÿæ‚‰çº¿æ€§ä»£æ•°çš„è¯»è€…åº”è¯¥çŸ¥é“: $d \times d$ ç»´åæ–¹å·®çŸ©é˜µçš„ç§©æœ€å¤§ä¸º $d-1$ï¼Œè€Œä¸”ç¡®å®å¯ä»¥å‘ç°ï¼Œæˆ‘ä»¬åªå¾—åˆ°äº†ä¸¤ä¸ªéé›¶ç‰¹å¾å€¼(å®é™…å¾—åˆ°çš„ç¬¬ 3~13 ä¸ªç‰¹å¾å€¼å¹¶éå®Œå…¨ä¸ºé›¶ï¼Œè€Œä¸”è¶‹è¿‘äº0çš„å®æ•°ï¼Œè¿™ä¸ªç»“æœæ˜¯ç”± NumPy æµ®ç‚¹è¿ç®—å¯¼è‡´çš„)ã€‚è¯·æ³¨æ„ï¼Œåœ¨æå°‘çš„æƒ…å†µä¸‹å¯è¾¾åˆ°å®Œç¾çš„å…±çº¿æ€§(æ‰€æœ‰æ ·æœ¬çš„ç‚¹è½åœ¨ä¸€æ¡ç›´çº¿ä¸Š)ï¼Œè¿™æ—¶åæ–¹å·®çŸ©é˜µçš„ç§©ä¸º1ï¼Œå°†å¯¼è‡´çŸ©é˜µåªæœ‰ä¸€ä¸ªå«éé›¶ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡ã€‚

ä¸ºäº†åº¦é‡çº¿æ€§åˆ¤åˆ«(ç‰¹å¾å‘é‡)å¯ä»¥è·å–å¤šå°‘åŒºåˆ†ç±»åˆ«çš„ä¿¡æ¯ï¼Œä¸å‰é¢ PCA å°èŠ‚ä¸­å¯¹ç´¯ç§¯æ–¹å·®çš„ç»˜åˆ¶ç±»ä¼¼ï¼Œæˆ‘ä»¬æŒ‰ç…§ç‰¹å¾å€¼é™åºç»˜åˆ¶å‡ºç‰¹å¾å¯¹çº¿æ€§åˆ¤åˆ«ä¿¡æ¯ä¿æŒç¨‹åºçš„å›¾åƒã€‚ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åœ¨æ­¤ä½¿ç”¨äº†åˆ¤å®šç±»åˆ«åŒºåˆ†èƒ½åŠ›çš„ç›¸å…³ä¿¡æ¯(discriminability)ã€‚

```python
tot = sum(eigen_vals.real)
discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]
cum_discr = np.cumsum(discr)
plt.bar(range(1, 14), discr, alpha=0.4, align='center',
        label='individual "discriminability" ')
plt.step(range(1, 14), cum_discr, where='mid',
        label='cumulative "discriminability" ')
plt.xlabel('Linear Discriminants')
plt.ylabel('"discriminability" ratio')
plt.ylim([-0.1, 1.1])
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```

ä»ç»“æœå›¾åƒä¸­å¯ä»¥çœ‹åˆ°ï¼Œå‰ä¸¤ä¸ªçº¿æ€§åˆ¤åˆ«å‡ ä¹è·å–åˆ°äº†è‘¡è„é…’è®­ç»ƒæ•°æ®é›†ä¸­å…¨éƒ¨æœ‰ç”¨ä¿¡æ¯:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232813255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

ä¸‹é¢æˆ‘ä»¬å åŠ è¿™ä¸¤ä¸ªåˆ¤åˆ«èƒ½åŠ›æœ€å¼ºçš„ç‰¹å¾å‘é‡åˆ—æ¥æ„å»ºè½¬æ¢çŸ©é˜µ $W$:

```python
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,
            eigen_pairs[1][1][:, np.newaxis].real))
print('Matrix W:\n', w)

# è¾“å‡º:
Matrix W:
 [[-0.0707  0.3778]
 [ 0.0359  0.2223]
 [-0.0263  0.3813]
 [ 0.1875 -0.2955]
 [-0.0033 -0.0143]
 [ 0.2328 -0.0151]
 [-0.7719 -0.2149]
 [-0.0803 -0.0726]
 [ 0.0896 -0.1767]
 [ 0.1815  0.2909]
 [-0.0631 -0.2376]
 [-0.3794 -0.0867]
 [-0.3355  0.586 ]]
```


## 5.2.3 å°†æ ·æœ¬æ˜ å°„åˆ°æ–°çš„ç‰¹å¾ç©ºé—´

é€šè¿‡ä¸Šä¸€å°èŠ‚ä¸­æ„å»ºçš„çŸ©é˜µ $W$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¹˜ç§¯çš„æ–¹å¼å¯¹è®­ç»ƒé›†è¿›è¡Œè½¬æ¢:

$$ X' = XW $$

```python
X_train_lda = X_train_std.dot(w)
colors = ['r', 'b', 'g']
markers = ['s', 'x', 'o']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_lda[y_train == l, 0],
        X_train_lda[y_train == l, 1],
        c=c, label=l, marker=m)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend(loc='best')
plt.show()
```

é€šè¿‡ç»“æœå›¾åƒå¯è§ï¼Œä¸‰ä¸ªè‘¡è„é…’ç±»åœ¨æ–°çš„ç‰¹å¾å­ç©ºé—´ä¸Šæ˜¯çº¿æ€§å¯åˆ†çš„:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830232834955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


## 5.2.4 ä½¿ç”¨ scikit-learn è¿›è¡Œ LDA åˆ†æ

è‡ªå·±å†™ä»£ç é€æ­¥å®ç° LDAï¼Œå¯¹äºç†è§£ LDA å†…éƒ¨çš„å·¥ä½œåŸç†åŠå…¶ä¸ PCA çš„å·®åˆ«æ˜¯ä¸€ç§å¾ˆå¥½çš„ä½“éªŒã€‚ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹ scikit-learn ä¸­å¯¹ LDA ç±»çš„å®ç°:

```python
# from sklearn.lda import LDA # Ver. < 0.18
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
x_train_lda = lda.fit_transform(X_train_std, y_train)
```

æ¥ä¸‹æ¥ï¼Œåœ¨å°†è®­ç»ƒæ•°æ®é€šè¿‡ LDA è¿›è¡Œè½¬æ¢åï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹é€»è¾‘æ–¯ç‰¹å›å½’åœ¨ç›¸å¯¹ä½ç»´æ•°æ®ä¸Šçš„è¡¨ç°:

```python
lr = LogisticRegression()
lr = lr.fit(X_train_lda, y_train)
plot_decision_regions(X_train_lda, y_train, classifier=lr)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend(loc='best')
plt.show()
```

ä»ç»“æœå›¾åƒä¸­å¯ä»¥çœ‹åˆ°ï¼Œé€»è¾‘æ–¯ç‰¹å›å½’æ¨¡å‹åªé”™è¯¯åœ°åˆ¤æ–­äº†ç±»åˆ«2ä¸­çš„ä¸€ä¸ªæ ·æœ¬:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/201908302329488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


é€šè¿‡é™ä½æ­£åˆ™åŒ–å¼ºåº¦ï¼Œæˆ‘ä»¬æˆ–è®¸å¯ä»¥å¯¹å†³ç­–æ ‘è¾¹ç•Œè¿›è¡Œè°ƒæ•´ï¼Œä»¥ä½¿å¾—é€»è¾‘æ–¯ç‰¹å›å½’æ¨¡å‹å¯ä»¥æ­£ç¡®åœ°å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ†ç±»ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å†æ¥çœ‹ä¸€ä¸‹æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ•ˆæœ:

```python
X_test_lda = lda.transform(X_test_std)
plot_decision_regions(X_test_lda, y_test, classifier=lr)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend(loc='best')
plt.show()
```

æ­£å¦‚ä»ç»“æœå›¾åƒä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨åªæœ‰ä¸¤ç»´çš„ç‰¹å¾å­ç©ºé—´æ¥æ›¿ä»£åŸå§‹æ•°æ®é›†ä¸­çš„13ä¸ªè‘¡è„é…’ç‰¹å¾æ—¶ï¼Œé€»è¾‘æ–¯ç‰¹å›å½’åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹æ ·æœ¬çš„åˆ†ç±»ç»“æœå¯è°“å®Œç¾:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830233000629.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

# 5.3 ä½¿ç”¨æ ¸ä¸»æˆåˆ†åˆ†æè¿›è¡Œéçº¿æ€§æ˜ å°„

è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•éƒ½å˜‰å®šè¾“å…¥æ•°æ®æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚æ„ŸçŸ¥å™¨ä¸ºäº†ä¿è¯å…¶æ”¶æ•›æ€§ï¼Œç”šè‡³è¦æ±‚æ‚¬é“¾æ•°æ®æ˜¯å®Œç¾çº¿æ€§å¯åˆ†çš„ã€‚æˆ‘ä»¬ç›®å‰å­¦ä¹ è¿‡çš„ç®—æ³•ä¸­ï¼Œåƒ Adalineã€é€»è¾‘æ–¯ç‰¹å›å½’å’Œ(æ ‡å‡†)æ”¯æŒå‘é‡æœºç­‰ï¼Œéƒ½æ— æ³•å®ç°å®Œç¾çš„çº¿æ€§åˆ’åˆ†çš„åŸå› å½’å’äºå™ªå£°ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹æˆ‘ä»¬é¢å¯¹çš„æ˜¯éçº¿æ€§é—®é¢˜ï¼Œé’ˆå¯¹æ­¤ç±»é—®é¢˜ï¼Œé€šè¿‡é™ç»´æŠ€æœ¯ï¼Œå¦‚ PCA å’Œ LDA ç­‰ï¼Œå°†å…¶è½¬åŒ–ä¸ºçº¿æ€§é—®é¢˜å¹¶ä¸æ˜¯æœ€å¥½çš„æ–¹æ³•ã€‚åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ä¸€ä¸‹åˆ©ç”¨æ ¸æŠ€å·§çš„ PCAï¼Œæˆ–è€…ç§°ä¸ºæ ¸ PCAï¼Œè¿™ä¸åœ¨ç¬¬3ç« ä¸­æˆ‘ä»¬ä»‹ç»è¿‡çš„æ ¸æ”¯æŒå‘é‡æœºçš„æ¦‚å¿µæœ‰ä¸€å®šç›¸å…³æ€§ã€‚ä½¿ç”¨æ ¸ PCAï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å°†éçº¿æ€§å¯åˆ†çš„æ•°æ®è½¬æ¢åˆ°ä¸€ä¸ªé€‚åˆå¯¹å…¶è¿›è¡Œçº¿æ€§åˆ†ç±»çš„æ–°çš„ä½ç»´å­ç©ºé—´ä¸Šã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830233026120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


## 5.3.1 æ ¸å‡½æ•°ä¸æ ¸æŠ€å·§

ä¼šè®®ä¸€ä¸‹æˆ‘ä»¬å†ç¬¬ä¸‰ç« ä¸­æ›¾è®¨è®ºçš„åŸºäºæ ¸çš„æ”¯æŒå‘é‡æœºï¼Œé€šè¿‡å°†éçº¿æ€§å¯åˆ†é—®é¢˜æ˜ å°„åˆ°ç»´åº¦æ›´é«˜çš„ç‰¹å¾ç©ºé—´ï¼Œä½¿å…¶åœ¨æ–°çš„ç‰¹å¾ç©ºé—´ä¸Šçº¿æ€§å¯åˆ†ã€‚ä¸ºäº†å°†æ ·æœ¬ $x \in R^d$ è½¬æ¢åˆ°ç»´åº¦æ›´é«˜çš„ $k$ ç»´å­ç©ºé—´ï¼Œæˆ‘ä»¬å®šä¹‰å¦‚ä¸‹éçº¿æ€§æ˜ å°„å‡½æ•° $\varphi$:

$$
\phi: R^d \rightarrow R^k (k >> d)
$$
æˆ‘ä»¬å¯ä»¥å°† $\varphi$çœ‹ä½œæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½å¤Ÿå¯¹åŸå§‹ç‰¹å¾è¿›è¡Œéçº¿æ€§ç»„åˆï¼Œä»¥å°†åŸå§‹çš„ $d$ ç»´æ•°æ®é›†æ˜ å°„åˆ°æ›´é«˜ç»´çš„ $k$ ç»´ç‰¹å¾ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œå¯¹äºäºŒç»´(d=2) ç‰¹å¾å‘é‡$x \in R^d$($x$ æ˜¯åŒ…å« $d$ ä¸ªç‰¹å¾çš„åˆ—å‘é‡)æ¥è¯´ï¼Œå¯ç”¨å¦‚ä¸‹æ˜ å°„å°†å…¶è½¬æ¢åˆ°ä¸‰ç»´ç©ºé—´:

$$
x = 
\begin{bmatrix}
x_1, x_2
\end{bmatrix}^T \\
\downarrow \phi \\
z = 
\begin{bmatrix}
x_1^2, \sqrt {2x_1x_2}, x_2^2
\end{bmatrix}^T
$$

è¯å¥è¯è¯´ï¼Œåˆ©ç”¨æ ¸ PCAï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡éçº¿æ€§æ˜ å°„å°†æ•°æ®è½¬æ¢åˆ°ä¸€ä¸ªé«˜ç»´ç©ºé—´ï¼Œç„¶ååœ¨æ­¤é«˜ç»´ç©ºé—´ä¸­ä½¿ç”¨æ ‡å‡† PCA å°†å…¶æ˜ å°„åˆ°å¦å¤–ä¸€ä¸ªä½ç»´ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡çº¿æ€§åˆ†ç±»å™¨å¯¹æ ·æœ¬è¿›è¡Œåˆ’åˆ†(å‰ææ¡ä»¶æ˜¯ï¼Œæ ·æœ¬å¯ä»¥æ ¹æ®è¾“å…¥ç©ºé—´çš„å¯†åº¦è¿›è¡Œåˆ’åˆ†)ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯ä¼šå¸¦æ¥é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ï¼Œè¿™ä¹Ÿæ­£æ˜¯æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦ä½¿ç”¨æ ¸æŠ€å·§çš„åŸå› ã€‚é€šè¿‡ä½¿ç”¨æ ¸æŠ€å·§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŸå§‹ç‰¹å¾ç©ºé—´ä¸­è®¡ç®—ä¸¤ä¸ªé«˜ç»´ç‰¹å¾ç©ºé—´ä¸­å‘é‡çš„ç›¸ä¼¼åº¦ã€‚

åœ¨æ›´æ·±å…¥äº†è§£äº†ä½¿ç”¨æ ¸æŠ€å·§è§£å†³è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹æœ¬ç« æœ€åˆé”ä»‹ç»çš„æ ‡å‡† PCA æ–¹æ³•ã€‚ä¸¤ä¸ªç‰¹å¾ $k$ å’Œ $j$ ä¹‹é—´åæ–¹å·®çš„è®¡ç®—å…¬å¼å¦‚ä¸‹:

$$
\sigma_{jk} = \frac {1}{n} \sum_{i=1}^n (x_j^{(i)} - \mu_j)(x_k^{(i)} - \mu_k)
$$

ç”±äºåœ¨å¯¹ç‰¹å¾åšæ ‡å‡†åŒ–å¤„ç†åï¼Œå…¶å‡å€¼ä¸º0ã€‚ä¾‹å¦‚: æˆ‘ä»¬å¯å°†ä¸Šè¿°å…¬å¼ç®€åŒ–ä¸º:

$$
\sigma_{jk} = \frac {1}{n}
\sum_{i=1}^n
x_j^{(i)} x_k^{(i)}
$$

è¯·æ³¨æ„ï¼Œä¸Šè¿°å…¬å¼æ˜¯ä¸¤ä¸ªç‰¹å¾å€¼ä¹‹é—´çš„åæ–¹å·®è®¡ç®—å…¬å¼ï¼Œä¸‹é¢ç»™å‡ºè®¡ç®—åæ–¹å·®çŸ©é˜µ $\sum$ çš„é€šç”¨å…¬å¼:

$$
\Sigma = \frac {1}{n}
\sum_{i=1}^n
x^{(i)} x^{(i)T} 
$$

Bernhard Schoellkopf æå‡ºäº†ä¸€ç§æ–¹æ³•$^{æ³¨1}$ï¼Œå¯ä»¥ä½¿ç”¨ $\varphi$ é€šè¿‡åœ¨åŸå§‹ç‰¹å¾ç©ºé—´ä¸Šçš„éçº¿æ€§ç‰¹å¾ç»„åˆæ¥æ›¿ä»£æ ·æœ¬é—´ç‚¹ç§¯çš„è®¡ç®—:

> æ³¨1: B.Scholkopf, A. Smola, and K.-R. Muler. Kernel Principal Component Analysis. pages 583-588, 1997.

$$
\Sigma =  \frac {1}{n}
\sum_{i=1}^n
\phi (x^{(i)}) \phi (x^{(i)})^T  
$$

ä¸ºäº†æ±‚å¾—æ­¤åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼Œä¹Ÿå°±æ˜¯ä¸»æˆåˆ†ï¼Œæˆ‘ä»¬éœ€è¦æ±‚è§£ä¸‹è¿°å…¬å¼:

$$
\Sigma \nu = \lambda \nu \\
\Rightarrow \frac {1}{n} \sum_{i=1}^n \phi (x^{(i)}) \phi (x^{(i)})^T \nu = \lambda \nu \\
\Rightarrow \nu = \frac {1}{\lambda n} \sum_{i=1}^n \phi (x^{(i)}) \phi (x^{(i)})^T \nu = \lambda \nu = \frac {1}{n} \sum_{i=1}^n a^{(i)} \phi (x^{(i)})
$$

å…¶ä¸­ï¼Œ$\lambda$ å’Œ $\nu$ åˆ†åˆ«ä¸ºåæ–¹å·®çŸ©é˜µ $\Sigma$ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œè¿™é‡Œçš„ $a$ å¯ä»¥é€šè¿‡æå–æ ¸(ç›¸ä¼¼) çŸ©é˜µ $K$ çš„ç‰¹å¾å‘é‡æ¥å¾—åˆ°ï¼Œå…·ä½“å†…å®¹åœ¨å°†åç»­æ®µè½ä¸­è¿›è¡Œä»‹ç»ã€‚

é¦–å…ˆï¼Œä½¿ç”¨çŸ©é˜µç¬¦å·æ¥è¡¨ç¤ºåæ–¹å·®çŸ©é˜µï¼Œå…¶ä¸­ $\varphi (X)$ æ˜¯ä¸€ä¸ª $n \times k$ç»´çš„çŸ©é˜µ:

$$
\Sigma = 
\frac {1}{n} \sum_{i=1}^n \phi (x^{(i)}) \phi (x^{(i)})^T
= \frac {1}{n} \phi (X)^T \phi (X)
$$

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç‰¹å¾å‘é‡çš„å…¬å¼è®°ä¸º:

$$
\nu = \frac {1}{n} \sum_{i=1}^n a^{(i)} \phi (x^{(i)})
= \nu \phi(X)^T a
$$

ç”±äº $\Sigma \nu = \lambda \nu$ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°:

$$
\frac {1}{n} \phi(X)^T \phi(X) \phi(X)^T a
= \lambda \phi(X) a
$$

ä¸¤è¾¹åŒä¹˜ä»¥ $\varphi(X)$ï¼Œå¯å¾—:

$$
\frac {1}{n} \phi(X) \phi(X)^T \phi(X) \phi(X)^T a =
\lambda \phi(X) \phi(X)^T a \\
\Rightarrow \phi(X) \phi(X)^T a = \lambda a \\
\Rightarrow \frac {1}{n} K a = \lambda a
$$

å…¶ä¸­ï¼Œ$K$ ä¸ºç›¸ä¼¼(æ ¸) çŸ©é˜µ:

$$
K = \phi(X) \phi(X)^T
$$

å›é¡¾ä¸€ä¸‹ 3.4èŠ‚çš„å†…å®¹ï¼Œé€šè¿‡æ ¸æŠ€å·§ï¼Œä½¿ç”¨æ ¸å‡½æ•° $K$ ä»¥é¿å…ä½¿ç”¨ $\varphi$ æ¥ç²¾ç¡®è®¡ç®—æ ·æœ¬é›†åˆ $x$ ä¸­æ ·æœ¬å¯¹ä¹‹é—´çš„ç‚¹ç§¯ï¼Œè¿™æ ·æˆ‘ä»¬å°±æ— éœ€å¯¹ç‰¹å¾å‘é‡è¿›è¡Œç²¾ç¡®çš„è®¡ç®—:

$$
K(x^{(i)}, x^{(j)}) = \phi(x^{(i)})^T \phi(x^{(j)})
$$

æ¢å¥è¯è¯´ï¼Œé€šè¿‡æ ¸ PCAï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°å·²ç»æ˜ å°„åˆ°å„æˆåˆ†çš„æ ·æœ¬ï¼Œè€Œä¸åƒæ ‡å‡† PCA æ–¹æ³•é‚£æ ·å»æ„å»ºä¸€ä¸ªè½¬æ¢çŸ©é˜µã€‚ç®€å•åœ°è¯´ï¼Œå¯ä»¥å°†æ ¸å‡½æ•°(æˆ–è€…ç®€ç§°æ ¸) ç†è§£ä¸º: é€šè¿‡ä¸¤ä¸ªå‘é‡ç‚¹ç§¯æ¥åº¦é‡å‘é‡é—´ç›¸ä¼¼åº¦çš„å‡½æ•°ã€‚æœ€å¸¸ç”¨çš„æ ¸å‡½æ•°æœ‰:

**å¤šé¡¹å¼æ ¸**:

$$
K(x^{(i)}, x^{(j)}) = (x^{(i)T} x^{(j)} + \theta)^p
$$

å…¶ä¸­ï¼Œé˜ˆå€¼ $\theta$ å’Œå¹‚çš„å€¼ $p$ éœ€è‡ªè¡Œå®šä¹‰ã€‚

**åŒæ›²æ­£åˆ‡(sigmoid)æ ¸**:

$$
K(x^{(i)}, x^{(j)}) = thah(\eta x^{(i)T} x^{(j)} + \theta)
$$

å¾„å‘åŸºæ ¸å‡½æ•°(Radial Basis Function, RBF) æˆ–è€…æˆä¸ºé«˜æ–¯æ ¸å‡½æ•°ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€å°èŠ‚çš„ç¤ºä¾‹ä¸­ç”¨åˆ°:

$$
K(x^{(i)}, x^{(j)}) = exp 
\large( 
-\frac {||x^{(i)} - x^{(j)}||^2}{2 \sigma^2}
\large )
$$

ä¹Ÿå¯ä»¥å†™ä½œ:

$$
K(x^{(i)}, x^{(j)}) = exp (-\gamma {||x^{(i)} - x^{(j)}||^2})
$$

ç»¼åˆä¸Šè¿°è®¨è®ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹ä¸‰ä¸ªæ­¥éª¤æ¥å®ç°ä¸€ä¸ªåŸºäº RBF æ ¸çš„ PCA:

1) ä¸ºäº†è®¡ç®—æ ¸(ç›¸ä¼¼)çŸ©é˜µ $k$ï¼Œæˆ‘ä»¬éœ€è¦åšå¦‚ä¸‹è®¡ç®—:

$$
K(x^{(i)}, x^{(j)}) = exp (-\gamma {||x^{(i)} - x^{(j)}||^2})
$$

æˆ‘ä»¬éœ€è¦è®¡ç®—ä»»æ„ä¸¤æ ·æœ¬å¯¹ä¹‹é—´çš„å€¼:

$$
K = 
\begin{bmatrix}
K(x^{(1)}, x^{(1)}) & K(x^{(1)}, x^{(2)}) & \cdots & K(x^{(1)}, x^{(n)}) \\
K(x^{(2)}, x^{(1)}) & K(x^{(2)}, x^{(2)}) & \cdots & K(x^{(2)}, x^{(n)}) \\
\vdots & \vdots & \ddots & \vdots \\
K(x^{(n)}, x^{(1)}) & K(x^{(n)}, x^{(2)}) & \cdots & K(x^{(n)}, x^{(n)}) \\
\end{bmatrix}
$$

ä¾‹å¦‚: å¦‚æœæ•°æ®é›†åŒ…å« 100 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå°†å¾—åˆ°ä¸€ä¸ª $100 \times 100$ ç»´çš„å¯¹ç§°çŸ©é˜µã€‚

2) é€šè¿‡å¦‚ä¸‹å…¬å¼è¿›è¡Œè®¡ç®—ï¼Œä½¿æ ¸çŸ©é˜µ $K$ æ›´ä¸ºèšé›†:

$$
K' = K-1_nK - K1_n + 1_nK1_n
$$

å…¶ä¸­ï¼Œ$l_n$ æ˜¯ä¸€ä¸ª $n \times n$ ç»´çš„çŸ©é˜µ(ä¸æ ¸çŸ©é˜µç»´åº¦ç›¸åŒ)ï¼Œå…¶æ‰€æœ‰çš„å€¼å‡ä¸º $\frac {1}{n}$ã€‚

3) å°†èšé›†åçš„æ ¸çŸ©é˜µçš„ç‰¹å¾å€¼æŒ‰ç…§é™åºæ’åˆ—ï¼Œé€‰æ‹©å‰ $k$ ä¸ªç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ä¸æ ‡å‡† PCA ä¸åŒï¼Œè¿™é‡Œçš„ç‰¹å¾å‘é‡ä¸æ˜¯ä¸»æˆåˆ†è½´ï¼Œè€Œæ˜¯å°†æ ·æœ¬æ˜ å°„åˆ°è¿™äº›è½´ä¸Šã€‚

è‡³æ­¤ï¼Œè¯»è€…å¯èƒ½ä¼šæ„Ÿåˆ°å›°æƒ‘: ä¸ºä»€ä¹ˆè¦åœ¨ç¬¬2æ­¥å¯¹æ ¸çŸ©é˜µè¿›è¡Œèšé›†å¤„ç†? æˆ‘ä»¬æ›¾ç»å‡å®šï¼Œæ•°æ®éœ€è¦ç»è¿‡æ ‡å‡†åŒ–å¤„ç†ï¼Œå½“åœ¨ç”Ÿæˆåæ–¹å·®çŸ©é˜µå¹¶é€šè¿‡ $\varphi$ ä»¥éçº¿æ€§ç‰¹å¾çš„ç»„åˆæ›¿ä»£ç‚¹ç§¯æ—¶ï¼Œæ‰€æœ‰ç‰¹å¾çš„çŸ©é˜µä¸º0ã€‚ç”±æ­¤ï¼Œåœ¨ç¬¬2éƒ¨è½´èšé›†æ ¸çŸ©é˜µå°±æ˜¾å¾—å¾ˆæœ‰å¿…è¦ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶æ²¡æœ‰ç²¾ç¡®è®¡ç®—æ–°çš„ç‰¹å¾ç©ºé—´ï¼Œè€Œä¸”ä¹Ÿä¸èƒ½ç¡®å®šæ–°ç‰¹å¾ç©ºé—´çš„ä¸­å¿ƒåœ¨é›¶ç‚¹ã€‚

ä¸‹ä¸€å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åŸºäºè¿™ä¸‰ä¸ªæ­¥éª¤ä½¿ç”¨ Python å®ç° PCAã€‚

## 5.3.2 ä½¿ç”¨ Python å®ç°æ ¸ä¸»æˆåˆ†åˆ†æ

åœ¨åƒç±³ä½ çš„å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ ¸ PCA ç›¸å…³çš„æ ¸å¿ƒæ¦‚å¿µã€‚ç°åœ¨æ ¹æ®ä¹‹å‰æ€»ç»“è¿‡çš„å®ç°æ ¸ PCA æ–¹æ³•çš„ä¸‰ä¸ªæ­¥éª¤ï¼Œä½¿ç”¨ Python å®ç°åŸºäº RBF æ ¸çš„ PCAã€‚å€ŸåŠ©äº SciPy å’Œ NumPy çš„å‡½æ•°ï¼Œæˆ‘ä»¬å°†ä¼šçœ‹åˆ°ï¼Œå®ç° PCA å®é™…ä¸Šå¾ˆç®€å•ã€‚

```python
from scipy.spatial.distance import pdist, squareform
from scipy import exp
from scipy.linalg import eigen
import numpy as np


def rbf_kernel_pca(X, gamma, n_components):
    """
    RBF kernel PCA implementation.
    
    Parameters
    -----------
    X: {NumPy ndarray}, shape = [n_samples, n_features]
    
    gamma: float
        Turning parameter of the RBF kernel
    
    n_components: float
        Number of principal components to return
    
    Returns
    -----------
    X_pc: {NumPy ndarray}, shape = [n_samples, k_features]
        Projected dataset
    """
    # Calculate pairwise squared Euclidean distances
    # in the MxN dimensional dataset.
    sq_dists = pdist(X, 'sqeuclidean')
    
    # Convert pairwise distance into a square matrix
    mat_sq_dists = squreform(sq_dists)
    
    # Compute the symmetric kernel matrix
    K = exp(-gamma * mat_sq_dists)
    
    # Center the kernel matrix
    N = K.shape[0]
    one_n = np.ones((N, N)) / N
    K = K - one_n.dot(K)  - K.dot(one_n) + one_n.dot(K).dot(one_n)
    
    # Obtaining eigenpairs from the centered kernel matrix
    # numpy.eigh returns them in sorted order
    eigvals, eigvecs = eigh(K)
    
    # Collect the top k eigenvectors (projected samples)
    X_pc = np.column_stack((eigvecs[:, i] for i in range(1, n_components + 1)))
    return X_pc
```

é‡‡ç”¨ RBF æ ¸å‡½æ•°å®ç°çš„ PCA è¿›è¡Œé™ç»´æ—¶å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯æˆ‘ä»¬å¿…é¡»åˆ¶å®šå…ˆéªŒè¯å‚æ•° rï¼Œéœ€è¦é€šè¿‡å®éªŒæ¥æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„ r å€¼ï¼Œæœ€å¥½æ˜¯é€šè¿‡å‚æ•°è°ƒä¼˜æ¥ç¡®å®šï¼Œä¾‹å¦‚ç½‘æ ¼æœç´¢ç®—æ³•ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬6ç« ä¸­å¯¹å…¶è¿›è¡Œæ·±å…¥æ¢è®¨ã€‚

### ç¤ºä¾‹1: åˆ†ç¦»åŠæœˆå½¢æ•°æ®

ç°åœ¨ï¼Œæˆ‘ä»¬å°†å®ç° rbf_kernel_pca æ–¹æ³•åº”ç”¨äºéçº¿æ€§ç¤ºä¾‹æ•°æ®é›†ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªåŒ…å« 100 ä¸ªæ ·æœ¬ç‚¹çš„äºŒç»´æ•°æ®é›†ï¼Œä»¥ä¸¤ä¸ªåŠæœˆå½¢çŠ¶è¡¨ç¤º:

```python
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, random_state=123)
plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5)
plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5)
plt.show()
```

å¤„äºæ¼”ç¤ºçš„éœ€è¦ï¼Œä½¿ç”¨ä¸‰è§’ç¬¦å·æ ‡è¯†çš„æ ‡è¯†ä¸€ä¸ªç±»åˆ«ä¸­çš„æ ·æœ¬ï¼Œä½¿ç”¨åœ†å½¢ç¬¦å·æ ‡è¯†çš„è¡¨ç¤ºå¦ä¸€ç±»åˆ«çš„æ ·æœ¬:

![](https://imgconvert.csdnimg.cn/aHR0cDovL2dpdGh1Yi5jb20vbWluZ21pbnl1L2ltYWdlcy9yYXcvbWFzdGVyL3B5dGhvbi1tYWNoaW5lLWxlYXJuaW5nL2NoMDUvNS0xMi5qcGc)

æ˜¾ç„¶ï¼Œè¿™ä¸¤ä¸ªåŠæœˆå½¢ä¸æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œè€Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡æ ¸ PCA å°†è¿™ä¸¤ä¸ªåŠæœˆå½¢æ•°æ®å±•å¼€ï¼Œä½¿å¾—æ•°æ®æˆä¸ºé€‚ç”¨äºæŸä¸€çº¿æ€§åˆ†ç±»å™¨çš„è¾“å…¥æ•°æ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æ ‡å‡†çš„ PCA å°†æ•°æ®æ˜ å°„åˆ°ä¸»æˆåˆ†ä¸Šï¼Œå¹¶è§‚å¯Ÿå…¶å½¢çŠ¶:

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scikit_pca = PCA(n_components=2)
X_spca = scikit_pca.fit_transform(X)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))
ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1],
             color='red', marker='^', alpha=0.5)
ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],
              color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1)) + 0.02,
              color='red', marker='^', alpha=0.5)
ax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1)) - 0.02,
              color='blue', marker='o', alpha=0.5)

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel('PC1')
plt.tight_layout()
plt.show()           
```

å¾ˆæ˜æ˜¾ï¼Œç»è¿‡æ ‡å‡† PCA çš„è½¬æ¢åï¼Œçº¿æ€§åˆ†ç±»å™¨æœªå¿…èƒ½å¾ˆå¥½åœ°å‘æŒ¥ä½œç”¨:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830233126814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬ä»…å›æ‰§ç¬¬ä¸€ä¸»æˆåˆ†çš„å›¾åƒæ—¶(è§å³å­å›¾)ï¼Œæˆ‘ä»¬åˆ†åˆ«å°†ä¸‰è§’å½¢å’Œåœ†å½¢ä»£è¡¨çš„æ ·æœ¬å‘ä¸Šæˆ–å‘ä¸‹åšäº†è½»å¾®è°ƒæ•´ï¼Œä»¥æ›´å¥½åœ°å±•ç¤ºç±»é—´é‡å ã€‚

> ğŸ”– PCA æ˜¯æ— ç›‘ç£æ–¹æ³•ï¼Œä¸ LDA ç›¸æ¯”ï¼Œå®ƒåœ¨ä½¿å¾—æ–¹å·®æœ€å¤§åŒ–çš„è¿‡ç¨‹ä¸­æœªä½¿ç”¨ç±»æ ‡ä¿¡æ¯ã€‚å‡ºäºå¢å¼ºå¯è§†åŒ–æ•ˆæœçš„è€ƒè™‘ï¼Œä¸ºäº†æ˜¾ç¤ºåˆ†ç±»çš„ç¨‹åº¦ï¼Œæˆ‘ä»¬æ‰åœ¨æ­¤ä½¿ç”¨äº†ä¸‰è§’å½¢å’Œåœ†å½¢ç¬¦å·ã€‚ã€

ç°åœ¨æˆ‘ä»¬å°†ä½¿ç”¨å‰ä¸€å°èŠ‚ä¸­å®ç°çš„å’Œæ ¸ PCA å‡½æ•° rbf_kernel_pca:

```python
from matplotlib.ticker import FormatStrFormatter

X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)

fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))
ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], 
            color='red', marker='^', alpha=0.5)
ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],
            color='blue', marker='o', alpha=0.5)

ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, 
            color='red', marker='^', alpha=0.5)
ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,
            color='blue', marker='o', alpha=0.5)

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel('PC1')
ax[0].xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))
ax[1].xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))

plt.tight_layout()
plt.show()
```

å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ä¸ªç±»åˆ«(åœ†å½¢å’Œä¸‰è§’å½¢) æ­¤æ—¶æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œè¿™ä½¿å¾—è½¬æ¢åçš„æ•°æ®é€‚åˆä½œä¸ºçº¿æ€§åˆ†ç±»å™¨çš„è®­ç»ƒæ•°æ®é›†:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830233222891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

ä¸è¿‡ï¼Œå¯¹äºå¯è°ƒæ•´å‚æ•° $\gamma$ï¼Œæ²¡æœ‰ä¸€ä¸ªé€šç”¨çš„å€¼ä½¿å…¶é€‚ç”¨äºä¸åŒçš„æ•°æ®é›†ã€‚é’ˆå¯¹ç»™å®šé—®é¢˜æ‰¾åˆ°ä¸€ä¸ªé€‚å®œçš„å‚æ•°å€¼éœ€è¦é€šè¿‡å®éªŒæ¥è§£å†³ã€‚åœ¨ç¬¬6ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¯è‡ªåŠ¨è¿›è¡Œå‚æ•°ä¼˜åŒ–ç­‰ä»»åŠ¡çš„æŠ€æœ¯ã€‚è¿™é‡Œï¼Œæˆ‘ä½¿ç”¨äº†ä¸€ä¸ªå·²æœ‰çš„èƒ½å¤Ÿç”Ÿæˆè‰¯å¥½ç»“æœçš„å€¼ã€‚

### ç¤ºä¾‹2. åˆ†ç¦»åŒå¿ƒåœ†

åœ¨ä¸Šä¸€å°èŠ‚ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡æ ¸ PCA åˆ†ç¦»åŠæœˆå½¢æ•°æ®ã€‚æ—¢ç„¶æˆ‘ä»¬å·²ç»æŠ•å…¥äº†å¦‚æ­¤å¤šçš„ç»å†å»ç†è§£æ ¸ PCA çš„æ¦‚å¿µï¼Œå°±å†çœ‹ä¸€ä¸‹å¦å¤–ä¸€ä¸ªå…³äºéçº¿æ€§é—®é¢˜çš„æœ‰è¶£ä¾‹å­â€”â€”åŒå¿ƒåœ†ã€‚

ä»£ç å¦‚ä¸‹:

```python
from sklearn.datasets import make_circles
X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)

plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)
plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)
plt.tight_layout()
plt.show()
```

åŒæ ·ï¼Œæˆ‘ä»¬å‡è®¾äº†ä¸€ä¸ªæ¶‰åŠä¸¤ä¸ªç±»åˆ«çš„é—®é¢˜ï¼Œä¸‰è§’å½¢å’Œåœ†å½¢åˆ†åˆ«æ ‡è¯†ä¸åŒç±»åˆ«çš„æ ·æœ¬:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830233239192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


é¦–å…ˆä½¿ç”¨æ ‡å‡† PCA æ–¹æ³•ï¼Œä»¥ä¾¿å°†ç»“æœä¸åŸºäº RBF æ ¸çš„ PCA ç”Ÿæˆçš„ç»“æœè¿›è¡Œæ¯”è¾ƒ:

```python
scikit_pca = PCA(n_components=2)
X_spca = scikit_pca.fit_transform(X)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],
              color='red', marker='^', alpha=0.5)
ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],
              color='blue', marker='o', alpha=0.5)

ax[1].scatter(X_spca[y == 0, 0], np.zeros((500, 1)) + 0.02,
              color='red', marker='^', alpha=0.5)
ax[1].scatter(X_spca[y == 1, 0], np.zeros((500, 1)) - 0.02,
              color='blue', marker='o', alpha=0.5)

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel('PC1')

plt.tight_layout()
plt.show()
```

å†ä¸€æ¬¡å‘ç°ï¼Œé€šè¿‡æ ‡å‡† PCA æ— æ³•å¾—åˆ°é€‚åˆäºçº¿æ€§åˆ†ç±»å™¨çš„è®­ç»ƒæ•°æ®:

![](https://imgconvert.csdnimg.cn/aHR0cDovL2dpdGh1Yi5jb20vbWluZ21pbnl1L2ltYWdlcy9yYXcvbWFzdGVyL3B5dGhvbi1tYWNoaW5lLWxlYXJuaW5nL2NoMDUvNS0yMS5qcGc)


ç»™å®šä¸€ä¸ªåˆé€‚çš„ $\gamma$ å€¼ï¼Œæ¥çœ‹çœ‹åŸºäº RBF çš„æ ¸ PCA å®ç°èƒ½å¤Ÿå¾—åˆ°ä»¤äººæ»¡æ„çš„ç»“æœ:

```python
X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))
ax[0].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1],
              color='red', marker='^', alpha=0.5)
ax[0].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1],
              color='blue', marker='o', alpha=0.5)

ax[1].scatter(X_kpca[y == 0, 0], np.zeros((500, 1)) + 0.02,
              color='red', marker='^', alpha=0.5)
ax[1].scatter(X_kpca[y == 1, 0], np.zeros((500, 1)) - 0.02,
              color='blue', marker='o', alpha=0.5)

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel('PC1')

plt.tight_layout()
plt.show()
```

åŸºäº RBF çš„æ ¸ PCA å†ä¸€æ¬¡å°†æ•°æ®æ˜ å°„åˆ°äº†ä¸€ä¸ªæ–°çš„å­ç©ºé—´ä¸­ï¼Œä½¿ä¸¤ä¸ªç±»åˆ«å˜å¾—çº¿æ€§å¯åˆ†:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830234012878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

## 5.3.3 æ˜ å°„æ–°çš„æ•°æ®ç‚¹

åœ¨ä¸Šè¿°ä¸¤ä¸ªæ ¸ PCA åº”ç”¨çš„ä¾‹å­(åŠæœˆå½¢å’ŒåŒå¿ƒåœ†)ä¸­ï¼Œæˆ‘ä»¬éƒ½å°†å•ä¸€æ•°æ®é›†æ˜ å°„åˆ°ä¸€ä¸ªæ–°çš„ç‰¹å¾ä¸Šã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è½¬æ¢å¤šä¸ªæ•°æ®é›†ï¼Œä¾‹å¦‚ï¼Œè®­ç»ƒæ•°æ®ã€æµ‹è¯•æ•°æ®ï¼Œä»¥åŠåœ¨å®Œæˆæ¨¡å‹æ„å»ºå’Œè¯„ä¼°åæ‰€è¦æ”¶é›†çš„æ–°æ ·æœ¬ã€‚æœ¬èŠ‚å°†ä»‹ç»å¦‚ä½•æ˜ å°„è®­ç»ƒæ•°æ®é›†ä»¥å¤–çš„æ•°æ®ç‚¹ã€‚

åœ¨æœ¬ç« å¼€å§‹æ—¶ä»‹ç»è¿‡çš„æ ‡å‡† PCA æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è½¬æ¢çŸ©é˜µå’Œè¾“å…¥æ ·æœ¬ä¹‹é—´çš„ç‚¹ç§¯æ¥å¯¹æ•°æ®è¿›è¡Œæ˜ å°„ï¼›æ˜ å°„çŸ©é˜µçš„åˆ—æ˜¯åæ–¹å·®çŸ©é˜µä¸­ $k$ ä¸ªæœ€å¤§ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡(v)ã€‚ç°åœ¨çš„é—®é¢˜æ˜¯: å¦‚æ­¤å°†æ­¤æ¦‚å¿µåº”ç”¨äºæ ¸ PCAï¼Ÿå›å¿†ä¸€ä¸‹æ ¸ PCA çš„åŸç†å¯ä»¥è®°å¾—ï¼Œæˆ‘ä»¬ä»èšé›†æ ¸çŸ©é˜µ(ä¸æ˜¯åæ–¹å·®çŸ©é˜µ) ä¸­å¾—åˆ°äº†ç‰¹å¾å‘é‡(a)ã€‚è¿™ä¸€ä½è¿™æ ·æœ¬å·²ç»æ˜ å°„åˆ°äº†ä¸»æˆåˆ†è½´vã€‚ç”±æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›å°†æ–°çš„æ ·æœ¬($x'$) æ˜ å°„åˆ°æ­¤ä¸»æˆåˆ†è½´ï¼Œéœ€è¦è¿›è¡Œå¦‚ä¸‹è®¡ç®—:

$$\phi(x')^T \nu$$

å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ¸æŠ€å·§ï¼Œè¿™æ ·å°±æ— éœ€ç²¾ç¡®è®¡ç®—æ˜ å°„ $\varphi(x')^T \nu$ã€‚ç„¶è€Œå€¼å¾—æ³¨æ„çš„æ˜¯: ä¸æ ‡å‡† PCA ç›¸æ¯”ï¼Œæ ¸ PCA æ˜¯ä¸€ç§åŸºäºå†…å­˜çš„æ–¹æ³•ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡æ˜ å°„æ–°çš„æ ·æœ¬å‰ï¼Œå¿…é¡»å†æ¬¡ä½¿ç”¨è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬éœ€è¦è®¡ç®—è®­ç»ƒæ•°æ®é›†ä¸­æ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬å’Œæ–°æ ·æœ¬ $x'$ ä¹‹é—´çš„ RBF æ ¸(ç›¸ä¼¼åº¦):

$$
\begin{align}
\phi(x')^T \nu &= \sum_i a^{(i)} \phi(x')^T \phi(x^{(i)}) \\
& = \sum_i a^{(i)} k(x', x^{(i)})^T
\end{align}
$$

å…¶ä¸­ï¼Œæ ¸çŸ©é˜µ K çš„ç‰¹å¾å‘é‡ a åŠç‰¹å¾å€¼ $\lambda$ éœ€æ»¡è¶³å¦‚ä¸‹ç­‰å¼:

$$
Ka = \lambda a
$$
åœ¨å®Œæˆæ–°æ ·æœ¬ä¸è®­ç»ƒæ•°æ®é›†å†…æ ·æœ¬é—´ç›¸ä¼¼åº¦çš„è®¡ç®—åï¼Œæˆ‘ä»¬è¿˜éœ€é€šè¿‡ç‰¹å¾å‘é‡å¯¹åº”çš„ç‰¹å¾å€¼æ¥å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚å¯ä»¥é€šè¿‡ä¿®æ”¹æ—©å‰å®ç°è¿‡çš„ rbf_kernel_pca å‡½æ•°æ¥è®©å…¶è¿”å›çŸ©é˜µçš„ç‰¹å¾å€¼:

```python
from scipy.spatial.distance import pdist, squareform
from scipy import exp
from scipy.linalg import eigh
import numpy as np

def rbf_kernel_pca(X, gamma, n_components):
    """
    RBF kernel PCA implementation.

    Parameters
    ------------
    X: {NumPy ndarray}, shape = [n_samples, n_features]
        
    gamma: float
      Tuning parameter of the RBF kernel
        
    n_components: int
      Number of principal components to return

    Returns
    ------------
     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]
       Projected dataset   
     
     lambdas: list
       Eigenvalues

    """
    # Calculate pairwise squared Euclidean distances
    # in the MxN dimensional dataset.
    sq_dists = pdist(X, 'sqeuclidean')

    # Convert pairwise distances into a square matrix.
    mat_sq_dists = squareform(sq_dists)

    # Compute the symmetric kernel matrix.
    K = exp(-gamma * mat_sq_dists)

    # Center the kernel matrix.
    N = K.shape[0]
    one_n = np.ones((N, N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)

    # Obtaining eigenpairs from the centered kernel matrix
    # numpy.eigh returns them in sorted order
    eigvals, eigvecs = eigh(K)

    # Collect the top k eigenvectors (projected samples)
    alphas = np.column_stack((eigvecs[:, -i]
                              for i in range(1, n_components + 1)))

    # Collect the corresponding eigenvalues
    lambdas = [eigvals[-i] for i in range(1, n_components + 1)]

    return alphas, lambdas
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„åŠæœˆå½¢æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ›´æ–°è¿‡çš„ RBF æ ¸PCA å®ç°æ¥å°†å…¶æ˜ å°„åˆ°ä¸€ä¸ªä¸€ç»´çš„ç©ºé—´ä¸Š:

```python
X, y = make_moons(n_samples=100, random_state=123)
alphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1)
```

ä¸ºäº†ç¡®ä¿æˆ‘ä»¬å·²ç»å®Œæˆäº†å®ç°æ–°æ ·æœ¬æ˜ å°„çš„ä»£ç ï¼Œå˜‰å®šåŠæœˆå½¢æ•°æ®é›†ä¸­çš„ç¬¬26ä¸ªç‚¹äº‹ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹ $x'$ï¼Œç°åœ¨è¦å°†å…¶æ˜ å°„åˆ°æ–°çš„å­ç©ºé—´ä¸­:

```python
>>> x_new = X[-1]
>>> x_new
array([ 0.4816, -0.3551])

>>> x_proj = alphas[-1] # original projection
>>> x_proj
array([ 0.1192])
```

é€šè¿‡æ‰§è¡Œä¸‹é¢çš„ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥é‡ç°åŸå§‹æ˜ å°„ã€‚ä½¿ç”¨ project_x å‡½æ•°ï¼Œè¿˜å¯ä»¥æ˜ å°„æ–°çš„æ•°æ®æ ·æœ¬ã€‚ä»£ç å¦‚ä¸‹:

```python
def project_x(x_new, X, gamma, alphas, lambdas):
    pair_dist = np.array([np.sum((x_new - row)**2) for row in X])
    k = np.exp(-gamma * pair_dist)
    return k.dot(alphas / lambdas)

# projection of the "new" datapoint
x_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas)
x_reproj 

# è¾“å‡º:
array([ 0.1192])
```

æœ€åï¼Œå°†ç¬¬ä¸€ä¸»æˆåˆ†ä¸Šçš„æ˜ å°„è¿›è¡Œå¯è§†åŒ–:

```python
plt.scatter(alphas[y == 0, 0], np.zeros((50)),
            color='red', marker='^', alpha=0.5)
plt.scatter(alphas[y == 1, 0], np.zeros((50)),
            color='blue', marker='o', alpha=0.5)
plt.scatter(x_proj, 0, color='black',
            label='original projection of point X[25]', marker='^', s=100)
plt.scatter(x_reproj, 0, color='green',
            label='remapped point X[25]', marker='x', s=500)
plt.legend(scatterpoints=1)

plt.tight_layout()
plt.show()
```

ä»ä¸‹å›¾å¯è§ï¼Œæˆ‘ä»¬å°†æ ·æœ¬ $x'$ æ­£ç¡®æ˜ å°„åˆ°äº†ç¬¬ä¸€ä¸»æˆåˆ†ä¸Š:

![](https://imgconvert.csdnimg.cn/aHR0cDovL2dpdGh1Yi5jb20vbWluZ21pbnl1L2ltYWdlcy9yYXcvbWFzdGVyL3B5dGhvbi1tYWNoaW5lLWxlYXJuaW5nL2NoMDUvNS0xNy5qcGc)


```python
X, y = make_moons(n_samples=100, random_state=123)
alphas, lambdas = rbf_kernel_pca(X[:-1, :], gamma=15, n_components=1)

def project_x(x_new, X, gamma, alphas, lambdas):
    pair_dist = np.array([np.sum((x_new - row)**2) for row in X])
    k = np.exp(-gamma * pair_dist)
    return k.dot(alphas / lambdas)

# projection of the "new" datapoint
x_new = X[-1]

x_reproj = project_x(x_new, X[:-1], gamma=15, alphas=alphas, lambdas=lambdas)

plt.scatter(alphas[y[:-1] == 0, 0], np.zeros((50)),
            color='red', marker='^', alpha=0.5)
plt.scatter(alphas[y[:-1] == 1, 0], np.zeros((49)),
            color='blue', marker='o', alpha=0.5)
plt.scatter(x_reproj, 0, color='green',
            label='new point [ 100.0,  100.0]', marker='x', s=500)
plt.legend(scatterpoints=1)
```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830234252410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


```python
plt.scatter(alphas[y[:-1] == 0, 0], np.zeros((50)),
            color='red', marker='^', alpha=0.5)
plt.scatter(alphas[y[:-1] == 1, 0], np.zeros((49)),
            color='blue', marker='o', alpha=0.5)
plt.scatter(x_proj, 0, color='black',
            label='some point [1.8713,  0.0093]', marker='^', s=100)
plt.scatter(x_reproj, 0, color='green',
            label='new point [ 100.0,  100.0]', marker='x', s=500)
plt.legend(scatterpoints=1)

plt.tight_layout()
plt.show()
```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830234241974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

## 5.3.4 scikit-learn ä¸­çš„æ ¸ä¸»æˆåˆ†åˆ†æ

scikit-learn çš„ sklearn.decomposition å­æ¨¡å—ä¸­å·²ç»å®ç°äº†ä¸€ç§æ ¸ PCA ç±»ã€‚å…¶ä½¿ç”¨æ–¹æ³•ä¸æ ‡å‡† PCA ç±»ç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ kernel å‚æ•°æ¥é€‰æ‹©ä¸åŒçš„æ ¸å‡½æ•°:

```python
from sklearn.decomposition import KernelPCA

X, y = make_moons(n_samples=100, random_state=123)
scikit_kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)
X_skernpca = scikit_kpca.fit_transform(X)
```

ä¸ºäº†éªŒè¯å¾—åˆ°çš„ç»“æœä¸æˆ‘ä»¬è‡ªå·±å®ç°çš„æ ¸ PCA æ˜¯å¦ä¸€è‡´ï¼Œæˆ‘ä»¬æ¥å›æ‰§åŠæœˆå½¢æ•°æ®æ˜ å°„åˆ°å‰ä¸¤ä¸ªä¸»æˆåˆ†çš„å›¾åƒ:

```python
plt.scatter(X_skernpca[y == 0, 0], X_skernpca[y == 0, 1],
            color='red', marker='^', alpha=0.5)
plt.scatter(X_skernpca[y == 1, 0], X_skernpca[y == 1, 1],
            color='blue', marker='o', alpha=0.5)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.tight_layout()
plt.show()
```

ä»ç»“æœå›¾åƒä¸­å¯è§ï¼Œé€šè¿‡ scikit-learn ä¸­ KernelPCA å¾—åˆ°çš„ç»“æœä¸æˆ‘ä»¬è‡ªå·±å®ç°çš„ç»“æœä¸€è‡´:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830234219894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


> ğŸ”– scikit-learn å®ç°äº†ä¸€äº›é«˜çº§çš„éçº¿æ€§é™ç»´æŠ€æœ¯ï¼Œè¿™äº›å†…å®¹å·²ç»è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ã€‚è¯»è€…å¯ä»¥é€šè¿‡é“¾æ¥ http://scikit-learn.org/stable/modules.mainfold.html æ¥äº†è§£ç›¸å…³å†…å®¹æ¦‚è¿°åŠå…¶ç¤ºä¾‹ã€‚

# 5.4 æœ¬ç« å°ç»“

åœ¨æœ¬ç« ä¸­ï¼Œè¯»è€…å­¦ä¹ äº†ä¸‰ç§ä¸åŒçš„åŸºäºç‰¹å¾æå–çš„åŸºæœ¬é™ç»´æŠ€æœ¯: æ ‡å‡† PCAã€LDAï¼Œä»¥åŠæ ¸ PCAã€‚ä½¿ç”¨ PCAï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¿½ç•¥ç±»æ ‡çš„æƒ…å†µä¸‹ï¼Œå°†æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´çš„å­ç©ºé—´ä¸Šï¼Œå¹¶æ²¿æ­£äº¤çš„ç‰¹å¾åæ ‡æ–¹å‘ä½¿æ–¹å·®æœ€å¤§åŒ–ã€‚ä¸ PCA ä¸åŒï¼ŒLDA æ˜¯ä¸€ç§ç›‘ç£é™ç»´æŠ€æœ¯ï¼Œè¿™æ„å‘³ç€: åœ¨çº¿æ€§ç‰¹å¾ä¸­å°è¯•ç±»åˆ«æœ€å¤§å¯åˆ†æ—¶ï¼Œéœ€è¦ä½¿ç”¨è®­ç»ƒæ•°æ®é›†ä¸­çš„ç±»åˆ«ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å­¦ä¹ äº†æ ¸ PCAï¼Œé€šè¿‡æ ¸ PCA å¯ä»¥å°†éçº¿æ€§æ•°æ®é›†æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´çš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œä½¿å¾—æ•°æ®çº¿æ€§å¯åˆ†ã€‚

åœ¨æŒæ¡äº†è¿™äº›æ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æŠ€æœ¯ä¹‹åï¼Œè¯»è€…å¯ä»¥å‡†å¤‡è¿›å…¥ä¸‹ä¸€ç« ï¼Œå­¦ä¹ å¦‚ä½•æœ‰æ•ˆåœ°ç»„åˆä¸åŒçš„é¢„å¤„ç†æŠ€æœ¯ï¼Œä»¥åŠæ¨¡å‹æ€§èƒ½è¯„ä¼°ç­‰æ–¹é¢çš„å†…å®¹ã€‚
