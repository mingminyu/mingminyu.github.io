# ç¬¬7ç«  é›†æˆå­¦ä¹ â€”â€”ç»„åˆä¸åŒçš„æ¨¡å‹

> GitHub Notebook åœ°å€: https://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch07/ch07.ipynb

åœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸åŒåˆ†ç±»æ¨¡å‹çš„è°ƒä¼˜å’Œè¯„ä¼°ç­‰ä»»åŠ¡çš„å®æˆ˜æ“ä½œã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨è¿™äº›æŠ€æœ¯çš„åŸºç¡€ä¸Šæ¢ç´¢ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ„å»ºä¸€ç»„åˆ†ç±»å™¨çš„é›†åˆï¼Œä½¿å¾—æ•´ä½“åˆ†ç±»æ•ˆæœä¼˜äºå…¶ä¸­äººä¸€ä¸ªå•ç‹¬çš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬å°†å­¦åˆ°:

- åŸºäºå¤šæ•°æŠ•ç¥¨çš„é¢„æµ‹
- é€šè¿‡å¯¹è®­ç»ƒæ•°æ®é›†çš„é‡å¤æŠ½æ ·å’Œéšæœºç»„åˆé™ä½æ¨¡å‹çš„è¿‡æ‹Ÿåˆ
- é€šè¿‡å¼±å­¦ä¹ æœºåœ¨è¯¯åˆ†ç±»æ•°æ®ä¸Šçš„å­¦ä¹ æ„å»ºæ€§èƒ½æ›´å¥½çš„æ¨¡å‹

# 7.1 é›†æˆå­¦ä¹ 

é›†æˆæ–¹æ³•(ensemble method)çš„ç›®æ ‡æ˜¯: å°†ä¸åŒçš„åˆ†ç±»å™¨ç»„åˆç§°ä¸ºä¸€ä¸ªå…ƒåˆ†ç±»å™¨ï¼Œä¸åŒ…å«äºå…¶ä¸­çš„å•ä¸ªåˆ†ç±»å™¨ç›¸æ¯”ï¼Œå…ƒåˆ†ç±»å™¨å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚ä¾‹å¦‚: å‡è®¾æˆ‘ä»¬æ”¶é›†äº† 10ä½ä¸“å®¶çš„é¢„æµ‹ç»“æœï¼Œé›†æˆæ–¹æ³•å…è®¸æˆ‘ä»¬ä¸€å®šç­–ç•¥å°†è¿™ 10ä½ä¸“å®¶çš„é¢„æµ‹è¿›è¡Œç»„åˆï¼Œä¸æ¯ä½ä¸“å®¶å•ç‹¬çš„é¢„æµ‹ç›¸æ¯”ï¼Œå®ƒå…·æœ‰æ›´å¥½çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æœ¬ç« æˆ‘ä»¬å°†ä»‹ç»å‡ ç§ä¸åŒçš„åˆ†ç±»å™¨é›†æˆæ–¹æ³•ã€‚åœ¨æœ¬èŠ‚ï¼Œæˆ‘ä»¬å…ˆä»‹ç»é›†æˆæ–¹æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠå®ƒä»¬ä¸ºä½•èƒ½å…·å¤‡è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

æœ¬ç« èšç„¦å‡ ç§æœ€æµè¡Œçš„é›†æˆæ–¹æ³•ï¼Œå®ƒä»¬éƒ½ä½¿ç”¨äº†å¤šæ•°æŠ•ç¥¨(majority voting) åŸåˆ™ã€‚å¤šæ•°æŠ•ç¥¨åˆ™æ˜¯æŒ‡å°†å¤§å¤šæ•°åˆ†ç±»å™¨é¢„æµ‹çš„ç»“æœä½œä¸ºæœ€ç»ˆç±»æ ‡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå°†å¾—ç¥¨ç‡è¶…è¿‡ 50% çš„ç»“æœä½œä¸ºç±»æ ‡ã€‚ä¸¥æ ¼æ¥è¯´ï¼Œå¤šæ•°æŠ•ç¥¨ä»…ç”¨äºäºŒç±»åˆ«åˆ†ç±»æƒ…å½¢ã€‚ä¸è¿‡ï¼Œå¾ˆå®¹æ˜“å°†å¤šæ•°æŠ•ç¥¨åŸåˆ™æ¨å¹¿åˆ°å¤šç±»åˆ«åˆ†ç±»ï¼Œä¹Ÿç§°ä½œç®€å•å¤šæ•°ç¥¨æ³•(plurality voting)ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©å¾—ç¥¨çš„ç±»æ ‡ã€‚ä¸‹å›¾è§£é‡Šäº†é›†æˆ 10ä¸ªåˆ†ç±»å™¨æ—¶ï¼Œå¤šæ•°åŠç®€å•å¤šæ•°ç¥¨æ³•è¡¨å†³çš„æ¦‚å¿µï¼Œå…¶ä¸­æ¯ä¸ªå•ç‹¬çš„ç¬¦å·(ä¸‰è§’å½¢ã€æ­£æ–¹å½¢å’Œåœ†)åˆ†åˆ«ä»£è¡¨ä¸€ä¸ªç±»æ ‡:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235543122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

åŸºäºè®­ç»ƒé›†ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒ m ä¸ªä¸åŒçš„æˆå‘˜åˆ†ç±»å™¨($C_1,\dots,C_m$)ã€‚åœ¨å¤šæ•°æŠ•ç¥¨åŸåˆ™ä¸‹ï¼Œå¯é›†æˆä¸åŒçš„åˆ†ç±»ç®—æ³•ï¼Œå¦‚å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€Logisticå›å½’ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æˆå‘˜åˆ†ç±»ç®—æ³•æ‹Ÿåˆä¸åŒçš„è®­ç»ƒå­é›†ã€‚è¿™ç§æ–¹æ³•å…¸å‹çš„ä¾‹å­å°±æ˜¯éšæœºæ£®æ—ç®—æ³•ï¼Œå®ƒç»„åˆäº†ä¸åŒçš„å†³ç­–æ ‘åˆ†ç±»å™¨ã€‚ä¸‹å›¾è§£é‡Šäº†ä½¿ç”¨å¤šæ•°æŠ•ç¥¨åŸåˆ™çš„é€šç”¨é›†æˆæ–¹æ³•çš„æ¦‚å¿µ:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235614165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

æƒ³è¦é€šè¿‡ç®€å•çš„å¤šæ•°æŠ•ç¥¨åŸåˆ™å¯¹ç±»æ ‡è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬è¦æ±‡æ€»åˆ†ç±»å™¨ $C_j$ çš„é¢„æµ‹ç±»æ ‡ï¼Œå¹¶é€‰å‡ºå¾—ç¥¨ç‡æœ€é«˜çš„ç±»æ ‡ $\hat y$:

$$
\hat y = model \{ C_1(x), C_2(x), \dots C_m(x) \}
$$

ä¾‹å¦‚ï¼Œåœ¨äºŒç±»åˆ«åˆ†ç±»ä¸­ï¼Œå‡å®š `class=-1`ã€`class=+1`ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¤šæ•°æŠ•ç¥¨é¢„æµ‹è¡¨ç¤ºä¸º:

$$
C(x) = sign
\begin{bmatrix}
\sum_j^m C_j(x)
\end{bmatrix}= 
\begin{cases}
1 & \text è‹¥ \sum_i C_j(x) \ge 0 \\
-1 & \text å…¶ä»–
\end{cases}
$$

ä¸ºäº†è¯´æ˜é›†æˆæ–¹æ³•çš„æ•ˆæœä¸ºä½•å¥½äºå•ä¸ªæˆå‘˜åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬å€Ÿç”¨ä¸‹ç»„åˆå­¦ä¸­çš„æ¦‚å¿µã€‚æ¥ä¸‹æ¥çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å‡å®šäºŒç±»åˆ«ä¸­çš„ n ä¸ªæˆå‘˜åˆ†ç±»å™¨éƒ½æœ‰ç›¸åŒçš„å‡ºé”™ç‡ã€‚æ­¤å¤–ï¼Œå‡å®šæ¯ä¸ªåˆ†ç±»å™¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸”å‡ºé”™ç‡ä¹‹é—´æ˜¯ä¸ç›¸å…³çš„ã€‚åŸºäºè¿™äº›å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆå‘˜åˆ†ç±»å™¨é›†æˆåçš„å‡ºé”™ç‡ç®€å•è¡¨ç¤ºä¸ºäºŒé¡¹åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°:

$$
P(y \ge k) = \sum_k^n 
\langle \mathop{}_{k}^{n} \rangle
\varepsilon^k (1-\varepsilon)^{n-k} = \varepsilon_{ensemble}
$$

å…¶ä¸­ï¼Œ$\langle \mathop{}_{k}^{n} \rangle$ æ˜¯ né€‰ kç»„åˆçš„äºŒé¡¹å¼ç³»æ•°ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬è®¡ç®—æ­¤é›†æˆåˆ†ç±»å™¨é¢„æµ‹ç»“æœå‡ºé”™çš„æ¦‚ç‡ã€‚å†æ¥çœ‹ä¸€ä¸ªæ›´å…·ä½“çš„ä¾‹å­: å‡å®šä½¿ç”¨ 11ä¸ªåˆ†ç±»å™¨(n=11)ï¼Œå•ä¸ªåˆ†ç±»å™¨å‡ºé”™ç±»ä¸º 0.25($\varepsilon =0.25$):

$$
P(y \ge k) = \sum_{k=6}^n 
\langle \mathop{}_{k}^{11 } \rangle
0.25^k (1-\varepsilon)^{11 -k} = 0.034
$$

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨æ»¡è¶³æ‰€æœ‰å‡è®¾çš„æ¡ä»¶ä¸‹ï¼Œé›†æˆåçš„å‡ºé”™ç‡(0.034) è¿œè¿œå°äºå•ä¸ªåˆ†ç±»å™¨çš„å‡ºé”™ç‡(0.25)ã€‚è¯·æ³¨æ„ï¼Œåœ¨æ­¤æ¼”ç¤ºç¤ºä¾‹ä¸­ï¼Œå½“é›†æˆåˆ†ç±»ä¸­åˆ†ç±»å™¨ä¸ªæ•° n ä¸ºå¶æ•°æ—¶ï¼Œè‹¥é¢„æµ‹ç»“æœä¸ºäº”äº”åˆ†ï¼Œæˆ‘ä»¬åˆ™å°†å…¶ä»¥é”™è¯¯å¯¹å¾…ï¼Œä¸è¿‡ä»…æœ‰ä¸€åŠçš„å¯èƒ½ä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚ä¸ºäº†æ¯”è¾ƒæˆå‘˜åˆ†ç±»å™¨åœ¨ä¸åŒå‡ºé”™ç‡çš„æƒ…å†µä¸‹ä¸é›†æˆåˆ†ç±»å™¨å‡ºé”™ç‡çš„å·®å¼‚ï¼Œæˆ‘ä»¬åˆ©ç”¨ Python å®ç°å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°:

```python
from scipy.version import version as SCIPY_VERSION

if SCIPY_VERSION >= '1.0.0':
    from scipy.special import comb
else:
    from scipy.misc import comb
import math

def ensemble_error(n_classifier, error):
    k_start = math.ceil(n_classifier / 2.0)
    probs = [comb(n_classifier, k) * error ** k * (1 - error) ** (n_classifier - k)
             for k in range(k_start, n_classifier + 1)]
    return sum(probs)

ensemble_error(n_classifier=11, error=0.25)
```

è¾“å‡ºç»“æœ:

```text
0.03432750701904297
```

å®ç° emsemble_error å‡½æ•°åï¼Œåœ¨æˆå‘˜åˆ†ç±»å™¨å‡ºé”™ç•¥ä»‹äº 0.0 åˆ° 1.0 èŒƒå›´å†…ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å¯¹åº”é›†æˆåˆ†ç±»å™¨çš„å‡ºé”™ç‡ï¼Œå¹¶ä»¥å‡½æ•°æ›²çº¿çš„å½¢å¼ç»˜åˆ¶å‡ºå®ƒä»¬ä¹‹é—´çš„å…³ç³»:

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

error_range = np.arange(0.0, 1.01, 0.01)
ens_errors = [ensemble_error(n_classifier=11, error=error) for error in error_range]
plt.plot(error_range, ens_errors, label="Ensemble error", linewidth=2)
plt.plot(error_range, error_range, linestyle="--", label="Base error", linewidth=2)
plt.xlabel("Base error")
plt.ylabel("Base/Ensemble error")
plt.legend(loc="upper left")
plt.grid()
plt.show()
```

![](https://imgconvert.csdnimg.cn/aHR0cDovL2dpdGh1Yi5jb20vbWluZ21pbnl1L2ltYWdlcy9yYXcvbWFzdGVyL3B5dGhvbi1tYWNoaW5lLWxlYXJuaW5nL2NoMDcvNy0zLmpwZw)


ä»ç»“æœå›¾åƒå¯è§: å½“æˆå‘˜åˆ†ç±»å™¨å‡ºé”™ç‡ä½äºéšæœºçŒœæµ‹æ—¶($\varepsilon < 0.5$)ï¼Œé›†æˆåˆ†ç±»å™¨çš„å‡ºé”™ç‡è¦ä½äºå•ä¸ªåˆ†ç±»å™¨ã€‚è¯·æ³¨æ„: yè½´åŒæ—¶æ ‡è¯†äº†æˆå‘˜åˆ†ç±»å™¨çš„å‡ºé”™ç‡(è™šçº¿)å’Œé›†æˆåˆ†äº†å™¨çš„å‡ºé”™ç‡(å®ç°):

# 7.2 å®ç°ä¸€ä¸ªç®€å•çš„å¤šæ•°æŠ•ç¥¨åˆ†ç±»å™¨

é€šè¿‡ä¸Šä¸€å°èŠ‚çš„ä»‹ç»ï¼Œæˆ‘ä»¬å·²ç»è¿‡é›†æˆå­¦ä¹ æœ‰äº†åˆæ­¥çš„äº†è§£ï¼Œç°åœ¨æ¥åšä¸€ä¸ªçƒ­èº«ç»ƒä¹ : åŸºäºå¤šæ•°æŠ•ç¥¨åŸåˆ™ï¼Œä½¿ç”¨ Python å®ç°ä¸€ä¸ªç®€å•çš„é›†æˆåˆ†ç±»å™¨ã€‚è™½ç„¶ä¸‹è¿°ç®—æ³•å¯é€šè¿‡ç®€å•é”™è¾“æŠ•ç¥¨æ¨å¹¿åˆ°å¤šåˆ†ç±»å™¨çš„æƒ…å½¢ï¼Œä¸è¿‡ä¸ºäº†ç®€å•æœŸé—´ï¼Œæˆ‘ä»¬ä»æ—§ä½¿ç”¨æ›´å¸¸å‡ºç°åœ¨æ–‡çŒ®ä¸­çš„æœ¯è¯­: å¤šæ•°æŠ•ç¥¨(majority voting)ã€‚

é›†æˆç®—æ³•å…è®¸æˆ‘ä»¬ä½¿ç”¨å•ç‹¬çš„æƒé‡å¯¹ä¸åŒåˆ†ç±»ç®—æ³•è¿›è¡Œç»„åˆã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ›´åŠ å¼ºå¤§çš„å…ƒåˆ†ç±»å™¨ï¼Œä»¥åœ¨ç‰¹å®šçš„æ•°æ®é›†ä¸Šå¹³è¡¡å•ä¸ªåˆ†ç±»å™¨çš„å¼±ç‚¹ã€‚é€šè¿‡æ›´ä¸¥æ ¼çš„æ•°å­¦æ¦‚å¿µï¼Œå¯ä»¥å°†åŠ æƒå¤šæ•°æŠ•ç¥¨è®°ä¸º:

$$
\hat y = arg \; \mathop{max}_{i}\sum_{j=1}^m w_j \chi_A(C_j(x) = i)
$$

å…¶ä¸­ï¼Œ$w_j$ æ˜¯æˆå‘˜åˆ†ç±»å™¨ $C_j$ å¯¹åº”çš„æƒé‡ï¼Œ$\hat y$ æ˜¯é›†æˆåˆ†ç±»å™¨çš„é¢„æµ‹ç±»æ ‡ï¼Œ$\chi_A$(å¸Œæœ›å­—æ¯chi)ä¸ºç‰¹å¾å‡½æ•° $[C_j(x)=i \in A]$ï¼ŒA ä¸ºç±»æ ‡çš„é›†åˆã€‚å¦‚æœæƒé‡å‡ç­‰ï¼Œå¯ä»¥å°†æ­¤å…¬å¼ç®€åŒ–ä¸º:

$$
\hat y=mode\{ C_1(x),C_2(x),...C_m(x)\}
$$

ä¸ºäº†æ›´å¥½åœ°ç†è§£æƒé‡åœ¨è¿™é‡Œçš„å«ä¹‰ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ä¸ªæ›´åŠ å…·ä½“çš„ä¾‹å­ã€‚å‡å®šæœ‰ä¸‰ä¸ªæˆå‘˜åˆ†ç±»å™¨ $C_j(j \in \{1,2,3\})$ï¼Œåˆ†åˆ«ç”¨å®ƒä»¬é¢„æµ‹æ ·æœ¬çš„ $x$ çš„ç±»æ ‡ã€‚å…¶ä¸­ä¸¤ä¸ªæˆå‘˜åˆ†ç±»å™¨çš„é¢„æµ‹ç»“æœä¸ºç±»åˆ« 0ï¼Œè€Œå¦ä¸€ä¸ªåˆ†ç±»å™¨ $C_3$ çš„é¢„æµ‹ç»“æœä¸ºç±»åˆ« 1.å¦‚æœæˆ‘ä»¬å¹³ç­‰åœ°çœ‹å¾…æ¯ä¸ªæˆå‘˜åˆ†ç±»å™¨ï¼ŒåŸºäºå¤šæ•°æŠ•ç¥¨åŸåˆ™ï¼Œæœ€ç»ˆçš„é¢„æµ‹ç»“æœåº”è¯¥æ˜¯æ ·æœ¬æœ¯è¯­ç±»åˆ« 0:

$$
C_1 \rightarrow 0, C_2 \rightarrow 0, C_3 \rightarrow 1 \\
\hat y = mode\{ 0,0,1 \} = 0
$$

æˆ‘ä»¬å°†æƒé‡ 0.6 èµ‹ç»™ $C_3$ï¼Œè€Œ $C_1$ å’Œ $C_2$ å‡ä¸º 0.2ï¼Œæœ‰:

$$
\begin{aligned}
\hat y &= arg \; \mathop{max}_{i}\sum_{j=1}^m w_j \chi_A(C_j(x) = i) \\
&= arg \; \mathop{max}_{i} [0.2 \times i_0 + 0.2 \times i_0 + 0.2 \times i_1] = 1
\end{aligned}
$$

æ›´ç›´è§‚åœ°ï¼Œç”±äº $3 \times 0.2 = 0.6$ï¼Œå¯ä»¥è®¤ä¸ºåˆ†ç±»å™¨ $C_3$ çš„ä¸€æ¬¡é¢„æµ‹æƒé‡ç›¸å½“äºåˆ†ç±»å™¨ $C_1$ æˆ– $C_2$ çš„ä¸‰æ¬¡é¢„æµ‹æƒé‡ä¹‹å’Œï¼Œæˆ‘ä»¬å¯ä»¥å†™ä½œ:

$$
\hat y = mode \{ 0,0,1,1,1 \} = 1
$$

ä¸ºäº†ä½¿ç”¨ Python ä»£ç å®ç°åŠ æƒå¤šæ•°æŠ•ç¥¨ï¼Œå¯ä»¥ä½¿ç”¨ NumPy ä¸­çš„ argmax å’Œ bincount å‡½æ•°:

```python
import numpy as np

np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]))
```

è¾“å‡ºç»“æœ:

```text
1
```

æˆ‘ä»¬å°†åœ¨ç¬¬3ç« ä¸­æ›¾è®¨è®ºè¿‡ï¼Œé€šè¿‡ predict_proba æ–¹æ³•ï¼Œscikir-learn ä¸­åˆ†ç±»å™¨å¯ä»¥è¿”å›æ ·æœ¬å±äºé¢„æµ‹ç±»æ ‡çš„æ¦‚ç‡ã€‚å¦‚æœé›†æˆåˆ†ç±»å™¨å®ç°å¾—åˆ°è‰¯å¥½çš„ä¿®æ­£ï¼Œé‚£ä¹ˆåœ¨å¤šæ•°æŠ•ç¥¨ä¸­ä½¿ç”¨ä¸çŒœæµ‹ç±»åˆ«çš„æ¦‚ç‡æ›¿ä»£ç±»æ ‡ä¼šéå¸¸æœ‰ç”¨ã€‚ä½¿ç”¨ç±»åˆ«æ¦‚ç‡è¿›è¡Œé¢„æµ‹çš„å¤šæ•°æŠ•ç¥¨ä¿®æ”¹ç‰ˆæœ¬å¯è®°ä¸º:

$$
\hat y = arg \; \mathop{max}_{i} \sum_{j=1}^m w_jp_{ij}
$$

å…¶ä¸­ï¼Œ$p_{ij}$ æ˜¯ç¬¬ $j$ ä¸ªåˆ†ç±»å™¨é¢„æµ‹æ ·æœ¬ç±»æ ‡ä¸º $i$ çš„æ¦‚ç‡ã€‚

ç»§ç»­å‰é¢çš„ä¾‹å­ï¼Œå‡è®¾æˆ‘ä»¬é¢ä¸´ä¸€ä¸ªäºŒç±»åˆ«åˆ†ç±»é—®é¢˜ï¼Œå…¶ç±»æ ‡ä¸º $i \in \{0,1\}$ï¼Œä¸‹é¢é›†æˆè¿™ä¸‰ä¸ªåˆ†ç±»å™¨ $C_j(j \in \{1,2,3\})$ã€‚å‡å®šåˆ†ç±»å™¨ $C_j$ é’ˆå¯¹æŸä¸€æ ·æœ¬ x æŒ‰ç…§å¦‚ä¸‹æ¦‚ç‡è¿”å›ç±»æ ‡çš„é¢„æµ‹ç»“æœ:

$$
C_1 \rightarrow [0.9,0.1], C_2 \rightarrow [0.8,0.2], C_3 \rightarrow [0.4,0.6] 
$$

æˆ‘ä»¬æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è®¡ç®—æ‰€å±ç±»åˆ«çš„æ¦‚ç‡:

$$
\begin{aligned}
p(i_0|x) = 0.2 \times 0.9 + 0.2 \times 0.8 + 0.6 \times 0.4 = 0.58 \\
p(i_0|x) = 0.2 \times 0.1 + 0.2 \times 0.2 + 0.6 \times 0.6 = 0.42
\end{aligned}
$$

ä¸ºå®ç°åŸºäºç±»åˆ«é¢„æµ‹æ¦‚ç‡çš„åŠ æƒå¤šæ•°æŠ•ç¥¨ï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨ NumPy ä¸­çš„ np.average å’Œ np.argmax æ–¹æ³•:

```python
ex = np.array([[0.9, 0.1], [0.8, 0.2], [0.4, 0.6]])
p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])
print(p)
print(np.argmax(p))
```

è¾“å‡ºç»“æœ:

```output
[0.58 0.42]
0
```

ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬ä½¿ç”¨ Python æ¥å®ç° MajorityVoteClassifier ç±»:

```python
from sklearn.base import BaseEstimator, ClassifierMixin, clone
from sklearn.preprocessing import LabelEncoder
from sklearn.externals import six
from sklearn.pipeline import _name_estimators
import numpy as np
import operator


class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):
    """A majority vote ensemble classifier
    
    Parameters
    ----------
    classifiers : array-like, shape = [n_classifiers]
      Different classifiers for ensemble
    
    vote : str, {'classlabel', 'probability'}
      Default: 'classlabel'
      If 'classlabel' the prediction is based on
      the argmax of class labels. Else if
      'probability', the argmax of the sum of
      probabilities is used to predict the class label
      (recommend for calibrated classifiers).
    
    weights : array-like, shape = [n_classifiers]
      Optional, defaulr: None
      If a list of `int` or `float` values are
      provided, the classifiers are weigted by
      importancce; Uses uniform weights if `weights=None`.
    """
    def __init__(self, classifiers, vote='classlabel', weights=None):
        self.classifiers = classifiers
        self.named_classifiers = {key: value 
                for key, value in _name_estimators(classifiers)}
        self.vote = vote
        self.weights = weights
    
    def fit(self, X, y):
        """Fit classifiers.
        
        Parameters
        ----------
        X : {array-like, sparse matrix},
            shape = [n_samples, n_features]
            Matrix of training samples.
            
        y : array-like, shape = [n_samples]
            Vector of target class labels/
        
        Returns
        -------
        self : object
        """
        # Use LabelEncoder to ensure class labels start
        # with 0, which is important for np.argmax
        # call in self.predict
        self.labelnc_ = LabelEncoder()
        self.labelnc_.fit(y)
        self.classes_ = self.labelnc_.classes_
        self.classifiers_ = [clone(clf).fit(X, self.labelnc_.transform(y)) 
                               for clf in self.classifiers]
        return self
```

ä¸ºäº†å¸®åŠ©è¯»è€…æ›´å¥½åœ°ç†è§£è¿™éƒ¨åˆ†å†…å®¹ï¼Œæˆ‘åœ¨ä»£ç ä¸­åŠ å…¥äº†å¤§é‡çš„æ³¨é‡Šã€‚ä¸è¿‡ï¼Œåœ¨å®ç°å…¶ä»–çš„æ–¹æ³•ä¹‹å‰ï¼Œæˆ‘ä»¬ç¨ä½œä¸­æ–­ï¼Œå…ˆå¿«é€Ÿåœ°è®¨è®ºä¸€ä¸‹çœ‹ä¼¼è®©äººçœ¼èŠ±ç¼­ä¹±çš„ä»£ç ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåŸºç±» BaseEstimator å’Œ ClassifierMixin è·å–æŸäº›æ–¹æ³•ï¼ŒåŒ…æ‹¬è®¾å®šåˆ†ç±»å™¨å‚æ•°çš„ set_params å’Œè¿”å›å‚æ•°çš„ get_params æ–¹æ³•ï¼Œä»¥åŠç”¨äºè®¡ç®—é¢„æµ‹å‡†ç¡®ç‡çš„ score æ–¹æ³•ã€‚æ­¤å¤–è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯¼å…¥ six åŒ…è‰ä¸›è€Œä½¿å¾— MajorityVoteClassifier ä¸ Python 2.7 å…¼å®¹ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬åŠ å…¥ predict æ–¹æ³•: å¦‚æœä½¿ç”¨å‚æ•° `vote='classlabel'` åˆå§‹åŒ– MajorityVote-Classifier å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å¤šæ•°æŠ•ç¥¨æ¥é¢„æµ‹ç±»æ ‡ï¼Œç›¸åï¼Œå¦‚æœä½¿ç”¨å‚æ•° `vote='probability'` åˆå§‹åŒ–é›†æˆåˆ†ç±»å™¨ï¼Œåˆ™å¯åŸºäºç±»åˆ«æˆå‘˜çš„æ¦‚ç‡è¿›è¡Œç±»æ ‡é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åŠ å…¥ predict_proba æ–¹æ³•æ¥è¿”å›å¹³å‡æ¦‚ç‡ï¼Œè¿™åœ¨è®¡ç®—å—è¯•å·¥ä½œè€…ç‰¹å¾çº¿ä¸‹åŒºåŸŸ(Receiver Operator Characteristic under the curve, ROC AUC) æ—¶éœ€è¦ç”¨åˆ°ã€‚

```python
from sklearn.base import BaseEstimator, ClassifierMixin, clone
from sklearn.preprocessing import LabelEncoder
from sklearn.externals import six
from sklearn.pipeline import _name_estimators
import numpy as np
import operator


class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):
    """A majority vote ensemble classifier
    
    Parameters
    ----------
    classifiers : array-like, shape = [n_classifiers]
      Different classifiers for ensemble
    
    vote : str, {'classlabel', 'probability'}
      Default: 'classlabel'
      If 'classlabel' the prediction is based on
      the argmax of class labels. Else if
      'probability', the argmax of the sum of
      probabilities is used to predict the class label
      (recommend for calibrated classifiers).
    
    weights : array-like, shape = [n_classifiers]
      Optional, defaulr: None
      If a list of `int` or `float` values are
      provided, the classifiers are weigted by
      importancce; Uses uniform weights if `weights=None`.
    """
    def __init__(self, classifiers, vote='classlabel', weights=None):
        self.classifiers = classifiers
        self.named_classifiers = {key: value 
                for key, value in _name_estimators(classifiers)}
        self.vote = vote
        self.weights = weights
    
    def fit(self, X, y):
        """Fit classifiers.
        
        Parameters
        ----------
        X : {array-like, sparse matrix},
            shape = [n_samples, n_features]
            Matrix of training samples.
            
        y : array-like, shape = [n_samples]
            Vector of target class labels/
        
        Returns
        -------
        self : object
        """
        # Use LabelEncoder to ensure class labels start
        # with 0, which is important for np.argmax
        # call in self.predict
        self.labelnc_ = LabelEncoder()
        self.labelnc_.fit(y)
        self.classes_ = self.labelnc_.classes_
        self.classifiers_ = [clone(clf).fit(X, self.labelnc_.transform(y)) 
                               for clf in self.classifiers]
        return self
    
    def predict(self, X):
        """Predict class labels for X.
        
        Parametres
        ----------
        X : {array-like, sparse matrix},
            shape = [n_samples, n_features]
            Matrix of training samples.
        
        Returns
        ----------
        maj_vote : array-like, shape = [n_samples]
            Predicted class labels.
        """
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else: # 'classlabel' vite
            # Collect results from clf.predict class
            predictions = np.asarray([clf.predict(X) 
                        for clf in self.classifiers_]).T
            maj_vote = np.apply_along_axis(
                lambda x: np.argmax(np.bincount(x, weights=self.weights)),
                axis=1, arr=predictions)
        maj_vote = self.labelnc_.inverse_transform(maj_vote)
        return maj_vote

    def predict_proba(self, X):
        """Predict class probabilities for X.

            Parametres
            ----------
            X : {array-like, sparse matrix},
                shape = [n_samples, n_features]
                Training vecctors, where n_samples is
                the number of samples and n_feayures
                is the number of features.

            Returns
            ----------
           avg_proba : array_like
               shape = [n_samples, n_classes]
               Weighted average probability for
               each class per sample.
        """
        probas = np.asarray([clf.predict_proba(X) 
                        for clf in self.classifiers_])
        avg_proba = np.average(probas, axis=0, weights=self.weights)
        return avg_proba

    def get_params(self, deep=True):
        """Get classifier parameter names for GridSearcch"""
        if not deep:
            return super(MajorityVoteClassifier, self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in six.iteritems(self.named_classifiers):
                for key, value in six.iteritems(step.get_params(deep=True)):
                    out['%s_%s' % (name, key)] = value
            return out
```

æ­¤å¤–è¿˜è¯·æ³¨æ„: è¿™é‡Œè¿˜å®šä¹‰äº†æˆ‘ä»¬è‡ªè¡Œä¿®æ”¹çš„ get_params æ–¹æ³•ï¼Œä»¥æ–¹ä¾¿ä½¿ç”¨ `_name_estimators` å‡½æ•°æ¥è®¿é—®é›†æˆåˆ†ç±»å™¨ä¸­ç‹¬ç«‹æˆå‘˜å‡½æ•°çš„å‚æ•°ã€‚å…¶å­˜å‚¨çœ‹èµ·æ¥è¿™æœ‰ç‚¹å¤æ‚ï¼Œä½†æ˜¯åœ¨åç»­å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç½‘ç»œæœç´¢æ¥è°ƒæ•´è¶…å‚ï¼Œé‚£æ—¶ä½ ä¼šæ„Ÿåˆ°è¿™æ ·åšæ„ä¹‰éå‡¡ã€‚

ğŸ”– ä¸»è¦å‡ºäºæ¼”ç¤ºçš„ç›®çš„ï¼Œæˆ‘ä»¬æ‰å®ç°äº† MajorityVoteClassifier ç±»ï¼Œä¸è¿‡è¿™ä¹Ÿå®Œæˆäº† scikit-learn ä¸­å…³äºå¤šæ•°æŠ•ç¥¨çš„ä¸€ä¸ªç›¸å¯¹å¤æ‚çš„ç‰ˆæœ¬ã€‚å®ƒå°†å‡ºç°åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬(v.0.17) çš„ sklearn.ensemble.VotingClassifier ç±»ä¸­ã€‚

## 7.2.1 åŸºäºå¤šæ•°æŠ•ç¥¨åŸåˆ™ç»„åˆä¸åŒçš„åˆ†ç±»ç®—æ³•

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†ä¸Šä¸€å°èŠ‚å®ç°çš„ MajorityVoteClassifier ç”¨äºå®æˆ˜äº†ã€‚ä¸è¿‡ï¼Œå…ˆå‡†å¤‡ä¸€ä¸ªå¯ä»¥ç”¨äºæµ‹è¯•çš„æ•°æ®é›†ã€‚æ—¢ç„¶å·²ç»ä¹¦å†™äº†ä» CSV æ–‡ä»¶ä¸­è¯»å–æ•°æ®çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°±èµ°æ·å¾„ä» scikit-learn çš„ dataset æ¨¡å—ä¸­åŠ è½½é¸¢å°¾èŠ±æ•°æ®ç»“é›†ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä½¿åˆ†ç±»æ›´å…·æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åªé€‰æ‹©å…¶ä¸­çš„ä¸¤ä¸ªç‰¹å¾: è¼ç‰‡å®½åº¦å’ŒèŠ±ç“£é•¿åº¦ã€‚è™½ç„¶ MajorityVoteClassifier å¯åº”ç”¨åˆ°å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ï¼Œä½†æˆ‘ä»¬åªåŒºåˆ†ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬: Iris-Versicolor å’Œ Iris-Virginicaï¼Œå¹¶ç»˜åˆ¶å…¶ ROC AUCã€‚ä»£ç å¦‚ä¸‹:

```py
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

iris = datasets.load_iris()
X, y = iris.data[50:, [1,2]], iris.target[50:]
le = LabelEncoder()
y = le.fit_transform(y)
```

ğŸ”– è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ sccikit-learn ä¸­çš„ predict_proba æ–¹æ³•(å¦‚æœé€‚ç”¨)è®¡ç®— ROC AUC çš„è¯„åˆ†ã€‚åœ¨ç¬¬3ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•é€šè¿‡ Logistic å›å½’æ¨¡å‹æ¥è®¡ç®—å„ç±»åˆ«çš„å½’å±æ¦‚ç‡ï¼›åœ¨å†³ç­–æ ‘ä¸­ï¼Œæ­¤æ¦‚ç‡æ˜¯é€šè¿‡è®­ç»ƒæ—¶æ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºçš„é¢‘åº¦å‘é‡(frequenccy vector) æ¥è®¡ç®—çš„ã€‚æ­¤å‘é‡æ”¶é›†å¯¹åº”èŠ‚ç‚¹ä¸­é€šè¿‡ç±»æ ‡åˆ†å¸ƒè®¡ç®—å¾—åˆ°å„ç±»æ ‡é¢‘ç‡å€¼ã€‚è¿›è€Œå¯¹é¢‘ç‡è¿›è¡Œå½’ä¸€åŒ–å­˜è½¦å¤„æï¼Œä½¿å¾—å®ƒä»¬çš„å’Œä¸º 1.ç±»ä¼¼åœ°ï¼Œk-è¿‘é‚»ç®—æ³•åˆ†ç±»å™¨éƒ½è¿”å›äº†ä¸ Logisticc å›å½’æ¨¡å‹ç±»ä¼¼çš„æ¦‚ç‡å€¼ï¼Œä½†å¿…é¡»æ³¨æ„ï¼Œè¿™äº›å€¼å¹¶éé€šè¿‡æ¦‚ç‡å¯†åº¦å‡½æ•°è®¡ç®—å¾—åˆ°çš„ã€‚

ä¸‹é¢æˆ‘ä»¬å°†æ ·æœ¬æŒ‰ç…§äº”äº”åˆ†åˆ’åˆ†ä¸ºè®­ç»ƒæ•°æ®åŠçŒœæµ‹æ•°æ®:

```python
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.5, random_state=1)
```

æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒæ•°æ®é›†è®­ç»ƒä¸‰ç§ä¸åŒç±»å‹çš„åˆ†ç±»å™¨: Logistic å›å½’åˆ†ç±»å™¨ã€å†³ç­–æ ‘åˆ†ç±»å™¨åŠ k-è¿‘é‚»åˆ†ç±»å™¨å„ä¸€ä¸ªï¼Œåœ¨å°†å®ƒä»¬ç»„åˆæˆé›†æˆåˆ†ç±»å™¨ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡ 10æŠ˜äº¤å‰éªŒè¯çœ‹ä¸€ä¸‹å„ä¸ªåˆ†ç±»å™¨åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°:

```python
from sklearn.cross_validation import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
import numpy as np

clf_lr = LogisticRegression(penalty='l2', C=0.001, random_state=0)
clf_dt = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)
clf_knn = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')
pipe_lr = Pipeline([['sc', StandardScaler()], ['clf', clf_lr]])
pipe_knn = Pipeline([['sc', StandardScaler()], ['clf', clf_knn]])

clf_labels = ['Logistic Regression', 'Decision Tree', 'KNN']
print('10-fold cross validation: \n')

for clf, label in zip([pipe_lr, clf_dt, pipe_knn], clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, 
                             cv=10, scoring='roc_auc')
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]" 
          % (scores.mean(), scores.std(), label))
```

è¾“å‡ºç»“æœ:

```Output
10-fold cross validation: 

ROC AUC: 0.92 (+/- 0.20) [Logistic Regression]
ROC AUC: 0.92 (+/- 0.15) [Decision Tree]
ROC AUC: 0.93 (+/- 0.10) [KNN]
```

è¯»è€…å¯èƒ½ä¼šæ„Ÿåˆ°å¥‡æ€ªï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬å°† Logistic å›å½’å’Œ KNN åˆ†ç±»å™¨çš„è®­ç»ƒä½œä¸ºæµæ°´çº¿çš„ä¸€éƒ¨åˆ†ï¼ŸåŸå› åœ¨äº: å¦‚æˆ‘ä»¬ç¬¬ 3ç« æ‰€è¿°ï¼Œä¸åŒäºå†³ç­–æ ‘ï¼ŒLogistic å›å½’ä¸ KNN ç®—æ³•(ä½¿ç”¨æ¬§å‡ é‡Œå¾—ä¸¾ä¾‹ä½œä¸ºè·ç¦»åº¦é‡æ ‡å‡†) å¯¹æ•°æ®ç¼©æ”¾ä¸æ•æ„Ÿã€‚è™½ç„¶é¸¢å°¾èŠ±ç‰¹å¾éƒ½ä»¥ç›¸åŒçš„å°ºåº¦(å˜ç±³)åº¦é‡ï¼Œä¸è¿‡å¯¹ç‰¹å¾æ ‡å‡†åŒ–å¤„ç†æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚

æ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»åˆ°äº†ï¼Œæˆ‘ä»¬ç°åœ¨åŸºäºå¤šæ•°æŠ•ç¥¨åŸåˆ™ï¼Œåœ¨ MajorityVoteClassifier ä¸­ç»„åˆå„æˆå‘˜åˆ†ç±»å™¨:

```python
mv_clf = MajorityVoteClassifier(classifiers=[pipe_lr, clf_dt, pipe_knn])
clf_labels += ['Majority Voting']
all_clf = [pipe_lr, clf_dt, pipe_knn, mv_clf]

for clf, label in zip([pipe_lr, clf_dt, pipe_knn, mv_clf], clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, 
                             cv=10, scoring='roc_auc')
    print("Accuracy: %0.2f (+/- %0.2f) [%s]" 
          % (scores.mean(), scores.std(), label))
```

è¾“å‡ºç»“æœ:

```output
Accuracy: 0.92 (+/- 0.20) [Logistic Regression]
Accuracy: 0.92 (+/- 0.15) [Decision Tree]
Accuracy: 0.93 (+/- 0.10) [KNN]
Accuracy: 0.97 (+/- 0.10) [Majority Voting]
```

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œä»¥ 10æŠ˜äº¤å‰éªŒè¯ä½œä¸ºè¯„ä¼°æ ‡å‡†ï¼ŒMajorityVotingClassifier çš„æ€§èƒ½ä¸å•ä¸ªæˆå‘˜åˆ†ç±»å™¨ç›¸æ¯”æœ‰ç€è´¨çš„æé«˜ã€‚

# 7.3 è¯„ä¼°ä¸è°ƒä¼˜é›†æˆåˆ†ç±»å™¨

åœ¨æœ¬èŠ‚ï¼Œæˆ‘ä»¬å°†åœ¨æµ‹è¯•æ•°æ®ä¸Šè®¡ç®— MajorityVoteClassifier ç±»çš„ ROC æ›²çº¿ï¼Œä»¥éªŒè¯å…¶åœ¨ä½ç½®æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯·è®°ä½ï¼Œè®­ç»ƒæ•°æ®å¹¶æœªåº”ç”¨äºæ¨¡å‹é€‰æ‹©ï¼Œä½¿ç”¨å®ƒä»¬çš„å”¯ä¸€ç›®çš„å°±æ˜¯å¯¹åˆ†ç±»ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›åšå‡ºæ— åå·®çš„ä¼°è®¡ã€‚ä»£ç å¦‚ä¸‹:

```python
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
import matplotlib.pyplot as plt

colors = ['black', 'orange', 'blue', 'green']
linestyles = [':', '--', '-.', '-']

for clf, label, clr, ls in zip(all_clf, clf_labels, colors, linestyles):
    # assuming the label of the positive class is 1
    y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred)
    roc_auc = auc(x=fpr, y=tpr)
    plt.plot(fpr, tpr, color=clr, linestyle=ls, 
             label='%s (auc = %0.2f)' % (label, roc_auc))

plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2)
plt.xlim([-0.1, 1.1])
plt.ylim([-0.1, 1.1])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()
```

ç”± ROC ç»“æœæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé›†æˆåˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜ç§€(ROC AUC=0.95)ï¼Œè€Œ KNN åˆ†ç±»å™¨å¯¹äºè®­ç»ƒæ•°æ®æœ‰äº›è¿‡æ‹Ÿåˆ(è®­ç»ƒé›†ä¸Šçš„ ROC AUC=0.93ï¼Œæµ‹è¯•é›†ä¸Š ROC AUC=0.86):

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235737653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

å› ä¸ºåªä½¿ç”¨äº†åˆ†ç±»æ ·æœ¬ä¸­çš„ä¸¤ä¸ªç‰¹å¾ï¼Œé›†æˆåˆ†ç±»å™¨çš„å†³ç­–åŒºåŸŸåˆ°åº•æ˜¯ä»€ä¹ˆæ ·å­å¯èƒ½ä¼šå¼•èµ·æˆ‘ä»¬çš„å…´è¶£ã€‚ç”±äº Logistic å›å½’å’Œ KNN æµæ°´çº¿ä¼šè‡ªåŠ¨å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå› æ­¤ä¸å¿…å®ç°å¯¹è®­ç»ƒç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ã€‚ä¸è¿‡å‡ºäºå¯è§†åŒ–è€ƒè™‘ï¼Œä¹Ÿå°±æ˜¯åœ¨ç›¸åŒçš„åº¦é‡æ ‡å‡†ä¸Šå®ç°å†³ç­–åŒºåŸŸï¼Œæˆ‘ä»¬åœ¨æ­¤å¯¹è®­ç»ƒæ•°æ®é›†åšäº†æ ‡å‡†åŒ–å¤„ç†ã€‚ä»£ç å¦‚ä¸‹:

```python
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from itertools import product

iris = datasets.load_iris()
X, y = iris.data[50:, [1,2]], iris.target[50:]
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
x_min = X_train_std[:, 0].min() - 1
x_max = X_train_std[:, 0].max() + 1
y_min = X_train_std[:, 1].min() - 1
y_max = X_train_std[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))
f, axarr = plt.subplots(nrows=2, ncols=2, 
                        sharex='col', sharey='row',
                        figsize=(7,5))

clf_lr = LogisticRegression(penalty='l2', C=0.001, random_state=0)
clf_dt = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)
clf_knn = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')
pipe_lr = Pipeline([['sc', StandardScaler()], ['clf', clf_lr]])
pipe_knn = Pipeline([['sc', StandardScaler()], ['clf', clf_knn]])
mv_clf = MajorityVoteClassifier(classifiers=[pipe_lr, clf_dt, pipe_knn])
clf_labels += ['Majority Voting']
all_clf = [pipe_lr, clf_dt, pipe_knn, mv_clf]

for idx, clf, tt in zip(product([0, 1], [0, 1]), all_clf, clf_labels):
    clf.fit(X_train_std, y_train)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)
    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0],
            X_train_std[y_train==0, 1], c='b', marker='^', s=50)
    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0],
            X_train_std[y_train==1, 1], c='r', marker='o', s=50)
    axarr[idx[0], idx[1]].set_title(tt)

plt.text(-3.5, -4.5, s='Sepal width [standardized]', 
         ha='center', va='center', fontsize=12)
plt.text(-10.5, 4.5, s='Petal length [standardized]', 
         ha='center', va='center', fontsize=12, rotation=90)
plt.show()
```

å°½ç®¡æœ‰è¶£ï¼Œä½†ä¹Ÿå¦‚æˆ‘ä»¬æ‰€é¢„æœŸçš„é‚£æ ·ï¼Œé›†æˆåˆ†ç±»å™¨çš„å†³ç­–åŒºåŸŸçœ‹èµ·æ¥åƒæ˜¯å„æˆå‘˜åˆ†ç±»å™¨å†³ç­–åŒºåŸŸçš„æ··åˆã€‚ä¹çœ‹ä¹‹ä¸‹ï¼Œå¤šæ•°æŠ•ç¥¨çš„å†³ç­–åŒºåŸŸä¸ KNN åˆ†ç±»å™¨å¾ˆç›¸ä¼¼ã€‚ä¸è¿‡ï¼Œæ­£å¦‚å•å±‚å†³ç­–æ ‘é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒåœ¨ sepal width $\ge$ 1æ—¶æ­£äº¤äº yè½´:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235753833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

åœ¨é›†æˆåˆ†ç±»çš„æˆå‘˜åˆ†ç±»å™¨è°ƒä¼˜ä¹‹å‰ï¼Œæˆ‘ä»¬è°ƒç”¨ä¸€ä¸‹ get_param æ–¹æ³•ï¼Œä»¥ä¾¿å¯¹å¦‚ä½•è®¿é—® GridSearch å¯¹è±¡å†…çš„å•ä¸ªå‚æ•°æœ‰ä¸ªåŸºæœ¬çš„è®¤è¯†:

```py
mv_clf.get_params()
```

è¾“å‡ºç»“æœ:

```output
{'decisiontreeclassifier': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,
             max_features=None, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=1, min_samples_split=2,
             min_weight_fraction_leaf=0.0, presort=False, random_state=0,
             splitter='best'),
 'decisiontreeclassifier_class_weight': None,
 'decisiontreeclassifier_criterion': 'entropy',
 'decisiontreeclassifier_max_depth': 1,
 'decisiontreeclassifier_max_features': None,
 'decisiontreeclassifier_max_leaf_nodes': None,
 'decisiontreeclassifier_min_impurity_decrease': 0.0,
 'decisiontreeclassifier_min_impurity_split': None,
 'decisiontreeclassifier_min_samples_leaf': 1,
 'decisiontreeclassifier_min_samples_split': 2,
 'decisiontreeclassifier_min_weight_fraction_leaf': 0.0,
 'decisiontreeclassifier_presort': False,
 'decisiontreeclassifier_random_state': 0,
 'decisiontreeclassifier_splitter': 'best',
 'pipeline-1': Pipeline(memory=None,
      steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ['clf', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,
           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
           penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
           verbose=0, warm_start=False)]]),
 'pipeline-1_clf': LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,
           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
           penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
           verbose=0, warm_start=False),
 'pipeline-1_clf__C': 0.001,
 'pipeline-1_clf__class_weight': None,
 'pipeline-1_clf__dual': False,
 'pipeline-1_clf__fit_intercept': True,
 'pipeline-1_clf__intercept_scaling': 1,
 'pipeline-1_clf__max_iter': 100,
 'pipeline-1_clf__multi_class': 'ovr',
 'pipeline-1_clf__n_jobs': 1,
 'pipeline-1_clf__penalty': 'l2',
 'pipeline-1_clf__random_state': 0,
 'pipeline-1_clf__solver': 'liblinear',
 'pipeline-1_clf__tol': 0.0001,
 'pipeline-1_clf__verbose': 0,
 'pipeline-1_clf__warm_start': False,
 'pipeline-1_memory': None,
 'pipeline-1_sc': StandardScaler(copy=True, with_mean=True, with_std=True),
 'pipeline-1_sc__copy': True,
 'pipeline-1_sc__with_mean': True,
 'pipeline-1_sc__with_std': True,
 'pipeline-1_steps': [('sc',
   StandardScaler(copy=True, with_mean=True, with_std=True)),
  ['clf',
   LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,
             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
             penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
             verbose=0, warm_start=False)]],
 'pipeline-2': Pipeline(memory=None,
      steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ['clf', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
            metric_params=None, n_jobs=1, n_neighbors=1, p=2,
            weights='uniform')]]),
 'pipeline-2_clf': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
            metric_params=None, n_jobs=1, n_neighbors=1, p=2,
            weights='uniform'),
 'pipeline-2_clf__algorithm': 'auto',
 'pipeline-2_clf__leaf_size': 30,
 'pipeline-2_clf__metric': 'minkowski',
 'pipeline-2_clf__metric_params': None,
 'pipeline-2_clf__n_jobs': 1,
 'pipeline-2_clf__n_neighbors': 1,
 'pipeline-2_clf__p': 2,
 'pipeline-2_clf__weights': 'uniform',
 'pipeline-2_memory': None,
 'pipeline-2_sc': StandardScaler(copy=True, with_mean=True, with_std=True),
 'pipeline-2_sc__copy': True,
 'pipeline-2_sc__with_mean': True,
 'pipeline-2_sc__with_std': True,
 'pipeline-2_steps': [('sc',
   StandardScaler(copy=True, with_mean=True, with_std=True)),
  ['clf',
   KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
              metric_params=None, n_jobs=1, n_neighbors=1, p=2,
              weights='uniform')]]}
```

å¾—åˆ° get_params æ–¹æ³•çš„è¿”å›å€¼åï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“æ€æ ·å»è®¿é—®æˆå‘˜åˆ†ç±»å™¨çš„å±æ€§äº†ã€‚å‡ºäºæ¼”ç¤ºçš„ç›®çš„ï¼Œå…ˆé€šè¿‡ç½‘æ ¼æœç´¢æ¥è°ƒæ•´ Logistic å›å½’åˆ†ç±»å™¨çš„æ­£åˆ™åŒ–ç³»æ•° C ä»¥åŠå†³ç­–æ ‘çš„æ·±åº¦ã€‚ä»£ç å¦‚ä¸‹:

```python
from sklearn.grid_search import GridSearchCV
params = {'decisiontreeclassifier__max_depth': [1, 2],
          'pipeline-1__clf__C': [0.001, 0.1, 100.0]}
grid = GridSearchCV(estimator=mv_clf, param_grid=params,
                    cv=10, scoring='roc_auc')
grid.fit(X_train, y_train)
```

è¾“å‡ºç»“æœ:

```output
GridSearchCV(cv=10, error_score='raise',
       estimator=MajorityVoteClassifier(classifiers=[Pipeline(memory=None,
     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ['clf', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', ra...ski',
           metric_params=None, n_jobs=1, n_neighbors=1, p=2,
           weights='uniform')]])],
            vote='classlabel', weights=None),
       fit_params={}, iid=True, n_jobs=1,
       param_grid={'decisiontreeclassifier__max_depth': [1, 2], 'pipeline-1__clf__C': [0.001, 0.1, 100.0]},
       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)
```

å®Œæˆç½‘æ ¼æœç´¢åï¼Œæˆ‘ä»¬å¯ä»¥è¾“å‡ºä¸åŒè¶…å‚å€¼çš„ç»„åˆï¼Œä»¥åŠ 10æŠ˜äº¤å‰éªŒè¯è®¡ç®—å¾—å‡ºçš„å¹³å‡ ROC AUCC å¾—åˆ†ã€‚ä»£ç å¦‚ä¸‹:

```python
for params, mean_score, scores in grid.grid_scores_:
    print("%0.3f +/- %0.2f %r" % (mean_score, scores.std()/2, params))

print('Best parameters: %s' % grid.best_params_)
print('Accuracy: %.2f' % grid.best_score_)
```

è¾“å‡ºç»“æœ:

```output
0.967 +/- 0.05 {'decisiontreeclassifier__max_depth': 1, 'pipeline-1__clf__C': 0.001}
0.967 +/- 0.05 {'decisiontreeclassifier__max_depth': 1, 'pipeline-1__clf__C': 0.1}
1.000 +/- 0.00 {'decisiontreeclassifier__max_depth': 1, 'pipeline-1__clf__C': 100.0}
0.967 +/- 0.05 {'decisiontreeclassifier__max_depth': 2, 'pipeline-1__clf__C': 0.001}
0.967 +/- 0.05 {'decisiontreeclassifier__max_depth': 2, 'pipeline-1__clf__C': 0.1}
1.000 +/- 0.00 {'decisiontreeclassifier__max_depth': 2, 'pipeline-1__clf__C': 100.0}

Best parameters: {'decisiontreeclassifier__max_depth': 1, 'pipeline-1__clf__C': 100.0}
Accuracy: 1.00
```

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå½“é€‰æ‹©çš„æ­£åˆ™åŒ–å¼ºåº¦è¾ƒå°æ—¶(C=100.0)ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°æœ€ä½³çš„äº¤å‰éªŒè¯ç»“æœï¼Œè€Œå†³ç­–æ ‘çš„æ·±åº¦ä¼¼ä¹å¯¹æ€§èƒ½æ²¡æœ‰ä»»ä½•å½±å“ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨å•å±‚å†³ç­–æ ‘è¶³ä»¥å¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†ã€‚è¯·æ³¨æ„ï¼Œåœ¨æ¨¡å‹è¯„ä¼°æ—¶ï¼Œä¸æ­¢ä¸€æ¬¡ä½¿ç”¨æµ‹è¯•é›†å¹¶éä¸€ä¸ªå¥½çš„åšæ³•ï¼Œæœ¬èŠ‚ä¸æ‰“ç®—è®¨è®ºè¶…å‚è°ƒä¼˜åçš„é›†æˆåˆ†ç±»å™¨æ³›åŒ–èƒ½åŠ›çš„è¯„ä¼°ã€‚æˆ‘ä»¬å°†åœ¨ç»§ç»­å­¦ä¹ å¦å¤–ä¸€ç§é›†æˆæ–¹æ³•: baggingã€‚

ğŸ”– æˆ‘ä»¬åœ¨æœ¬èŠ‚å®ç°çš„å¤šæ•°æŠ•ç¥¨æ–¹æ³•æœ‰æ—¶ä¹Ÿç§°ä¸ºå †å (stacking)ã€‚ä¸è¿‡ï¼Œå †å ç®—æ³•æ›´å…¸å‹åœ°åº”ç”¨äºç»„åˆ Logistic å›å½’æ¨¡å‹ï¼Œä»¥å„ç‹¬ç«‹åˆ†ç±»å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡é’ˆå¯¹è¿™äº›è¾“å…¥ç»“æœçš„ç»§æ‰¿æ¥é¢„æµ‹æœ€ç»ˆçš„ç±»æ ‡ï¼Œè¯¦æƒ…è¯·å‚é˜… David H.Wolpert çš„è®ºæ–‡: Stacked generalization.Neural networks,5(2):241â€”â€”259,1992ã€‚

# 7.4 baggingâ€”é€šè¿‡ bootstrap æ ·æœ¬æ„å»ºé›†æˆåˆ†ç±»å™¨

bagging æ˜¯ä¸€ç§ä¸ä¸Šä¸€èŠ‚å®ç°çš„ MajorityVoteClassifier å…³ç³»ç´§å¯†çš„é›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235845185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

ä¸è¿‡ï¼Œæ­¤ç®—æ³•æ²¡æœ‰ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒé›†æ‹Ÿåˆé›†æˆåˆ†ç±»å™¨ä¸­çš„å•ä¸ªæˆå‘˜åˆ†ç±»å™¨ã€‚ç”±äºåŸå§‹è®­ç»ƒé›†ä½¿ç”¨äº† bootstrap æŠ½æ ·(æœ‰æ”¾å›çš„éšæœºæŠ½æ ·)ï¼Œè¿™ä¹Ÿå°±æ˜¯ bagging è¢«ç§°ä¸º bootstrap aggregating çš„åŸå› ã€‚ä¸ºäº†ç”¨æ›´å…·ä½“çš„ä¾‹å­æ¥è§£é‡Š botstraping çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹å›¾æ‰€ç¤ºçš„ç¤ºä¾‹æ¥è¯´æ˜ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬ğŸˆ¶ 7ä¸ªä¸åŒçš„è®­ç»ƒæ ·ä¾‹(ä½¿ç”¨ç´¢å¼•1~7æ¥è¡¨ç¤º)ï¼Œåœ¨æ¯ä¸€è½®çš„ bagging å¾ªç¯ä¸­ï¼Œå®ƒä»¬éƒ½è¢«å¯æ”¾å›éšæœºæŠ½æ ·ã€‚æ¯ä¸ª bootstrap æŠ½æ ·éƒ½è¢«ç”¨äºåˆ†ç±»å™¨ $C_j$ çš„æ‚¬é“¾ï¼Œè¿™å°±æ˜¯ä¸€æ£µå…¸å‹çš„æœªå‰ªæçš„å†³ç­–æ ‘:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235906566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)


bagging è¿˜ä¸æˆ‘ä»¬åœ¨ç¬¬3ç« ä¸­æ›¾ç»ä»‹ç»è¿‡çš„éšæœºæ£®æ—ç´§å¯†ç›¸å…³ã€‚å®é™…ä¸Šï¼Œéšæœºæ£®æ—æ˜¯ bagging çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå®ƒä½¿ç”¨äº†éšæœºçš„ç‰¹å¾å­é›†å»æ‹Ÿåˆå•æ£µå†³ç­–ã€‚bagging æœ€æ—©ç”± Leo Breiman åœ¨1994å¹´çš„ä¸€ä»½æŠ€æœ¯æŠ¥å‘Šä¸­æå‡ºã€‚ä»–è¿˜è¡¨ç¤ºï¼Œbagging å¯ä»¥æé«˜ä¸ç¨³å®šçš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”å¯ä»¥é™ä½è¿‡æ‹Ÿåˆçš„ç¨‹åº¦ã€‚æˆ‘å¼ºçƒˆå»ºè®®è¯»è€…é˜…è¯» L.Breiman çš„è®ºæ–‡$1$ï¼Œä»¥æ›´æ·±å…¥äº†è§£ baggingã€‚æ­¤æ–‡çŒ®å¯åœ¨ç½‘ä¸Šå…è´¹è·å–ã€‚

> æ³¨1: L.Breoman.Bagging Predictors. Machine Learning, 24(2):123-140,1996.

ä¸ºäº†æ£€éªŒä¸€ä¸‹ bagging çš„å®é™…æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬4ç« ä¸­ç”¨åˆ°çš„è‘¡è„é…’æ•°æ®é›†æ„å»ºä¸€ä¸ªæ›´å¤æ‚çš„åˆ†ç±»é—®é¢˜ã€‚åœ¨æ­¤æˆ‘ä»¬åªè€ƒè™‘è‘¡è„é…’ä¸­çš„ç±»åˆ«2å’Œç±»åˆ«3ï¼Œä¸”åªé€‰æ‹© Alchohol å’Œ Hue è¿™ä¸¤ä¸ªç‰¹å¾ã€‚

```python
import pandas as pd

data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'
df_wine = pd.read_csv(data_url, header=None)
df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 
        'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids',
        'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity',
        'Hue', 'OD280/OD315 of diluted wines', 'Proline']
df_wine = df_wine[df_wine['Class label'] != 1]
y = df_wine['Class label'].values
x = df_wine[['Alcohol', 'Hue']].values
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç±»æ ‡ç¼–ç ä¸ºäºŒè¿›åˆ¶å½¢å¼ï¼Œå¹¶å°†æ•°æ®é›†æŒ‰ç…§å…­å››åˆ†çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†:

```python
from sklearn.preprocessing import LabelEncoder
from sklearn.cross_validation import train_test_split

le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.4, random_state=1)
```

scikit-learn ä¸­å·²ç»å®ç°äº† BaggingClassifier ç›¸å…³ç®—æ³•ï¼Œæˆ‘ä»¬å¯ä» ensemble å­æ¨¡å—ä¸­å¯¼å…¥ä½¿ç”¨ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å°¾å‰ªæçš„å†³ç­–æ ‘ä½œä¸ºæˆå‘˜åˆ†ç±»å™¨ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šé€šè¿‡ä¸åŒçš„ bootstrap æŠ½æ ·æ‹Ÿåˆ 500 æ£µå†³ç­–æ ‘:

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(criterion='entropy', max_depth=None)
bag = BaggingClassifier(base_estimator=tree, n_estimators=500,
        max_samples=1.0, max_features=1.0, bootstrap=True,
        bootstrap_features=False, n_jobs=1, random_state=1)
```

ç„¶åæˆ‘ä»¬å°†è®¡ç®—è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡ï¼Œä»¥æ¯”è¾ƒ bagging åˆ†ç±»å™¨ä¸å•æ£µæœªå‰ªæå†³ç­–æ ‘çš„æ€§èƒ½å·®å¼‚:

```python
from sklearn.metrics import accuracy_score

tree = tree.fit(X_train, y_train)
y_train_std = tree.predict(X_train)
y_test_std = tree.predict(X_test)
tree_train = accuracy_score(y_train, y_train_std)
tree_test = accuracy_score(y_test, y_test_std)
print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test))
```

è¾“å‡ºç»“æœ:

```output
Decision tree train/test accuracies 1.000/0.833
```

åŸºäºä¸Šè¿°ä»£ç æ‰§è¡Œçš„ç»“æœå¯è§ï¼Œæœªç»å‰ªæçš„å†³ç­–æ ‘å‡†ç¡®åœ°é¢„æµ‹äº†è®­ç»ƒæ•°æ®çš„æ‰€æœ‰åˆ—ç±»æ ‡ï¼›ä½†æ˜¯ï¼Œæµ‹è¯•æ•°æ®ä¸Šæä½çš„å‡†ç¡®ç‡è¡¨æ˜è¯¥æ¨¡å‹æ–¹å·®è¿‡é«˜(è¿‡æ‹Ÿåˆ):

```python
bag = bag.fit(X_train, y_train)
y_train_pred = bag.predict(X_train)
y_test_pred = bag.predict(X_test)
bag_train = accuracy_score(y_train, y_train_pred)
bag_test = accuracy_score(y_test, y_test_pred)
print('Bagging train/test accuracies %.3f/%.3f' % (bag_train, bag_test))
```

è¾“å‡ºç»“æœ:

```output
Bagging train/test accuracies 1.000/0.896
```

è™½ç„¶å†³ç­–æ ‘ä¸ bagging åˆ†ç±»å™¨åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ç›¸ä¼¼(å‡ä¸º 1.0)ï¼Œä½†æ˜¯ bagging åˆ†ç±»å™¨åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ³›åŒ–æ€§èƒ½ç¨æœ‰èƒœå‡ºã€‚ä¸‹é¢æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹å†³ç­–æ ‘ä¸ bagging åˆ†ç±»å™¨çš„å†³ç­–åŒºåŸŸ:

```python
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from itertools import product
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

tree = DecisionTreeClassifier(criterion='entropy', max_depth=None)
bag = BaggingClassifier(base_estimator=tree, n_estimators=500,
        max_samples=1.0, max_features=1.0, bootstrap=True,
        bootstrap_features=False, n_jobs=1, random_state=1)

data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'
df_wine = pd.read_csv(data_url, header=None)
df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 
        'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids',
        'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity',
        'Hue', 'OD280/OD315 of diluted wines', 'Proline']
df_wine = df_wine[df_wine['Class label'] != 1]
y = df_wine['Class label'].values
X = df_wine[['Alcohol', 'Hue']].values
le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.4, random_state=1)

x_min = X_train[:, 0].min() - 1
x_max = X_train[:, 0].max() + 1
y_min = X_train[:, 1].min() - 1
y_max = X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))
f, axarr = plt.subplots(nrows=1, ncols=2, 
                        sharex='col', sharey='row',
                        figsize=(8,3))

for idx, clf, tt in zip([0, 1], [tree, bag], ['Decision Tree', 'Bagging']):
    clf.fit(X_train, y_train)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    axarr[idx].contourf(xx, yy, Z, alpha=0.3)
    axarr[idx].scatter(X_train[y_train==0, 0],
            X_train[y_train==0, 1], c='b', marker='^')
    axarr[idx].scatter(X_train[y_train==1, 0],
            X_train[y_train==1, 1], c='r', marker='o')
    axarr[idx].set_title(tt)

axarr[0].set_ylabel('Alcohol', fontsize=12)
plt.text(10.2, -1.2, s='Hue', ha='center', va='center', fontsize=12)
plt.show()
```



ç”±ç»“æœå›¾åƒå¯è§ï¼Œä¸æ·±åº¦ä¸º 3çš„å†³ç­–æ ‘çº¿æ€§åˆ†æ®µè¾¹ç•Œç›¸æ¯”ï¼Œbagging é›†æˆåˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œæ˜¾å¾—æ›´å¹³æ»‘:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235930575.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

æœ¬èŠ‚æˆ‘ä»¬åªæ˜¯ç®€å•é€šè¿‡ç¤ºä¾‹äº†è§£äº† baggingã€‚åœ¨å®æˆ˜ä¸­ï¼Œåˆ†ç±»ä»»åŠ¡ä¼šæ›´åŠ å¤æ‚ï¼Œæ•°æ®é›†ç»´åº¦ä¼šæ›´é«˜ï¼Œä½¿ç”¨å•æ£µæ ‘å¾ˆå®¹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆï¼Œè¿™æ—¶ bagging ç®—æ³•å°±å¯æ˜¾ç¤ºå‡ºå…¶ä¼˜åŠ¿äº†ã€‚æœ€åï¼Œæˆ‘ä»¬éœ€è¦æ³¨æ„ bagging ç®—æ³•æ˜¯é™ä½æ¨¡å‹æ–¹å·®çš„ä¸€ç§æœ‰æ•ˆé˜²èŒƒã€‚ç„¶è€Œï¼Œbagging åœ¨é™ä½æ¨¡å‹åå·®æ–¹é¢ä½œç”¨ä¸å¤§ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬é€‰æ‹©æœªå‰ªæå†³ç­–æ ‘ç­‰ä½åå·®åˆ†ç±»å™¨ä½œä¸ºé›†æˆç®—æ³•æˆå‘˜åˆ†ç±»å™¨çš„åŸå› ã€‚

# 7.5 é€šè¿‡è‡ªé€‚åº” boosting æé«˜å¼±å­¦ä¹ æœºçš„æ€§èƒ½

åœ¨æœ¬èŠ‚å¯¹é›†æˆæ–¹æ³•çš„ä»‹ç»ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è®¨è®º boosting ç®—æ³•ä¸­ä¸€ä¸ªå¸¸ç”¨ä¾‹å­:

ğŸ”– AdaBoost èƒŒåçš„æœ€åˆæƒ³æ³•æ˜¯ç”± Robert Schapire äº 1990å¹´æå‡ºçš„(R.E.Schapire.The Strength of Weak Learnability. Machine Learning, 5(2):197-227, 1990)ã€‚å½“ Robert Schapire ä¸ Yoav Freund åœ¨ç¬¬13å±Šå›½é™…ä¼šè®®(ICML 1996)ä¸Šå‘è¡¨ AdaBoost ç®—æ³•åï¼Œéšåçš„å‡ å¹´ä¸­ï¼Œæ­¤ç®—æ³•å°±æˆä¸ºæœ€å¹¿ä¸ºåº”ç”¨çš„ç»“åˆé›†æˆæ–¹æ³•(Y.Freund, R.E.Schapire, et al.Experiments with a New Boosting Algorithm. In ICML, volume 96, pages 148-156,1996)ã€‚åŸºäºä»–ä»¬çš„å¼€æ”¾æ€§å·¥ä½œï¼ŒFreund å’Œ Schapire åœ¨2003å¹´è·å¾—äº† Goedel å¥–ï¼Œè¿™æ˜¯å‘è¡¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸè®ºæ–‡æœ€è´Ÿç››åçš„å¥–é¡¹ã€‚

åœ¨ boosting ä¸­ï¼Œé›†æˆåˆ†ç±»å™¨åŒ…å«å¤šä¸ªéå¸¸ç®€å•çš„æˆå‘˜åˆ†ç±»å™¨ï¼Œè¿™äº›æˆå‘˜åˆ†ç±»å™¨ä»…ç¨å¥½äºéšæœºçŒœæµ‹ï¼Œå¸¸è¢«ç§°ä½œå¼±å­¦ä¹ æœºã€‚å…¸å‹çš„å¼±å­¦ä¹ æœºä¾‹å­å°±æ˜¯å•å±‚å†³ç­–æ ‘ã€‚boosting ä¸»è¦é’ˆå¯¹éš¾ä»¥åŒºåˆ†çš„è®­ç»ƒæ ·æœ¬ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¼±å­¦ä¹ æœºé€šè¿‡åœ¨é”™è¯¯åˆ†ç±»æ ·æœ¬ä¸Šçš„å­¦ä¹ æ¥æé«˜é›†æˆåˆ†ç±»çš„æ€§èƒ½ã€‚ä¸ bagging ä¸åŒï¼Œåœ¨ boosting çš„åˆå§‹åŒ–é˜¶æ®µï¼Œç®—æ³•ä½¿ç”¨æ— æ”¾å›æŠ½æ ·ä»è®­ç»ƒæ ·æœ¬ä¸­éšæœºæŠ½å–ä¸€ä¸ªå­é›†ã€‚åŸå§‹çš„ boosting è¿‡ç¨‹å¯æ€»ç»“ä¸ºå¦‚ä¸‹å››ä¸ªæ­¥éª¤:

1. ä»è®­ç»ƒè¡— $D$ ä¸­ä»¥æ— æ”¾å›æŠ½æ ·æ–¹å¼éšæœºæŠ½å–ä¸€ä¸ªè®­ç»ƒå­é›† $d_1$ï¼Œç”¨äºå¼±å­¦ä¹ æœº $C_1$ çš„è®­ç»ƒã€‚
2. ä»è®­ç»ƒé›† $D$ ä¸­æ— æ”¾å›æŠ½æ ·æ–¹å¼éšæœºæŠ½å–ç¬¬2ä¸ªè®­ç»ƒå­é›† $d2$ï¼Œå¹¶å°† $C_1$ $C_1$ ä¸­è¯¯åˆ†ç±»æ ·æœ¬çš„ 50% åŠ å…¥åˆ°è®­ç»ƒé›†ä¸­ï¼Œè®­ç»ƒå¾—åˆ°å¼±å­¦ä¹ æœº $C_2$ã€‚
3. ä»è®­ç»ƒé›† $D$ ä¸­æŠ½å– $C_1$ å’Œ $C_2$åˆ†ç±»ç»“æœä¸ä¸€è‡´çš„æ ·æœ¬ç”Ÿæˆè®­ç»ƒæ ·æœ¬é›† $d_3$ï¼Œä»¥æ­¤è®­ç»ƒç¬¬ 3ä¸ªå¼±å­¦ä¹ æœº $C_3$ã€‚
4. é€šè¿‡å¤šæ•°æŠ•ç¥¨ç»„åˆä¸‰ä¸ªå¼±å­¦ä¹ æœº $C_1$ã€$C_2$ å’Œ $C_3$ã€‚

æ­£å¦‚ Leo Breiman æ‰€è¿°$1$ï¼Œä¸ bagging æ¨¡å‹ç›¸æ¯”ï¼Œboosting å¯åŒæ—¶é™ä½åå·®å’Œæ–¹å·®ã€‚åœ¨å®è·µä¸­ï¼Œboosting ç®—æ³•(å¦‚ AdaBoost) ä¹Ÿå­˜åœ¨æ˜æ˜¾çš„é«˜æ–¹å·®é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹è®­ç»ƒæ•°æ®æœ‰è¿‡æ‹Ÿåˆå€¾å‘$2$ã€‚

> æ³¨1: L.Breiman. Bias, Variance, and Arcing Classifiers. 1996.
> æ³¨2: G.Raetsch, T.Onoda, and K.R.Mueller. An Improvement of Adaboost Aboid Overfitting. In Proc. of the Int. Conf. on Neural Inforamtion Processing. Citeseer, 1998.

AdaBoost ä¸è¿™é‡Œè®¨è®ºçš„åŸå§‹ boosting è¿‡ç¨‹ä¸åŒï¼Œå®ƒä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†æ¥è®­ç»ƒå¼±å­¦ä¹ æœºï¼Œå…¶ä¸­è®­ç»ƒæ ·æœ¬åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šé‡æ–°è¢«èµ‹äºˆä¸€ä¸ªæƒé‡ï¼Œåœ¨ä¸Šä¸€å¼±å­¦ä¹ æœºé”™è¯¯åœ°åŸºç¡€ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œè¿›è€Œæ„å»ºä¸€ä¸ªæ›´å¼ºå¤§çš„åˆ†ç±»å™¨ã€‚åœ¨æ·±å…¥åˆ° AdaBoost ç®—æ³•å…·ä½“ç»†èŠ‚ä¹‹å‰ï¼Œå…ˆé€šè¿‡ä¸‹å›¾æ›´æ·±å…¥äº†è§£ä¸€ä¸‹ AdaBoost çš„åŸºæœ¬æ¦‚å¿µ:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190830235950580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

ä¸ºå¾ªåºæ¸è¿›åœ°ä»‹ç» AdaBoostï¼Œæˆ‘ä»¬ä»å­å›¾1å¼€å§‹ï¼Œå®ƒä»£è¡¨äº†ä¸€ä¸ªäºŒç±»åˆ«åˆ†ç±»é—®é¢˜çš„è®­ç»ƒé›†ï¼Œå…¶ä¸­æ‰€æœ‰çš„æ ·æœ¬éƒ½è¢«èµ‹äºˆç›¸åŒçš„æƒé‡ã€‚åŸºäºæ­¤è®­ç»ƒé›†ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€æ£µå•å±‚å†³ç­–æ ‘(ä»¥è™šçº¿è¡¨ç¤º)ï¼Œå®ƒå°è¯•å°½å¯èƒ½é€šè¿‡æœ€å°åŒ–ä»£ä»·å‡½æ•°(æˆ–è€…æ˜¯ç‰¹æ®Šæƒ…å†µä¸‹å†³ç­–æ ‘é›†æˆä¸­çš„ä¸çº¯åº¦å¾—åˆ†)åˆ’åˆ†ä¸¤ç±»æ ·æœ¬(ä¸‰è§’å½¢å’Œåœ†å½¢)ã€‚åœ¨ä¸‹ä¸€è½®(å­å›¾2)ä¸­ï¼Œæˆ‘ä»¬ä¸ºå‰é¢è¯¯åˆ†ç±»çš„æ ·æœ¬(åœ†å½¢)èµ‹äºˆæ›´é«˜çš„æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é™ä½è¢«æ­£ç¡®åˆ†ç±»æ ·æœ¬çš„æƒé‡ã€‚ä¸‹ä¸€æ£µå•å±‚å†³ç­–æ ‘å°†æ›´åŠ ä¸“æ³¨äºå…·æœ‰æœ€å¤§æƒé‡çš„è®­ç»ƒæ ·æœ¬ï¼Œä¹Ÿå°±æ˜¯é‚£äº›éš¾äºåŒºåˆ†çš„æ ·æœ¬ã€‚å¦‚å­å›¾2æ‰€ç¤ºï¼Œå¼±å­¦ä¹ æœºé”™è¯¯åˆ’åˆ†äº†åœ†å½¢ç±»çš„ä¸‰ä¸ªæ ·æœ¬ï¼Œå®ƒä»¬åœ¨å­å›¾3ä¸­è¢«èµ‹äºˆæ›´å¤§çš„æƒé‡ã€‚å‡å®š AdaBoost é›†æˆåªåŒ…å« 3è½® boosting è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿç”¨åŠ æƒå¤šæ•°æŠ•ç¥¨å°†ä¸åŒé‡é‡‡æ ·è®­ç»ƒå­é›†ä¸Šå¾—åˆ°çš„ä¸‰ä¸ªå¼±å­¦ä¹ æœºè¿›è¡Œç»„åˆï¼Œå¦‚å­å›¾4æ‰€ç¤ºã€‚

ç°åœ¨æˆ‘ä»¬é˜Ÿ AdaBoost çš„åŸºæœ¬æ¦‚å¿µæœ‰äº†æ›´å¥½çš„è®¤è¯†ï¼Œä¸‹é¢é€šè¿‡ä¼ªä»£ç æ›´æ·±å…¥åœ°å­¦ä¹ è¯¥ç®—æ³•ã€‚ä¸ºäº†ä¾¿äºé˜è¿°ï¼Œæˆ‘ä»¬ç”¨åå­—ç¬¦å·($\times$)æ¥è¡¨ç¤ºå‘é‡çš„å…ƒç´ ç›¸ä¹˜ï¼Œç”¨ç‚¹å·($\cdot$)è¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„å†…ç§¯ï¼Œç®—æ³•æ­¥éª¤å¦‚ä¸‹:

1. ä»¥ç­‰å€¼æ–¹å¼ä¸ºæƒé‡å‘é‡ $w$ èµ‹å€¼ï¼Œå…¶ä¸­ $\sum_i w_i=1$ã€‚
2. åœ¨ $m$ è½® boosting æ“ä½œä¸­ï¼Œå¯¹ç¬¬ $j$ è½®åšå¦‚ä¸‹æ“ä½œ:
3. è®­ç»ƒä¸€ä¸ªåŠ æƒçš„å¼±å­¦ä¹ æœº: $C_j=train(X, y, w)$ã€‚
4. é¢„æµ‹æ ·æœ¬ç±»æ ‡ $\hat y = predict(C_j, X)$ã€‚
5. è®¡ç®—æƒé‡é”™è¯¯ç‡ $ \varepsilon = w \cdot (\hat y == y)$ã€‚
6. è®¡ç®—ç›¸å…³ç³»æ•°: $ \alpha_j =0.5 \log \frac {1-\varepsilon}{\varepsilon}$ã€‚
7. æ›´æ–°æƒé‡: $w:=w \times \exp(-\alpha_j \times \hat y \times y)$ã€‚
8. å½’ä¸€åŒ–æƒé‡ï¼Œä½¿å…¶å’Œä¸º 1: $w:w/ \sum_i w_i$ã€‚
9. å®Œæˆæœ€ç»ˆçš„é¢„æµ‹: $\hat y = \large( \sum_{j=1}^ m (\alpha_j \times p redict(C_j, X) > 0) \large)$ã€‚

è¯·æ³¨æ„ï¼Œä½5æ­¥ä¸­è¡¨è¾¾å¼ ($\hat y == y$) çš„å€¼ä¸º 1 æˆ– 0ï¼Œå…¶ä¸­ 1è¡¨ç¤ºå¯¹åº”çš„é¢„æµ‹ç»“æœæ­£ç¡®ï¼Œ0 è¡¨ç¤ºé”™è¯¯ã€‚

è™½ç„¶ AdaBoost ç®—æ³•çœ‹ä¸Šå»å¾ˆç®€å•ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥ç”¨ä¸‹é¢è¡¨æ ¼ä¸­çš„ 10 ä¸ªæ ·æœ¬ä½œä¸ºè®¯è½®æœºï¼Œé€šè¿‡ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ›´æ·±å…¥åœ°äº†è§£æ­¤ç®—æ³•:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190831000023348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

è¡¨æ ¼çš„ç¬¬ 1åˆ—ä¸ºæ ·æœ¬çš„ç´¢å¼•åºå·ï¼Œä¸º 1~10ã€‚å‡è®¾æ­¤è¡¨æ ¼ä»£è¡¨äº†ä¸€ä¸ªä¸€ç»´çš„æ•°æ®é›†ï¼Œç¬¬ 2åˆ—å°±ç›¸å½“äºå•ä¸ªæ ·æœ¬çš„å€¼ã€‚ç¬¬ 3åˆ—æ˜¯å¯¹åº”äºè®­ç»ƒæ ·æœ¬ $x_i$ çš„çœŸå®ç±»æ ‡ $y_i$ï¼Œå…¶ä¸­ $y_i \in \{1, -1\}$ã€‚ç¬¬ 4åˆ—ä¸ºæ ·æœ¬çš„åˆå§‹æƒé‡ï¼Œæƒé‡å€¼ç›¸ç­‰ï¼Œä¸”é€šè¿‡å½’ä¸€åŒ–ä½¿å…¶å’Œä¸º 1ã€‚åœ¨è®­ç»ƒæ ·æœ¬æ•°é‡ä¸º 10 çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æƒé‡ $w$ ä¸­çš„æƒé‡å‘é‡ $w$ ä¸­çš„æƒå€¼ $w_i$ éƒ½è®¾å®šä¸º 0.1ã€‚å‡å®šæˆ‘ä»¬çš„åˆ’åˆ†æ ‡å‡†ä¸º $x \leq 0.3$ï¼Œç¬¬ 5åˆ—å­˜å‚¨äº†é¢„æµ‹ç±»æ ‡ $\hat y$ã€‚åŸºäºå‰é¢çš„ä¼ªä»£ç ç»™å‡ºçš„æƒé‡æ›´æ–°è§„åˆ™ï¼Œæœ€åä¸€åˆ—å­˜å‚¨äº†æ›´æ–°åçš„æƒé‡å€¼ã€‚

å’‹ä¸€çœ‹ï¼Œæƒé‡æ›´æ–°çš„è®¡ç®—ä¼¼ä¹æœ‰äº›å¤æ‚ï¼Œæˆ‘ä»¬æ¥é€æ­¥è®²è§£è®¡ç®—è¿‡ç¨‹ã€‚é¦–å…ˆè®¡ç®—ç¬¬ 5æ­¥ä¸­æƒé‡çš„é”™è¯¯ç‡:

$$
\begin{aligned}
\varepsilon & = 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0 + 0.1 \times 0   \\
& = \frac {3}{10}
\end{aligned}
$$

ä¸‹é¢æ¥è®¡ç®—ç›¸å…³ç³»æ•° $\alpha_j$(ç¬¬6æ­¥)ï¼Œè¯¥ç³»æ•°å°†åœ¨ç¬¬ 7æ­¥æƒé‡æ›´æ–°åŠæœ€åä¸€ä¸ªæ­¥éª¤å¤šæ•°æŠ•ç¥¨é¢„æµ‹ä¸­ä½œä¸ºæƒé‡æ¥ä½¿ç”¨ã€‚

$$
\alpha = \frac {0.5 \log (1-\varepsilon)}{\varepsilon} \thickapprox 0.424
$$

åœ¨è®¡ç®—å¾—åˆ°ç›¸å…³ç³»æ•° $\alpha_j$ åï¼Œæˆ‘ä»¬å¯æ ¹æ®ä¸‹è¿°å…¬å¼æ›´æ–°æƒé‡å‘é‡:

$$
w: = w \times \exp(- \alpha_j \times \hat y \times y)
$$

å…¶ä¸­ï¼Œ$\hat y \times y$ ä¸ºé¢„æµ‹äº†ç±»æ ‡å‘é‡å’ŒçœŸå®ç±»æ ‡å‘é‡é€å…ƒç´ ç›¸ä¹˜ã€‚ç”±æ­¤ï¼Œå¦‚æœæŸä¸ªé¢„æµ‹å€¼ $\hat y_i$ æ­£ç¡®ï¼Œåˆ™ $\hat y \times y$ çš„ç¬¦å·ä¸ºæ­£ï¼Œè€Œä¸” $\alpha_i$ æœ¬ç¥å€¼ä¸ºæ­£ï¼Œåˆ™ç¬¬ $i$ ä¸ªæƒé‡ä¼šè¢«é™ä½:

$$
0.1 \times \exp(-0.424 \times 1 \times 1) \approx 0.066
$$

ç±»ä¼¼åœ°ï¼Œå¦‚æœ $\hat y$ é¢„æµ‹çš„ç±»æ ‡é”™è¯¯ï¼Œåˆ™ç¬¬ $i$ ä¸ªæƒé‡å°†æŒ‰ç…§å¦‚ä¸‹æ–¹å¼æ›´æ–°:

$$
0.1 \times \exp(-0.424 \times 1 \times (-1)) \approx 0.153
$$

æˆ–è€…å‘è¿™æ ·:

$$
0.1 \times \exp(-0.424 \times (-1) \times (1)) \approx 0.153
$$

åœ¨å®Œæˆæ‰€æœ‰æƒé‡å‘é‡å€¼çš„æ›´æ–°åï¼Œæˆ‘ä»¬é€šè¿‡å½’ä¸€åŒ–ä½¿æ‰€æœ‰æƒé‡çš„å’Œä¸º 1(ç¬¬8ä¸ªæ­¥éª¤):

$$
w: = \frac {w} {\sum_i w_i}
$$

å…¶ä¸­, $\sum_i w_i = 7 \times 0.065 + 3 \times 0.153 = 0.914 $ã€‚

ç”±æ­¤ï¼Œå¯¹äºåˆ†ç±»æ­£ç¡®çš„æ ·æœ¬ï¼Œå…¶å¯¹åº”çš„æƒé‡åœ¨ä¸‹ä¸€è½® boosting ä¸­å°†ä»åˆå§‹çš„ 0.1 é™ä¸º $ 0.066/0/914 \approx 0.072$ã€‚åŒæ ·ï¼Œå¯¹äºåˆ†ç±»é”™è¯¯çš„æ ·æœ¬ï¼Œå…¶å¯¹åº”æƒé‡å°†ä» 0.1 æé«˜åˆ° $0.153/0.914 \approx 0.167$ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œè¿™å°±æ˜¯ AdaBoostã€‚ä¸‹é¢è¿›å…¥å®è·µæ“ä½œï¼Œé€šè¿‡ scikit-learn è®­ç»ƒä¸€ä¸ª AdaBoost é›†æˆåˆ†ç±»å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šä¸€èŠ‚è®­ç»ƒ bagging å…ƒåˆ†ç±»å™¨çš„è‘¡è„é…’æ•°æ®é›†ã€‚é€šè¿‡ base_estimator å±æ€§ï¼Œæˆ‘ä»¬åœ¨ 500 æ£µå•å±‚å†³ç­–æ ‘ä¸Šè®­ç»ƒ AdaBoostClassifier ã€‚

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import pandas as pd

data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'
df_wine = pd.read_csv(data_url, header=None)
df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 
        'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids',
        'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity',
        'Hue', 'OD280/OD315 of diluted wines', 'Proline']
df_wine = df_wine[df_wine['Class label'] != 1]
y = df_wine['Class label'].values
X = df_wine[['Alcohol', 'Hue']].values
le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.4, random_state=1)

tree = DecisionTreeClassifier(criterion='entropy', max_depth=1)
ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500,
                         learning_rate=0.1, random_state=0)
tree = tree.fit(X_train, y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)
tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)
print('Decision tree train/test accuracies %.3f/%.3f' 
      %(tree_train, tree_test))
```

è¾“å‡ºç»“æœ:

```output
Decision tree train/test accuracies 0.845/0.854
```

ç”±ç»“æœå¯è§ï¼Œä¸ä¸Šä¸€èŠ‚ä¸­ä¸ºå‰ªæå†³ç­–æ ‘ç›¸æ¯”ï¼Œå•å±‚å†³ç­–æ ‘å¯¹äºè®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆçš„ç¨‹åº¦æ›´åŠ ä¸¥é‡ä¸€ç‚¹:

```python
ada = ada.fit(X_train, y_train)
y_train_pred = ada.predict(X_train)
y_test_pred = ada.predict(X_test)
ada_train = accuracy_score(y_train, y_train_pred)
ada_test = accuracy_score(y_test, y_test_pred)
print('AdaBoost tree train/test accuracies %.3f/%.3f' 
      %(ada_train, ada_test))
```

è¿˜å¯ä»¥çœ‹åˆ°ï¼ŒAdaBoost æ¨¡å‹å‡†ç¡®é¢„æµ‹äº†æ‰€æœ‰çš„è®­ç»ƒé›†ç±»æ ‡ï¼Œä¸å•å±‚å†³ç­–æ ‘ç›¸æ¯”ï¼Œå®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ç¨å¥½ã€‚ä¸è¿‡ï¼Œåœ¨ä»£ç ä¸­ä¹Ÿå¯çœ‹åˆ°ï¼Œæˆ‘ä»¬åœ¨é™ä½æ¨¡å‹åå·®çš„åŒæ—¶ï¼Œä½¿å¾—æ–¹å·®æœ‰äº†é¢å¤–çš„å¢åŠ ã€‚

å¤„äºæ¼”ç¤ºçš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¦å¤–ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½†å¯ä»¥å‘ç°ï¼Œä¸å•å±‚å†³ç­–æ ‘ç›¸æ¯”ï¼ŒAdaBoost  åˆ†ç±»å™¨èƒ½å¤Ÿäº›è®¸æé«˜åˆ†ç±»æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ä¸Šä¸€èŠ‚ä¸­è®­ç»ƒçš„ bagging åˆ†ç±»å…¶çš„å‡†ç¡®ç‡æ¥è¿‘ã€‚ä¸è¿‡è¯·æ³¨æ„: é‡å¤ä½¿ç”¨æµ‹è¯•è¡—è¿›è¡Œæ¨¡å‹é€‰æ‹©ä¸æ˜¯ä¸€ä¸ªå¥½çš„æ–¹æ³•ã€‚é›†æˆåˆ†ç±»å™¨çš„æ³›åŒ–æ€§èƒ½å¯èƒ½ä¼šè¢«è¿‡æ¸¡ä¼˜åŒ–ï¼Œå¯¹æ­¤æˆ‘ä»¬åœ¨ç¬¬6ç« ä¸­å·²ç»åšè¿‡è¯¦ç»†ä»‹ç»ã€‚

æœ€åï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å†³ç­–åŒºåŸŸçš„å½¢çŠ¶:

```python
x_min = X_train[:, 0].min() - 1
x_max = X_train[:, 0].max() + 1
y_min = X_train[:, 1].min() - 1
y_max = X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))
f, axarr = plt.subplots(nrows=1, ncols=2, 
                        sharex='col', sharey='row',
                        figsize=(8,3))

for idx, clf, tt in zip([0, 1], [tree, ada], 
                        ['Decision Tree', 'AdaBoost']):
    clf.fit(X_train, y_train)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    axarr[idx].contourf(xx, yy, Z, alpha=0.3)
    axarr[idx].scatter(X_train[y_train==0, 0],
            X_train[y_train==0, 1], c='b', marker='^')
    axarr[idx].scatter(X_train[y_train==1, 0],
            X_train[y_train==1, 1], c='r', marker='o')
    axarr[idx].set_title(tt)

axarr[0].set_ylabel('Alcohol', fontsize=12)
plt.text(10.2, -1.2, s='Hue', ha='center', va='center', fontsize=12)
plt.show()
```



é€šè¿‡è§‚å¯Ÿå†³ç­–åŒºåŸŸï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° AdaBoost çš„å†³ç­–åŒºåŸŸæ¯”å•å±‚å†³ç­–æ ‘çš„å†³ç­–åŒºåŸŸå¤æ‚å¾—å¤šã€‚æ­¤å¤–ï¼Œè¿˜æ³¨æ„åˆ° AdaBoost å¯¹å¾…ç‰¹å¾ç©ºé—´çš„åˆ’åˆ†ä¸ä¸Šä¸€èŠ‚ä¸­è®­ç»ƒçš„ bagging åˆ†ç±»å™¨ååˆ†ç±»ä¼¼ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2019083100005055.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70)

å¯¹é›†æˆæŠ€æœ¯åšä¸€ä¸ªæ€»ç»“: æ¯‹åº¸ç½®ç–‘ï¼Œä¸å•å±‚åˆ†ç±»å™¨ç›¸æ¯”ï¼Œé›†æˆå­¦ä¹ æé«˜äº†è®¡ç®—å¤æ‚åº¦ã€‚ä½†åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬éœ€ä»”ç»†è¡¡é‡æ˜¯å¦æ„¿æ„ä¸ºé€‚åº¦é¢„æµ‹æ€§ä½ é‚£ä¸ªè€Œä»˜å‡ºæ›´å¤šçš„è®¡ç®—æˆæœ¬ã€‚

å…³äºé¢„æµ‹æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¸¤è€…ä¹‹é—´çš„æƒé‡é—®é¢˜ï¼Œä¸€ä¸ªå¸¸è¢«æåŠçš„ä¾‹å­å°±æ˜¯è‘—åçš„ä¸€ç™¾ä¸‡ç¾å…ƒçš„ Netflix ç«èµ›(`$1 Million Netfix Prize`)ï¼Œæœ€ç»ˆèƒœå‡ºè€…å°±ä½¿ç”¨äº†é›†æˆæŠ€æœ¯ã€‚æ­¤ç®—æ³•çš„è¯¦ç»†å†…å®¹è¯·å‚è§ A.Toescher ç­‰äººçš„è®ºæ–‡$3$ã€‚è™½ç„¶è·èƒœé˜Ÿä¼å¾—åˆ°äº†ä¸€ç™¾ä¸‡ç¾å…ƒçš„å¥–åŠ±ï¼Œä½†ç”±äºæ¨¡å‹é«˜é«˜å¤æ‚æ€§ï¼ŒNetflix å…¬å¸ä¸€ç›´æœªå°†è¯¥æ¨¡å‹æŠ•è¯¸å®é™…åº”ç”¨ä¸­$4$:

> æ³¨3: A.Toescher, M.Jahrer, and R.M.Bell. The Bigchaos Solution to the Netfix Grand Prize. Netflix prize documentation, 2009. (è¯»è€…å¯é€šè¿‡ http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf è·å–)ã€‚
> æ³¨4: è¯·å‚è§ http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html ã€‚


# 7.6 æœ¬ç« å°ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é›†æˆé¢†åŸŸä¸­æœ€çƒ­é—¨ä¸”åº”ç”¨æœ€å¹¿æ³›çš„æŠ€æœ¯ã€‚é›†æˆæ–¹æ³•é€šè¿‡ç»„åˆä¸åŒçš„åˆ†ç±»å™¨æ¨¡å‹æ¥æŠµæ¶ˆåˆ†ç±»å™¨å›ºæœ‰çš„ç¼ºé™·ï¼Œé€šå¸¸èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªç¨³å®šä¸”æ€§èƒ½ä¼˜å¼‚çš„æ¨¡å‹ï¼Œå› æ­¤åœ¨ä¸šç•Œå’Œæœºå™¨å­¦ä¹ é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›çš„è¿½æ§ã€‚

åœ¨æœ¬ç« å¼€å§‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬ç”¨ Python å®ç°äº†ä¸€ä¸ª MajorityVoteClassifier ç±»ï¼Œå®ƒå¯ä»¥é€šè¿‡ç»„åˆä¸åŒçš„ç®—æ³•å¾—åˆ°ä¸€ä¸ªåˆ†ç±»å™¨ã€‚è¿›è€Œæˆ‘ä»¬å­¦ä¹ äº† baggingï¼Œå®ƒèƒ½å¤Ÿåœ¨è®­ç»ƒé›†ä¸Šé€šè¿‡ boostrap è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œå¹¶ä»¥å¤šæ•°æŠ•ç¥¨ä¸ºå‡†åˆ™ç»„åˆå¤šä¸ªå•ç‹¬è®­ç»ƒçš„æˆå‘˜åˆ†ç±»å™¨ï¼Œç§°ä¸ºä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆé™ä½æ¨¡å‹æ–¹å·®çš„æ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬è®¨è®ºäº† AdaBoostï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºå¼±å­¦ä¹ æœºçš„ç®—æ³•ï¼Œèƒ½å¤Ÿä»å‰ä¸€ä¸ªå¼±å­¦ä¹ æœºé”™è¯¯ä¸­è¿›è¡Œå­¦ä¹ ã€‚

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å„ç§å­¦ä¹ ç®—æ³•ã€è°ƒä¼˜å’Œè¯„ä¼°æŠ€æœ¯ã€‚åé¢çš„ç« èŠ‚ï¼Œæˆ‘ä»¬å°†ç€çœ¼æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªç‰¹å®šåº”ç”¨â€”â€”æƒ…æ„Ÿåˆ†æï¼Œè¿™å·²æˆä¸ºå’Œç¤¾äº¤æ—¶ä»£ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜ã€‚