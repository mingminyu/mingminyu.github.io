---
date: 2025-02-23
authors:
  - mingminyu
categories:
  - 深度学习
tags:
  - 转载
  - Transformer
slug: dive-into-tranformer-p2
readtime: 20
---

# Transformer 解析

> 原文地址：https://mp.weixin.qq.com/s/jMUjAzyhX1sIbmZXx1ILKg

Transformer 是深度学习中一种用于处理序列数据（如文本、音频、时间序列等）的模型架构，具有并行处理和捕捉远程依赖关系的能力，它的核心创新是自注意力机制（Self-Attention Mechanism），使得模型能够在不依赖于递归结构的情况下处理序列中的全局依赖。

Transformer 最初由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出，与传统的循环神经网络（RNN）和长短期记忆网络（LSTM）不同，它不需要按顺序处理数据，而是通过 **自注意力机制**（Self-Attention Mechanism）来处理数据。这种设计使得 Transformer 能够更有效地并行处理数据，从而在处理长序列时更加高效。

![Transformer架构](https://mingminyu.github.io/webassets/images/20250223-03.png)

<!-- more -->

## 1. 从 LSTM/RNN 转向 Transformer

在自然语言处理 (NLP) 的发展过程中，长短期记忆 (LSTM) 和循环神经网络 (RNN) 发挥了至关重要的作用，使模型能够捕捉序列数据中的时间依赖性。然而，随着任务复杂性的增加，LSTM 和 RNN 这些模型面临局限性，特别是在处理长距离依赖性和并行化方面。

- **顺序处理**：LSTM 和 RNN 以顺序方式处理数据，因此处理长序列时速度较慢，并且难以并行化。这会影响可扩展性，尤其是在处理大型数据集时。
- **梯度消失/爆炸**：随着序列变长，这些模型难以应对梯度消失或爆炸，导致深度网络训练困难，并且在长距离上丢失重要背景。
- **有限的长距离依赖关系捕获**：虽然 LSTM 和 RNN 理论上可以捕获长期依赖关系，但在实践中，它们难以有效理解序列中远距离单词之间的关系。

Transformer 通过引入自注意力机制，使模型能够同时关注序列的所有部分，这一转变解决了 LSTM/RNN 模型的几个局限性：

- **并行化**：与 LSTM/RNN 不同，Transformer 并行处理序列，大大提高了计算效率，使其适合在大规模数据集上进行训练。
- **有效的长距离依赖关系**：自注意力机制使 Transformer 能够直接模拟序列中所有单词之间的关系，而不管距离如何，从而更好地理解上下文。
- **可扩展性**：Transformer 可以有效扩展，处理更大的模型和数据集，而不会出现 LSTM/RNN 所面临的训练问题。

Transformer 模型主要由编码器（Encoder）和解码器（Decoder）两部分组成，编码器和解码器各由多个相同的层（layer）堆叠而成。在原始论文中，Transformer 的编码器和解码器各有 6 层，但可以根据所需的复杂性扩展到任意数量的 N 层。

![编码器和解码器架构](https://mingminyu.github.io/webassets/images/20250223-04.png)

## 2 编码器工作流程

编码器是 Transformer 架构的重要组成部分，它负责将输入序列（如文本句子）转换为包含丰富上下文信息的高维表示。编码器由多个相同的层组成，每个层都有两个主要子层：**多头自注意力机制** 和 **前馈神经网络**。此外，每个子层都包含 ==残差连接== 和 ==层归一化==，以增强稳定性和性能。

![编码器工作流程](https://mingminyu.github.io/webassets/images/20250223-05.png)

### 2.1 输入嵌入

首先将输入序列中的每个词转换为固定维度的词向量，该过程从最底层的编码器开始，其中输入标记（单词或子单词）使用嵌入层转换为数值向量。这些嵌入捕获标记的语义含义并将其转换为固定大小的向量，通常大小为 **512**。

![编码器工作流程](https://mingminyu.github.io/webassets/images/20250223-06.webp)

### 2.2 位置编码

与 RNN 不同，Transformer 缺乏内置机制来捕获 token 的顺序。为了解决这个问题，位置编码被添加到输入嵌入中，提供有关每个 token 在序列中的位置的信息。
研究人员介绍了一种使用正弦和余弦函数生成位置编码的方法，可以表示任意长度的序列。

$$
\begin{align}
PE_{(pos, 2i)} &= \sin(\frac{\text{pos}}{10000^{2i/d}}) \\
PE_{(pos, 2i+1)} &= \cos(\frac{\text{pos}}{10000^{2i/d}})
\end{align}
$$

其中，pos 是位置，$i$ 是维度索引，$d$ 是嵌入维度。位置编码将被添加到词向量中，使得每个词的表示包含了位置信息。

![](https://mingminyu.github.io/webassets/images/20250223-07.webp)

## 2.3 编码器的堆叠

Transformer 编码器由一堆相同的层组成，原始模型中通常有 6 个层。每层由两个关键子层组成：

- **多头自注意力机制**：作为 Transformer 的核心组件，每个词都通过与序列中所有其他词的关系进行编码，多头注意力机制允许模型在多个子空间中独立关注不同的上下文。
- **前馈神经网络**：每个词的注意力输出经过一个前馈神经网络进行进一步处理，这个网络对每个位置的输入独立计算，且通常由两个线性变换和一个 `ReLU` 激活函数组成。

为了提高稳定性和性能，在每个子层都应用了残差连接，然后进行层归一化。这确保信息在网络中顺畅流动，同时保持数据的完整性。

![](https://mingminyu.github.io/webassets/images/20250223-08.webp)

### 2.3.1 多头注意力机制

多头注意力机制（Multi-Head Attention）是 Transformer 模型中的一个关键组件，用于增强模型的表达能力和捕捉输入序列中的不同特征。它的主要思想是在同一时间通过多个独立的注意力头（Attention Head）来关注序列中不同部分的信息，然后将这些信息综合起来，生成更丰富的表示。

![多头注意力机制](https://mingminyu.github.io/webassets/images/20250223-09.webp)

多头注意力机制的计算过程可以分为以下几个步骤：

- **线性变换生成多个头**：对输入序列的每个词向量分别通过三个不同的线性变换，生成对应的查询（Query）、键（Key）和值（Value）向量。对于每个头，这些线性变换的参数是独立的。如果模型有 $h$ 个头，每个头的查询、键和值向量的维度通常被设置为总维度的 $\frac{1}{h}$，这样所有头的输出拼接在一起后仍然保持原来的维度。

![](https://mingminyu.github.io/webassets/images/20250223-10.webp)

- **独立计算每个头的注意力**：每个头独立执行自注意力机制，即计算查询和键之间的相关性（注意力得分），然后用这些得分对值进行加权求和，生成每个头的输出。具体来说，对于第 $i$ 个头，计算如下：

$$
\text{Attention}_i (Q_i, K_i, V_i) = \text{softmax} \left( \frac{Q_i K_i^T}{\sqrt{d_k}} \right) V_i
$$

![](https://mingminyu.github.io/webassets/images/20250223-12.webp)

- **拼接头的输出**：将所有注意力头的输出向量拼接在一起，得到一个大的向量。拼接后，向量的维度恢复到与输入向量相同的维度。
- **线性变换整合信息**：最后，通过一个线性变换将拼接后的向量进行投影，生成多头注意力机制的最终输出。

### 2.3.2 层归一化和残差连接

在编码器架构中，每个子层后面都有一个归一化步骤。此外，还采用了残差连接，将每个子层的输入添加到其输出中。这种技术有助于缓解梯度消失问题，通过确保重要信息在通过网络时得到保留，促进更深层次模型的训练。

$$
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
$$

其中，$\text{Sublayer}(x)$ 是对输入 $x$ 进行子层操作（如多头注意力或前馈网络）后的输出结果。

![](https://mingminyu.github.io/webassets/images/20250223-11.webp)


### 2.3.3 前馈神经网络

前馈神经网络通常由两个全连接层（Fully Connected Layers）和一个非线性激活函数（通常是ReLU）组成，它用于对每个位置的表示进行进一步的非线性变换。它在每个位置上独立操作，不共享参数。

![](https://mingminyu.github.io/webassets/images/20250223-13.webp)

### 2.3.4 编码器的输出

最终编码器层的输出由一系列向量组成，每个向量都提供了输入序列的丰富上下文表示。然后，这些向量作为输入传递到 Transformer 模型中的解码器，它们在引导解码过程中起着至关重要的作用。这种细致的编码过程确保解码器在翻译或生成任务期间能够有效地专注于输入序列的相关部分。

## 3. 解码器工作流程

解码器结构与编码器类似，包含多个相同的层（通常也是 6 层），但解码器的每一层有三个子层：一个用于处理解码器输入的多头自注意力机制，一个用于关注编码器输出的多头注意力机制，以及一个前馈神经网络。

![](https://mingminyu.github.io/webassets/images/20250223-14.webp)

### 3.1 Masked 多头自注意力子层

在标准的多头注意力机制中，每个位置的查询（Query）会与所有位置的键（Key）进行点积计算，得到注意力分数，然后与值（Value）加权求和，生成最终的输出。然而，在解码器中，生成序列时不能访问未来的信息。因此需要使用掩码（Mask）机制来屏蔽掉未来位置的信息，防止信息泄露。

具体来说，在计算注意力得分时，对未来的位置进行屏蔽，将这些位置的得分设为负无穷大，使得 Softmax 归一化后的权重为零。例如，在计算单词 “are” 的注意力得分时，该机制可确保 “are” 无法访问序列中稍后出现的 “you” 的信息。

![](https://mingminyu.github.io/webassets/images/20250223-15.webp)

### 3.2 编码器-解码器多头注意力子层

编码器-解码器多头注意力子层在 Transformer 解码器中起到了关键作用，它使解码器能够有效地关注输入序列（编码器的输出），从而在生成序列时参考原始输入信息。具体来说，编码器-解码器多头注意力的基本思想是通过对编码器输出（Key 和 Value）和解码器当前输入（Query）来生成新的表示。这种机制使得解码器能够在生成序列时动态地选择关注输入序列的不同部分。

![](https://mingminyu.github.io/webassets/images/20250223-16.webp)

### 3.3 前馈神经网络

与编码器类似，这里不再赘述。

### 3.4 线性分类器和 Softmax 生成输出概率

数据通过 Transformer 模型的旅程最终会通过最后的线性层，该层起到分类器的作用。该分类器的大小对应于所涉及的类别总数（词汇表中包含的单词数）。例如，在 1000 个不同类别代表 1000 个不同单词的场景中，分类器的输出将是一个包含 1000 个元素的数组。

然后将该输出引入到 softmax 层，将其转换为一系列概率分数，每个分数介于 0 和 1 之间。这些概率分数中的最高值是关键，其对应的索引直接指向模型预测为序列中的下一个单词。

![](https://mingminyu.github.io/webassets/images/20250223-17.webp)
