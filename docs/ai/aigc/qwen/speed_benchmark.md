# 效率评估

本文介绍 Qwen2 模型（原始模型和量化模型）的效率测试结果，包括推理速度与不同上下文长度时的显存占用。

???+ "Transformers 和 vLLM 测试环境"

    === "HF Transformers"

        - NVIDIA A100 80GB
        - CUDA 11.8
        - Pytorch 2.1.2+cu118
        - Flash Attention 2.3.3
        - Transformers 4.38.2
        - AutoGPTQ 0.7.1
        - AutoAWQ 0.2.4

    === "vLLM"

        - NVIDIA A100 80GB
        - CUDA 11.8
        - Pytorch 2.3.0+cu118
        - Flash Attention 2.5.6
        - Transformers 4.40.1
        - vLLM 0.4.2

!!! warning "注意事项"

    - 为保证 使用 GPU 数量尽可能少，这里 `batch_size` 设置为 1。
    - 测试生成 2048 tokens 时的速度与显存占用，输入长度分别为 1、6144、14336、30720、63488、129024 tokens。(超过 32K 长度仅有 Qwen2-72B-Instruct 与 Qwen2-7B-Instruct 支持)
    - 对于 vLLM，由于 GPU 显存预分配，实际显存使用难以评估。默认情况下，统一设定为 `gpu_memory_utilization=0.9`、`max_model_len=32768`、`enforce_eager=False`。

## 1. Qwen2 Transformers 模型效率对比

???+ "Qwen2 不同 Transformers 模型的效率对比"

    === "0.5B"

        | 输入长度 | 量化 | GPU 数量 | 速度(tokens/s) | 占用显存(GB) |
        | ------------ | ------------ | ------- | --------------- | -------------- |
        | 1            | BF16         | 1       | 49.94           | 1.17           |
        |              | GPTQ-Int8    | 1       | 36.35           | 0.85           |
        |              | GPTQ-Int4    | 1       | 49.56           | 0.68           |
        |              | AWQ          | 1       | 38.78           | 0.68           |
        | 6144         | BF16         | 1       | 50.83           | 6.42           |
        |              | GPTQ-Int8    | 1       | 36.56           | 6.09           |
        |              | GPTQ-Int4    | 1       | 49.63           | 5.93           |
        |              | AWQ          | 1       | 38.73           | 5.92           |
        | 14336        | BF16         | 1       | 49.56           | 13.48          |
        |              | GPTQ-Int8    | 1       | 36.23           | 13.15          |
        |              | GPTQ-Int4    | 1       | 48.68           | 12.97          |
        |              | AWQ          | 1       | 38.94           | 12.99          |
        | 30720        | BF16         | 1       | 49.25           | 27.61          |
        |              | GPTQ-Int8    | 1       | 34.61           | 27.28          |
        |              | GPTQ-Int4    | 1       | 48.18           | 27.12          |
        |              | AWQ          | 1       | 38.19           | 27.11          |

    === "1.5B"

        | 输入长度 | 量化 | GPU 数量 | 速度(tokens/s) | 占用显存(GB) |
        | ------------ | ------------ | ------- | --------------- | -------------- |
        | 1            | BF16         | 1       | 40.89           | 3.44           |
        |              | GPTQ-Int8    | 1       | 31.51           | 2.31           |
        |              | GPTQ-Int4    | 1       | 49.56           | 1.67           |
        |              | AWQ          | 1       | 33.62           | 1.64           |
        | 6144         | BF16         | 1       | 40.86           | 8.74           |
        |              | GPTQ-Int8    | 1       | 31.31           | 7.59           |
        |              | GPTQ-Int4    | 1       | 42.78           | 6.95           |
        |              | AWQ          | 1       | 32.90           | 6.92           |
        | 14336        | BF16         | 1       | 40.08           | 15.92          |
        |              | GPTQ-Int8    | 1       | 31.19           | 14.79          |
        |              | GPTQ-Int4    | 1       | 42.25           | 14.14          |
        |              | AWQ          | 1       | 33.24           | 14.12          |
        | 30720        | BF16         | 1       | 34.09           | 30.31          |
        |              | GPTQ-Int8    | 1       | 28.52           | 29.18          |
        |              | GPTQ-Int4    | 1       | 31.30           | 28.54          |
        |              | AWQ          | 1       | 32.16           | 28.51          |

    === "7B"

        | 输入长度 | 量化 | GPU 数量 | 速度(tokens/s) | 占用显存(GB) |
        | ------------ | ------------ | ------- | --------------- | -------------- |
        | 1            | BF16         | 1       | 37.97           | 14.92           |
        |              | GPTQ-Int8    | 1       | 30.85           | 8.97           |
        |              | GPTQ-Int4    | 1       | 36.17           | 6.06           |
        |              | AWQ          | 1       | 33.08           | 5.93           |
        | 6144         | BF16         | 1       | 34.74           | 20.26           |
        |              | GPTQ-Int8    | 1       | 31.13           | 14.31           |
        |              | GPTQ-Int4    | 1       | 33.34           | 11.40           |
        |              | AWQ          | 1       | 30.86           | 11.27          |
        | 14336        | BF16         | 1       | 26.63           | 27.71          |
        |              | GPTQ-Int8    | 1       | 24.58           | 21.76          |
        |              | GPTQ-Int4    | 1       | 25.81           | 18.86          |
        |              | AWQ          | 1       | 27.61           | 18.72          |
        | 30720        | BF16         | 1       | 17.49           | 42.62          |
        |              | GPTQ-Int8    | 1       | 16.69           | 36.67          |
        |              | GPTQ-Int4    | 1       | 17.17           | 33.76          |
        |              | AWQ          | 1       | 17.87           | 33.63          |

    === "57B-A14B"

        | 输入长度 | 量化 | GPU 数量 | 速度(tokens/s) | 占用显存(GB) |
        | ------------ | ------------ | ------- | --------------- | -------------- |
        | 1            | BF16         | 2       | 4.76           | 110.29           |
        |              | GPTQ-Int4    | 1       | 5.55           | 30.38           |
        | 6144         | BF16         | 2       | 4.90           | 117.80           |
        |              | GPTQ-Int4    | 1       | 5.44           | 35.67           |
        | 14336        | BF16         | 1       | 4.58           | 128.17          |
        |              | GPTQ-Int4    | 1       | 5.31           | 43.11           |
        | 30720        | BF16         | 2       | 4.12           | 163.77          |
        |              | GPTQ-Int4    | 1       | 4.72           | 58.01          |

    === "72B"

        | 输入长度 | 量化 | GPU 数量 | 速度(tokens/s) | 占用显存(GB) |
        | ------------ | ------------ | ------- | --------------- | -------------- |
        | 1            | BF16         | 2       | 7.45           | 134.74           |
        |              | GPTQ-Int8    | 2       | 7.30           | 71.00          |
        |              | GPTQ-Int4    | 1       | 9.05           | 41.80           |
        |              | AWQ          | 1       | 9.96           | 41.31           |
        | 6144         | BF16         | 2       | 5.99           | 144.38           |
        |              | GPTQ-Int8    | 2       | 5.93           | 80.60           |
        |              | GPTQ-Int4    | 1       | 6.79           | 47.90           |
        |              | AWQ          | 1       | 7.49           | 47.42          |
        | 14336        | BF16         | 3       | 4.12           | 169.93          |
        |              | GPTQ-Int8    | 2       | 4.43           | 95.14          |
        |              | GPTQ-Int4    | 1       | 4.87           | 57.79          |
        |              | AWQ          | 1       | 5.23           | 57.30          |
        | 30720        | BF16         | 3       | 2.86           | 209.03          |
        |              | GPTQ-Int8    | 2       | 2.83           | 124.20          |
        |              | GPTQ-Int4    | 2       | 3.02           | 107.94          |
        |              | AWQ          | 2       | 1.85           | 88.60          |

## 2. Qwen2 vLLM 模型效率对比

数据由 vLLM 吞吐量测试脚本测得，可通过以下命令复现:

```bash linenums="1"
python vllm/benchmarks/benchmark_throughput.py \
  --input-len 1000 \
  --output-len 100 \
  --model <model_path> \
  --num-prompts <number of prompts> \
  --enforce-eager \
  -tp 2
```

???+ "Qwen2 不同 vLLM 模型的效率对比"

    === "0.5B"

        | 输入长度  | 量化        | GPU 数量 | 速度(tokens/s) |
        | ----- | --------- | ------ | ------------ |
        | 1     | BF16      | 1      | 270.49       |
        |       | GPTQ-Int8 | 1      | 235.95       |
        |       | GPTQ-Int4 | 1      | 240.07       |
        |       | AWQ       | 1      | 233.31       |
        | 6144  | BF16      | 1      | 256.16       |
        |       | GPTQ-Int8 | 1      | 224.30       |
        |       | GPTQ-Int4 | 1      | 226.41       |
        |       | AWQ       | 1      | 222.83       |
        | 14336 | BF16      | 1      | 108.89       |
        |       | GPTQ-Int8 | 1      | 108.10       |
        |       | GPTQ-Int4 | 1      | 106.51       |
        |       | AWQ       | 1      | 104.16       |
        | 30720 | BF16      | 1      | 97.20        |
        |       | GPTQ-Int8 | 1      | 94.49        |
        |       | GPTQ-Int4 | 1      | 93.94        |
        |       | AWQ       | 1      | 92.23        |

    === "1.5B vLLM"

        | 输入长度  | 量化        | GPU 数量 | 速度(tokens/s) |
        | ----- | --------- | ------ | ------------ |
        | 1     | BF16      | 1      | 175.55       |
        |       | GPTQ-Int8 | 1      | 172.28       |
        |       | GPTQ-Int4 | 1      | 184.58       |
        |       | AWQ       | 1      | 170.87       |
        | 6144  | BF16      | 1      | 166.23       |
        |       | GPTQ-Int8 | 1      | 164.32       |
        |       | GPTQ-Int4 | 1      | 174.04       |
        |       | AWQ       | 1      | 162.81       |
        | 14336 | BF16      | 1      | 83.67       |
        |       | GPTQ-Int8 | 1      | 98.63       |
        |       | GPTQ-Int4 | 1      | 97.65       |
        |       | AWQ       | 1      | 92.48       |
        | 30720 | BF16      | 1      | 77.69        |
        |       | GPTQ-Int8 | 1      | 86.42        |
        |       | GPTQ-Int4 | 1      | 87.49        |
        |       | AWQ       | 1      | 82.88        |

    === "7B"

        | 输入长度  | 量化        | GPU 数量 | 速度(tokens/s) |
        | ----- | --------- | ------ | ------------ |
        | 1     | BF16      | 1      | 80.45       |
        |       | GPTQ-Int8 | 1      | 114.32       |
        |       | GPTQ-Int4 | 1      | 143.40       |
        |       | AWQ       | 1      | 96.65       |
        | 6,144  | BF16      | 1      | 76.41       |
        |       | GPTQ-Int8 | 1      | 107.02       |
        |       | GPTQ-Int4 | 1      | 131.55       |
        |       | AWQ       | 1      | 91.38       |
        | 14,336 | BF16      | 1      | 66.54       |
        |       | GPTQ-Int8 | 1      | 89.72       |
        |       | GPTQ-Int4 | 1      | 97.93       |
        |       | AWQ       | 1      | 76.87       |
        | 30,720 | BF16      | 1      | 55.83        |
        |       | GPTQ-Int8 | 1      | 71.58        |
        |       | GPTQ-Int4 | 1      | 81.48        |
        |       | AWQ       | 1      | 63.62        |
        | 63,488 | BF16      | 1      | 41.20        |
        |       | GPTQ-Int8 | 1      | 49.37        |
        |       | GPTQ-Int4 | 1      | 54.12        |
        |       | AWQ       | 1      | 45.89        |
        | 129,024 | BF16      | 1      | 25.01        |
        |       | GPTQ-Int8 | 1      | 27.73        |
        |       | GPTQ-Int4 | 1      | 29.39        |
        |       | AWQ       | 1      | 27.13        |

    === "57B-A14B"

        | 输入长度 | 量化 | GPU 数量 | 速度(tokens/s) |
        | ------------ | ------------ | ------- | --------------- |
        | 1            | BF16         | 2       | 31.44           | 
        | 6144         | BF16         | 2       | 31.77           | 
        | 14336        | BF16         | 2       | 21.25           | 
        | 30720        | BF16         | 2       | 20.24         | 

    === "72B"

        | 输入长度  | 量化        | GPU 数量 | 速度(tokens/s) |
        | ----- | --------- | ------ | ------------ |
        | 1     | BF16      | 2      | 17.68       |
        |       | BF16      | 4      | 30.01       |
        |       | GPTQ-Int8 | 2      | 27.56       |
        |       | GPTQ-Int4 | 1      | 29.60       |
        |       | GPTQ-Int4 | 2      | 42.82       |
        |       | AWQ       | 2      | 27.73       |
        | 6,144  | BF16      | 4      | 27.98       |
        |       | GPTQ-Int8 | 2      | 25.46       |
        |       | GPTQ-Int4 | 1      | 25.16       |
        |       | GPTQ-Int4 | 2      | 38.23       |
        |       | AWQ       | 2      | 25.77      |
        | 14,336 | BF16      | 4      | 21.81       |
        |       | GPTQ-Int8 | 2      | 22.71       |
        |       | GPTQ-Int4 | 2      | 26.64       |
        |       | AWQ       | 2      | 21.50       |
        | 30,720 | BF16      | 4      | 19.43        |
        |       | GPTQ-Int8 | 2      | 18.69        |
        |       | GPTQ-Int4 | 2      | 23.12        |
        |       | AWQ       | 2      | 18.09        |
        | 63,488 | BF16      | 4      | 17.46        |
        |       | GPTQ-Int8 | 2      | 15.30        |
        |       | GPTQ-Int4 | 2      | 13.23        |
        |       | AWQ       | 2      | 13.14        |
        | 129,024 | BF16      | 4      | 11.70        |
        |       | GPTQ-Int8 | 4      | 12.94        |
        |       | GPTQ-Int4 | 2      | 8.33        |
        |       | AWQ       | 2      | 7.78        |

        不同参数设定下，表现可能会出现差异。

混合专家模型 (Mixture-of-Experts, MoE) 与稠密模型相比，当批大小较大时，吞吐量更大。下表展示了有关数据：

| 模型                    | 量化 | 提示词   | QPS  | Tokens/s |
|--------------------------|--------------|-------------|------|----------|
| Qwen1.5-32B-Chat         | BF16         | 100         | 6.68 | 7343.56  |
| Qwen2-57B-A14B-Instruct  | BF16         | 100         | 4.81 | 5291.15  |
| Qwen1.5-32B-Chat         | BF16         | 1000        | 7.99 | 8791.35  |
| Qwen2-57B-A14B-Instruct  | BF16         | 1000        | 5.18 | 5698.37  |
